{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Physical GPUs, 1 Logical GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 18:00:53.477930: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-11 18:00:54.038970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1024 MB memory:  -> device: 2, name: Quadro RTX 8000, pci bus id: 0000:a6:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# from MD_AE_tools.models.models import *\n",
    "\n",
    "from MD_AE_tools.models.models_no_bias import *\n",
    "import MD_AE_tools.mode_decomposition as md\n",
    "import MD_AE_tools.ae_mode_evaluation as mode_eval\n",
    "import myplot\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from numpy import einsum\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import time\n",
    "import os\n",
    "import configparser\n",
    "import datetime\n",
    "import wandb\n",
    "\n",
    "# get system information\n",
    "config = configparser.ConfigParser()\n",
    "config.read('_system.ini')\n",
    "system_info = config['system_info']\n",
    "\n",
    "# use gpu\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[2], 'GPU')# use [] for cpu only, gpus[i] for the ith gpu\n",
    "        tf.config.set_logical_device_configuration(gpus[2],[tf.config.LogicalDeviceConfiguration(memory_limit=1024)]) # set hard memory limit\n",
    "        # tf.config.experimental.set_memory_growth(gpus[0], True) # allow memory growth\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data_file = './data/PIV4_downsampled_by8.h5'\n",
    "Ntrain = 1632 # snapshots for training\n",
    "Nval = 550 # sanpshots for validation\n",
    "Ntest = 550\n",
    "\n",
    "# Boolean \n",
    "LATENT_STATE = True # save latent state\n",
    "SHUFFLE = True # shuffle before splitting into sets, test set is extracted before shuffling\n",
    "REMOVE_MEAN = True # train on fluctuating velocity\n",
    "\n",
    "## ae configuration\n",
    "lmb = 0.000 #1e-05 #regulariser\n",
    "drop_rate = 0.0\n",
    "features_layers = [32, 64, 128]\n",
    "latent_dim = 2\n",
    "act_fct = 'linear'\n",
    "resize_meth = 'bilinear'\n",
    "filter_window= (5,5)\n",
    "batch_norm = False\n",
    "\n",
    "## training\n",
    "nb_epoch = 3000\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "Nz = 24 # grid size\n",
    "Ny = 21\n",
    "Nu = 2\n",
    "Nt = 2732 # number of snapshots available\n",
    "D = 196.5 # mm diameter of bluff body\n",
    "U_inf = 15 # m/s freestream velocity\n",
    "f_piv = 720.0 # Hz PIV sampling frequency  \n",
    "dt = 1.0/f_piv \n",
    "Nx = [Ny, Nz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating POD ...\n",
      "User has selected classic POD\n",
      "POD done.\n",
      "MSE reconstructed with 2 POD modes 1.4777343273162842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ym917/Codes/MD-CNN-AE/MD_AE_tools/mode_decomposition.py:220: RuntimeWarning: invalid value encountered in sqrt\n",
      "  normQ = (Q_POD.T @ Q_POD*self.w).real**0.5\n"
     ]
    }
   ],
   "source": [
    "#============================= READ DATA FROM FILE ================================\n",
    "hf = h5py.File('./data/ufluc_shuffle_1632.h5','r')\n",
    "u_all = np.array(hf.get('u_all'))\n",
    "u_train = np.array(hf.get('u_train'))\n",
    "u_val = np.array(hf.get('u_val'))\n",
    "u_test = np.array(hf.get('u_test'))\n",
    "u_mean_all = np.array(hf.get('u_mean_all'))\n",
    "u_mean_train = np.array(hf.get('u_mean_train'))\n",
    "u_mean_val = np.array(hf.get('u_mean_val'))\n",
    "u_mean_test = np.array(hf.get('u_mean_test'))\n",
    "hf.close()\n",
    "\n",
    "# POD data\n",
    "x = einsum('t y z u -> y z t u',np.squeeze(u_train))\n",
    "X = np.vstack((x[:,:,:,0],x[:,:,:,1]))\n",
    "pod_data = md.POD(X,method='classic')\n",
    "Q_POD_data,lam_data = pod_data.get_modes\n",
    "Q_mean = pod_data.Q_mean\n",
    "recons_data = pod_data.reconstruct(latent_dim,shape=[2,Ny,Nz,u_train.shape[1]])\n",
    "recons_data = np.transpose(recons_data,[3,1,2,0])\n",
    "\n",
    "A_data = pod_data.get_time_coefficient\n",
    "pod_modes_t = []\n",
    "for i in range(latent_dim):\n",
    "    Q_add = pod_data.Phi[:,[i]] @ A_data[:,[i]].T\n",
    "    rebuildv = np.reshape(Q_add,[2,Ny,Nz,A_data.shape[0]])\n",
    "    pod_modes_t.append(rebuildv)\n",
    "pod_modes_t = np.array(pod_modes_t) # [latent_dim,velocity,Ny,Nz,time]\n",
    "print('MSE reconstructed with 2 POD modes',tf.keras.losses.MeanSquaredError()(u_train[0,...],recons_data).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-training linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define model\n",
    "mdl_linear = MD_Autoencoder(Nx,Nu,features_layers=features_layers,latent_dim=latent_dim,\n",
    "        filter_window=filter_window,act_fct='linear',batch_norm=batch_norm,\n",
    "        drop_rate=drop_rate, lmb=lmb)\n",
    "mdl_linear.compile(optimizer=Adam(learning_rate=learning_rate),loss='mse')\n",
    "mdl_linear.evaluate(u_train[0,:,:,:,:],u_train[0,:,:,:,:],verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_linear.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_train = []\n",
    "hist_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## training\n",
    "pat = 200 # EarlyStopping\n",
    "tempfn = './temp_pre-training-linear.h5'\n",
    "model_cb=ModelCheckpoint(tempfn, monitor='val_loss',save_best_only=True,verbose=0,save_weights_only=True)\n",
    "early_cb=EarlyStopping(monitor='val_loss', patience=pat,verbose=0)\n",
    "cb = [model_cb, early_cb]\n",
    "\n",
    "# mdl_linear.load_weights('./_experiments/pre-training-linear-unshuffle-2.h5')\n",
    "# Training\n",
    "hist0 = mdl_linear.fit(u_train[0,:,:,:,:], u_train[0,:,:,:,:],\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(u_val[0,:,:,:,:], u_val[0,:,:,:,:]),\n",
    "                callbacks=cb,verbose=2)  \n",
    "hist_train.extend(hist0.history['loss'])\n",
    "hist_val.extend(hist0.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('linear')\n",
    "plt.plot(hist_train,label='train')\n",
    "plt.plot(hist_val,label='val')\n",
    "plt.ylim([1.2,1.6])\n",
    "# plt.xlim([1500,2000])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl_linear.load_weights('temp_pre-training-linear.h5')\n",
    "mdl_linear.load_weights('_experiments/pre-training-linear-unshuffle-2.h5')\n",
    "print('best training loss:',mdl_linear.evaluate(u_train[0,:,:,:,:],u_train[0,:,:,:,:],verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ae modes from training or testing set\n",
    "which = 'train'\n",
    "\n",
    "latent_test = mdl_linear.encoder.predict(u_test[0,:,:,:,:])\n",
    "latent_train = mdl_linear.encoder.predict(u_train[0,:,:,:,:])\n",
    "linear_decoders = []\n",
    "\n",
    "if which == 'train':\n",
    "    z_l = latent_train\n",
    "    u_in_l = u_train[0,:,:,:,:]\n",
    "elif which == 'test':\n",
    "    z_l = latent_test\n",
    "    u_in_l = u_test[0,:,:,:,:]\n",
    "\n",
    "for name in mdl_linear.name_decoder:\n",
    "    linear_decoders.append(mdl_linear.get_layer(name))\n",
    "linear_modes = []\n",
    "for i in range(latent_dim):\n",
    "    linear_modes.append(linear_decoders[i].predict(np.reshape(z_l[:,i],(-1,1))))\n",
    "linear_modes = np.array(linear_modes)\n",
    "y_test_sum = np.sum(linear_modes,axis=0)\n",
    "    \n",
    "y_test = mdl_linear.predict(u_in_l)\n",
    "\n",
    "print('Are results calculated the two ways the same?',np.array_equal(y_test_sum,y_test))\n",
    "\n",
    "loss_test_1 = mdl_linear.evaluate(u_in_l,u_in_l,verbose=0)\n",
    "loss_test_2 = tf.keras.losses.MeanSquaredError()(u_in_l,y_test)\n",
    "print('total loss:',loss_test_1,'    mse loss:',loss_test_2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for z in range(latent_dim):\n",
    "    # plt.plot(latent_test[:,z],label=str(z+1))\n",
    "    plt.plot(latent_train[:,z],label=str(z+1))\n",
    "plt.xlim([0,600])\n",
    "plt.title('latent variables')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov = latent_test.T@latent_test/(Ntest-1)\n",
    "cov = latent_train.T@latent_train/(Ntrain-1)\n",
    "# cov = np.cov(latent_test.T)\n",
    "vmax = np.max(cov)\n",
    "plt.figure()\n",
    "plt.title('linear')\n",
    "plt.imshow(cov,cmap='seismic',vmin=-vmax,vmax=vmax,extent=[0.5,latent_dim+0.5,latent_dim+0.5,0.5]) # vmin=-1,vmax=1,\n",
    "plt.xticks(np.arange(latent_dim)+1)\n",
    "plt.yticks(np.arange(latent_dim)+1)\n",
    "plt.colorbar()\n",
    "\n",
    "sort_idx = np.argsort(np.diag(cov))\n",
    "rank_var = np.arange(1,latent_dim+1)[np.flip(sort_idx)]\n",
    "print('variance:',np.diag(cov),' rank:',rank_var)\n",
    "\n",
    "# compare two variables\n",
    "pltz = [0,1]\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(latent_test[:,pltz[0]],latent_test[:,pltz[1]],alpha=0.5)\n",
    "plt.xlabel(\"latent variable %i\"%(pltz[0]+1))\n",
    "plt.ylabel(\"latent variable %i\"%(pltz[1]+1))\n",
    "# plt.xlim([-10,10])\n",
    "# plt.ylim([-10,10])\n",
    "# plt.plot([0,0.45391235],[0,0.89104634],linewidth=3,color='r')\n",
    "# plt.plot([0,-0.89104634],[0,0.45391235],linewidth=3,color='r')\n",
    "plt.show()\n",
    "covar_sum = np.sum(abs(cov-np.diag(np.diag(cov))),axis=0)\n",
    "sort_idx = np.argsort(covar_sum)\n",
    "rank_cov = np.arange(1,latent_dim+1)[np.flip(sort_idx)]\n",
    "print('covariance sum',rank_cov)\n",
    "\n",
    "det = np.linalg.det(np.corrcoef(latent_test.T))\n",
    "print('determinant of corr matrix: ',det) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_1 = md.POD(latent_train.T,method='classic')\n",
    "m_1,lam_1 = pod_1.get_modes()\n",
    "print(m_1,lam_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,5,figsize=(12,2))\n",
    "fig.suptitle('linear')\n",
    "ax[0].set_ylabel('v')\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    if i < len(linear_decoders):\n",
    "        m = linear_decoders[i].predict(np.reshape(1,(1,1)))\n",
    "        im = ax.imshow(m[0,:,:,0],'jet')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(str(i+1))\n",
    "        ax_divider = make_axes_locatable(ax)\n",
    "        cax = ax_divider.append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
    "        plt.colorbar(im,cax=cax)\n",
    "plt.tight_layout()\n",
    "        \n",
    "\n",
    "fig,ax = plt.subplots(1,5,figsize=(12,2))\n",
    "fig.suptitle('linear')\n",
    "ax[0].set_ylabel('w')\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    if i < len(linear_decoders):\n",
    "        m = linear_decoders[i].predict(np.reshape(1,(1,1)))\n",
    "        im = ax.imshow(m[0,:,:,1],'jet')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(str(i+1))\n",
    "        ax_divider = make_axes_locatable(ax)\n",
    "        cax = ax_divider.append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
    "        plt.colorbar(im,cax=cax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(latent_dim):\n",
    "    print('mse loss ae mode %i:'%(i+1),tf.keras.losses.MeanSquaredError()(u_in_l,linear_modes[i,:,:,:,:]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nonlinear before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_nonlinear = MD_Autoencoder(Nx,Nu,features_layers=features_layers,latent_dim=latent_dim,\n",
    "        filter_window=filter_window,act_fct='tanh',batch_norm=batch_norm,\n",
    "        drop_rate=drop_rate, lmb=lmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 21, 24, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " encoder (Encoder)              (None, 2)            259904      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " z_0 (Lambda)                   (None, 1)            0           ['encoder[0][0]']                \n",
      "                                                                                                  \n",
      " z_1 (Lambda)                   (None, 1)            0           ['encoder[0][0]']                \n",
      "                                                                                                  \n",
      " decoder_0 (Decoder)            (None, 21, 24, 2)    258752      ['z_0[0][0]']                    \n",
      "                                                                                                  \n",
      " decoder_1 (Decoder)            (None, 21, 24, 2)    258752      ['z_1[0][0]']                    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 21, 24, 2)    0           ['decoder_0[0][0]',              \n",
      "                                                                  'decoder_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 777,408\n",
      "Trainable params: 777,408\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mdl_nonlinear.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 18:01:05.279038: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss after loading weights from linear pre-training: 1.843330979347229\n"
     ]
    }
   ],
   "source": [
    "# load weights\n",
    "mdl_nonlinear.compile(optimizer=Adam(learning_rate=learning_rate),loss='mse')\n",
    "mdl_nonlinear.evaluate(u_train[0,:,:,:,:],u_train[0,:,:,:,:],verbose=0)\n",
    "# mdl_nonlinear.load_weights('temp_pre-training-linear.h5')\n",
    "# mdl_nonlinear.load_weights('_experiments/pre-training-linear-unshuffle-2.h5')\n",
    "print('training loss after loading weights from linear pre-training:',mdl_nonlinear.evaluate(u_train[0,:,:,:,:],u_train[0,:,:,:,:],verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get decoders\n",
    "which = 'train'\n",
    "\n",
    "latent_test_2 = mdl_nonlinear.encoder.predict(u_test[0,:,:,:,:])\n",
    "latent_train_2 = mdl_nonlinear.encoder.predict(u_train[0,:,:,:,:])\n",
    "nonlinear_decoders = []\n",
    "for name in mdl_nonlinear.name_decoder:\n",
    "    nonlinear_decoders.append(mdl_nonlinear.get_layer(name))\n",
    "\n",
    "nonlinear_modes = []\n",
    "if which == 'train':\n",
    "    for i in range(latent_dim):\n",
    "        nonlinear_modes.append(nonlinear_decoders[i].predict(np.reshape(latent_train_2[:,i],(-1,1))))\n",
    "elif which == 'test':\n",
    "    for i in range(latent_dim):\n",
    "        nonlinear_modes.append(nonlinear_decoders[i].predict(np.reshape(latent_test_2[:,i],(-1,1))))\n",
    "        \n",
    "nonlinear_modes = np.array(nonlinear_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for z in range(latent_dim):\n",
    "    # plt.plot(latent_test_2[:,z],label=str(z+1))\n",
    "    plt.plot(latent_train_2[:,z],label=str(z+1))\n",
    "plt.xlim([0,600])\n",
    "plt.title('latent variables, weight loaded, not trained')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov_2 = latent_test_2.T@latent_test_2/(Ntest-1)\n",
    "cov_2 = latent_train_2.T@latent_train_2/(Ntrain-1)\n",
    "# cov_2 = np.cov(latent_test.T)\n",
    "vmax = np.max(cov_2)\n",
    "plt.figure()\n",
    "plt.title('nonlinear before training')\n",
    "plt.imshow(cov_2,cmap='seismic',vmin=-vmax,vmax=vmax,extent=[0.5,latent_dim+0.5,latent_dim+0.5,0.5]) # vmin=-1,vmax=1,\n",
    "plt.xticks(np.arange(latent_dim)+1)\n",
    "plt.yticks(np.arange(latent_dim)+1)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "sort_idx = np.argsort(np.diag(cov_2))\n",
    "rank_var_2 = np.arange(1,latent_dim+1)[np.flip(sort_idx)]\n",
    "print('variance:',np.diag(cov_2),' rank:',rank_var_2)\n",
    "\n",
    "# compare two variables\n",
    "pltz = [0,1]\n",
    "plt.figure()\n",
    "plt.scatter(latent_test_2[:,pltz[0]],latent_test_2[:,pltz[1]])\n",
    "plt.xlabel(\"latent variable %i\"%(pltz[0]+1))\n",
    "plt.ylabel(\"latent variable %i\"%(pltz[1]+1))\n",
    "plt.show()\n",
    "\n",
    "det_2 = np.linalg.det(np.corrcoef(latent_test_2.T))\n",
    "print('determinant of corr matrix: ',det_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,5,figsize=(12,2))\n",
    "fig.suptitle('nonlinear ae modes before training')\n",
    "ax[0].set_ylabel('v')\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    if i < len(nonlinear_decoders):\n",
    "        m = nonlinear_decoders[i].predict(np.reshape(1,(1,1)))\n",
    "        im = ax.imshow(m[0,:,:,0],'jet')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(str(i+1))\n",
    "        ax_divider = make_axes_locatable(ax)\n",
    "        cax = ax_divider.append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
    "        plt.colorbar(im,cax=cax)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig,ax = plt.subplots(1,5,figsize=(12,2))\n",
    "fig.suptitle('nonlinear ae modes before training')\n",
    "ax[0].set_ylabel('w')\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    if i < len(nonlinear_decoders):\n",
    "        m = nonlinear_decoders[i].predict(np.reshape(1,(1,1)))\n",
    "        im = ax.imshow(m[0,:,:,1],'jet')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(str(i+1))\n",
    "        ax_divider = make_axes_locatable(ax)\n",
    "        cax = ax_divider.append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
    "        plt.colorbar(im,cax=cax)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decompose ae modes\n",
    "which_decoder = 1-1\n",
    "# PlotWhichVelocity = 'v' \n",
    "save_img = False\n",
    "vy = nonlinear_modes[which_decoder,:,:,:,0]\n",
    "vy = np.transpose(vy,[1,2,0])\n",
    "vz = nonlinear_modes[which_decoder,:,:,:,1]\n",
    "vz = np.transpose(vz,[1,2,0])\n",
    "X = np.vstack((vz,vy)) # new shape [2*ny,nz,nt]\n",
    "# POD\n",
    "pod = md.POD(X)\n",
    "Q_POD,lam = pod.get_modes()\n",
    "Q_mean = pod.Q_mean\n",
    "\n",
    "\n",
    "## plot POD\n",
    "fig1 = plt.figure(figsize=(10, 3))\n",
    "title = \"POD Modes in decoder \" + str(which_decoder+1)\n",
    "plt.suptitle(title)\n",
    "\n",
    "# v\n",
    "grid0 = ImageGrid(fig1,211,share_all=True,nrows_ncols=(1, 5),axes_pad=0.4,label_mode=\"1\",cbar_location=\"right\",cbar_mode=\"each\",cbar_size=\"7%\",cbar_pad=\"3%\")\n",
    "grid0.axes_llc.set_xticks([])\n",
    "grid0.axes_llc.set_yticks([])\n",
    "grid0.axes_llc.set_ylabel('v')\n",
    "iphi = 0\n",
    "for ax, cax in zip(grid0, grid0.cbar_axes):\n",
    "    pltV = Q_POD[:,iphi];\n",
    "    pltV = np.reshape(pltV,[2*Ny,Nz])\n",
    "    im = ax.imshow(pltV[Ny:,:],'jet')\n",
    "    cb = cax.colorbar(im)\n",
    "    iphi += 1\n",
    "# w\n",
    "grid1 = ImageGrid(fig1,212,share_all=True,nrows_ncols=(1, 5),axes_pad=0.4,label_mode=\"1\",cbar_location=\"right\",cbar_mode=\"each\",cbar_size=\"7%\",cbar_pad=\"3%\")\n",
    "grid1.axes_llc.set_xticks([])\n",
    "grid1.axes_llc.set_yticks([])\n",
    "grid1.axes_llc.set_ylabel('w')\n",
    "iphi = 0\n",
    "for ax, cax in zip(grid1, grid1.cbar_axes):\n",
    "    pltV = Q_POD[:,iphi];\n",
    "    pltV = np.reshape(pltV,[2*Ny,Nz])\n",
    "    im = ax.imshow(pltV[0:Ny,:],'jet')\n",
    "    cb = cax.colorbar(im)\n",
    "    iphi += 1\n",
    "\n",
    "\n",
    "## Spectrum\n",
    "energy_1 = np.cumsum(lam/np.sum(lam))\n",
    "print('spectrum (first 5): ', energy_1[:15])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nonlinear after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_train_3 = []\n",
    "hist_val_3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "17/17 - 2s - loss: 1.6977 - val_loss: 1.5906 - 2s/epoch - 134ms/step\n",
      "Epoch 2/3000\n",
      "17/17 - 0s - loss: 1.5971 - val_loss: 1.5266 - 395ms/epoch - 23ms/step\n",
      "Epoch 3/3000\n",
      "17/17 - 0s - loss: 1.5496 - val_loss: 1.5063 - 400ms/epoch - 24ms/step\n",
      "Epoch 4/3000\n",
      "17/17 - 0s - loss: 1.5228 - val_loss: 1.4934 - 392ms/epoch - 23ms/step\n",
      "Epoch 5/3000\n",
      "17/17 - 0s - loss: 1.5125 - val_loss: 1.4906 - 392ms/epoch - 23ms/step\n",
      "Epoch 6/3000\n",
      "17/17 - 0s - loss: 1.5099 - val_loss: 1.4836 - 407ms/epoch - 24ms/step\n",
      "Epoch 7/3000\n",
      "17/17 - 0s - loss: 1.5055 - val_loss: 1.4823 - 394ms/epoch - 23ms/step\n",
      "Epoch 8/3000\n",
      "17/17 - 0s - loss: 1.5023 - val_loss: 1.4830 - 392ms/epoch - 23ms/step\n",
      "Epoch 9/3000\n",
      "17/17 - 0s - loss: 1.4982 - val_loss: 1.4776 - 396ms/epoch - 23ms/step\n",
      "Epoch 10/3000\n",
      "17/17 - 0s - loss: 1.4964 - val_loss: 1.4791 - 394ms/epoch - 23ms/step\n",
      "Epoch 11/3000\n",
      "17/17 - 0s - loss: 1.4965 - val_loss: 1.4784 - 352ms/epoch - 21ms/step\n",
      "Epoch 12/3000\n",
      "17/17 - 0s - loss: 1.4969 - val_loss: 1.4902 - 343ms/epoch - 20ms/step\n",
      "Epoch 13/3000\n",
      "17/17 - 0s - loss: 1.4988 - val_loss: 1.4752 - 353ms/epoch - 21ms/step\n",
      "Epoch 14/3000\n",
      "17/17 - 0s - loss: 1.4968 - val_loss: 1.4794 - 365ms/epoch - 21ms/step\n",
      "Epoch 15/3000\n",
      "17/17 - 0s - loss: 1.4976 - val_loss: 1.4769 - 352ms/epoch - 21ms/step\n",
      "Epoch 16/3000\n",
      "17/17 - 0s - loss: 1.4964 - val_loss: 1.4768 - 359ms/epoch - 21ms/step\n",
      "Epoch 17/3000\n",
      "17/17 - 0s - loss: 1.4930 - val_loss: 1.4772 - 396ms/epoch - 23ms/step\n",
      "Epoch 18/3000\n",
      "17/17 - 0s - loss: 1.4912 - val_loss: 1.4718 - 390ms/epoch - 23ms/step\n",
      "Epoch 19/3000\n",
      "17/17 - 0s - loss: 1.4881 - val_loss: 1.4721 - 406ms/epoch - 24ms/step\n",
      "Epoch 20/3000\n",
      "17/17 - 0s - loss: 1.4879 - val_loss: 1.4726 - 396ms/epoch - 23ms/step\n",
      "Epoch 21/3000\n",
      "17/17 - 0s - loss: 1.4896 - val_loss: 1.4734 - 353ms/epoch - 21ms/step\n",
      "Epoch 22/3000\n",
      "17/17 - 0s - loss: 1.4892 - val_loss: 1.4714 - 352ms/epoch - 21ms/step\n",
      "Epoch 23/3000\n",
      "17/17 - 0s - loss: 1.4891 - val_loss: 1.4696 - 359ms/epoch - 21ms/step\n",
      "Epoch 24/3000\n",
      "17/17 - 0s - loss: 1.4857 - val_loss: 1.4730 - 397ms/epoch - 23ms/step\n",
      "Epoch 25/3000\n",
      "17/17 - 0s - loss: 1.4864 - val_loss: 1.4728 - 357ms/epoch - 21ms/step\n",
      "Epoch 26/3000\n",
      "17/17 - 0s - loss: 1.4888 - val_loss: 1.4703 - 352ms/epoch - 21ms/step\n",
      "Epoch 27/3000\n",
      "17/17 - 0s - loss: 1.4878 - val_loss: 1.4738 - 351ms/epoch - 21ms/step\n",
      "Epoch 28/3000\n",
      "17/17 - 0s - loss: 1.4871 - val_loss: 1.4701 - 363ms/epoch - 21ms/step\n",
      "Epoch 29/3000\n",
      "17/17 - 0s - loss: 1.4861 - val_loss: 1.4746 - 351ms/epoch - 21ms/step\n",
      "Epoch 30/3000\n",
      "17/17 - 0s - loss: 1.4874 - val_loss: 1.4714 - 360ms/epoch - 21ms/step\n",
      "Epoch 31/3000\n",
      "17/17 - 0s - loss: 1.4854 - val_loss: 1.4681 - 402ms/epoch - 24ms/step\n",
      "Epoch 32/3000\n",
      "17/17 - 0s - loss: 1.4815 - val_loss: 1.4717 - 393ms/epoch - 23ms/step\n",
      "Epoch 33/3000\n",
      "17/17 - 0s - loss: 1.4825 - val_loss: 1.4690 - 354ms/epoch - 21ms/step\n",
      "Epoch 34/3000\n",
      "17/17 - 0s - loss: 1.4804 - val_loss: 1.4690 - 396ms/epoch - 23ms/step\n",
      "Epoch 35/3000\n",
      "17/17 - 0s - loss: 1.4798 - val_loss: 1.4704 - 413ms/epoch - 24ms/step\n",
      "Epoch 36/3000\n",
      "17/17 - 0s - loss: 1.4790 - val_loss: 1.4725 - 392ms/epoch - 23ms/step\n",
      "Epoch 37/3000\n",
      "17/17 - 0s - loss: 1.4801 - val_loss: 1.4790 - 356ms/epoch - 21ms/step\n",
      "Epoch 38/3000\n",
      "17/17 - 0s - loss: 1.4890 - val_loss: 1.4751 - 352ms/epoch - 21ms/step\n",
      "Epoch 39/3000\n",
      "17/17 - 0s - loss: 1.4824 - val_loss: 1.4791 - 370ms/epoch - 22ms/step\n",
      "Epoch 40/3000\n",
      "17/17 - 0s - loss: 1.4807 - val_loss: 1.4738 - 361ms/epoch - 21ms/step\n",
      "Epoch 41/3000\n",
      "17/17 - 0s - loss: 1.4789 - val_loss: 1.4702 - 394ms/epoch - 23ms/step\n",
      "Epoch 42/3000\n",
      "17/17 - 0s - loss: 1.4747 - val_loss: 1.4738 - 414ms/epoch - 24ms/step\n",
      "Epoch 43/3000\n",
      "17/17 - 0s - loss: 1.4735 - val_loss: 1.4722 - 405ms/epoch - 24ms/step\n",
      "Epoch 44/3000\n",
      "17/17 - 0s - loss: 1.4774 - val_loss: 1.4774 - 368ms/epoch - 22ms/step\n",
      "Epoch 45/3000\n",
      "17/17 - 0s - loss: 1.4721 - val_loss: 1.4871 - 413ms/epoch - 24ms/step\n",
      "Epoch 46/3000\n",
      "17/17 - 0s - loss: 1.4751 - val_loss: 1.4762 - 403ms/epoch - 24ms/step\n",
      "Epoch 47/3000\n",
      "17/17 - 0s - loss: 1.4740 - val_loss: 1.4770 - 393ms/epoch - 23ms/step\n",
      "Epoch 48/3000\n",
      "17/17 - 0s - loss: 1.4738 - val_loss: 1.4762 - 403ms/epoch - 24ms/step\n",
      "Epoch 49/3000\n",
      "17/17 - 0s - loss: 1.4729 - val_loss: 1.4771 - 391ms/epoch - 23ms/step\n",
      "Epoch 50/3000\n",
      "17/17 - 0s - loss: 1.4651 - val_loss: 1.4763 - 423ms/epoch - 25ms/step\n",
      "Epoch 51/3000\n",
      "17/17 - 0s - loss: 1.4609 - val_loss: 1.4808 - 410ms/epoch - 24ms/step\n",
      "Epoch 52/3000\n",
      "17/17 - 0s - loss: 1.4624 - val_loss: 1.4831 - 386ms/epoch - 23ms/step\n",
      "Epoch 53/3000\n",
      "17/17 - 0s - loss: 1.4592 - val_loss: 1.4803 - 412ms/epoch - 24ms/step\n",
      "Epoch 54/3000\n",
      "17/17 - 0s - loss: 1.4588 - val_loss: 1.4800 - 406ms/epoch - 24ms/step\n",
      "Epoch 55/3000\n",
      "17/17 - 0s - loss: 1.4589 - val_loss: 1.4832 - 364ms/epoch - 21ms/step\n",
      "Epoch 56/3000\n",
      "17/17 - 0s - loss: 1.4557 - val_loss: 1.4842 - 401ms/epoch - 24ms/step\n",
      "Epoch 57/3000\n",
      "17/17 - 0s - loss: 1.4574 - val_loss: 1.4833 - 361ms/epoch - 21ms/step\n",
      "Epoch 58/3000\n",
      "17/17 - 0s - loss: 1.4592 - val_loss: 1.4903 - 340ms/epoch - 20ms/step\n",
      "Epoch 59/3000\n",
      "17/17 - 0s - loss: 1.4579 - val_loss: 1.4865 - 367ms/epoch - 22ms/step\n",
      "Epoch 60/3000\n",
      "17/17 - 0s - loss: 1.4554 - val_loss: 1.4872 - 404ms/epoch - 24ms/step\n",
      "Epoch 61/3000\n",
      "17/17 - 0s - loss: 1.4539 - val_loss: 1.4912 - 391ms/epoch - 23ms/step\n",
      "Epoch 62/3000\n",
      "17/17 - 0s - loss: 1.4550 - val_loss: 1.4925 - 360ms/epoch - 21ms/step\n",
      "Epoch 63/3000\n",
      "17/17 - 0s - loss: 1.4547 - val_loss: 1.4897 - 361ms/epoch - 21ms/step\n",
      "Epoch 64/3000\n",
      "17/17 - 0s - loss: 1.4551 - val_loss: 1.4921 - 361ms/epoch - 21ms/step\n",
      "Epoch 65/3000\n",
      "17/17 - 0s - loss: 1.4528 - val_loss: 1.4865 - 397ms/epoch - 23ms/step\n",
      "Epoch 66/3000\n",
      "17/17 - 0s - loss: 1.4536 - val_loss: 1.4957 - 355ms/epoch - 21ms/step\n",
      "Epoch 67/3000\n",
      "17/17 - 0s - loss: 1.4525 - val_loss: 1.4877 - 401ms/epoch - 24ms/step\n",
      "Epoch 68/3000\n",
      "17/17 - 0s - loss: 1.4502 - val_loss: 1.4884 - 408ms/epoch - 24ms/step\n",
      "Epoch 69/3000\n",
      "17/17 - 0s - loss: 1.4485 - val_loss: 1.4864 - 406ms/epoch - 24ms/step\n",
      "Epoch 70/3000\n",
      "17/17 - 0s - loss: 1.4486 - val_loss: 1.4915 - 370ms/epoch - 22ms/step\n",
      "Epoch 71/3000\n",
      "17/17 - 0s - loss: 1.4493 - val_loss: 1.4901 - 361ms/epoch - 21ms/step\n",
      "Epoch 72/3000\n",
      "17/17 - 0s - loss: 1.4493 - val_loss: 1.4970 - 353ms/epoch - 21ms/step\n",
      "Epoch 73/3000\n",
      "17/17 - 0s - loss: 1.4494 - val_loss: 1.4866 - 354ms/epoch - 21ms/step\n",
      "Epoch 74/3000\n",
      "17/17 - 0s - loss: 1.4511 - val_loss: 1.4913 - 364ms/epoch - 21ms/step\n",
      "Epoch 75/3000\n",
      "17/17 - 0s - loss: 1.4495 - val_loss: 1.4928 - 365ms/epoch - 21ms/step\n",
      "Epoch 76/3000\n",
      "17/17 - 0s - loss: 1.4485 - val_loss: 1.4969 - 351ms/epoch - 21ms/step\n",
      "Epoch 77/3000\n",
      "17/17 - 0s - loss: 1.4448 - val_loss: 1.4907 - 393ms/epoch - 23ms/step\n",
      "Epoch 78/3000\n",
      "17/17 - 0s - loss: 1.4443 - val_loss: 1.4948 - 404ms/epoch - 24ms/step\n",
      "Epoch 79/3000\n",
      "17/17 - 0s - loss: 1.4459 - val_loss: 1.4893 - 359ms/epoch - 21ms/step\n",
      "Epoch 80/3000\n",
      "17/17 - 0s - loss: 1.4449 - val_loss: 1.4890 - 357ms/epoch - 21ms/step\n",
      "Epoch 81/3000\n",
      "17/17 - 0s - loss: 1.4443 - val_loss: 1.4935 - 385ms/epoch - 23ms/step\n",
      "Epoch 82/3000\n",
      "17/17 - 0s - loss: 1.4436 - val_loss: 1.5000 - 373ms/epoch - 22ms/step\n",
      "Epoch 83/3000\n",
      "17/17 - 0s - loss: 1.4448 - val_loss: 1.4905 - 362ms/epoch - 21ms/step\n",
      "Epoch 84/3000\n",
      "17/17 - 0s - loss: 1.4471 - val_loss: 1.5090 - 372ms/epoch - 22ms/step\n",
      "Epoch 85/3000\n",
      "17/17 - 0s - loss: 1.4486 - val_loss: 1.4872 - 356ms/epoch - 21ms/step\n",
      "Epoch 86/3000\n",
      "17/17 - 0s - loss: 1.4440 - val_loss: 1.4924 - 369ms/epoch - 22ms/step\n",
      "Epoch 87/3000\n",
      "17/17 - 0s - loss: 1.4459 - val_loss: 1.4874 - 368ms/epoch - 22ms/step\n",
      "Epoch 88/3000\n",
      "17/17 - 0s - loss: 1.4423 - val_loss: 1.4943 - 415ms/epoch - 24ms/step\n",
      "Epoch 89/3000\n",
      "17/17 - 0s - loss: 1.4418 - val_loss: 1.4931 - 415ms/epoch - 24ms/step\n",
      "Epoch 90/3000\n",
      "17/17 - 0s - loss: 1.4418 - val_loss: 1.5003 - 349ms/epoch - 21ms/step\n",
      "Epoch 91/3000\n",
      "17/17 - 0s - loss: 1.4405 - val_loss: 1.5000 - 429ms/epoch - 25ms/step\n",
      "Epoch 92/3000\n",
      "17/17 - 0s - loss: 1.4420 - val_loss: 1.4976 - 357ms/epoch - 21ms/step\n",
      "Epoch 93/3000\n",
      "17/17 - 0s - loss: 1.4412 - val_loss: 1.4903 - 370ms/epoch - 22ms/step\n",
      "Epoch 94/3000\n",
      "17/17 - 0s - loss: 1.4399 - val_loss: 1.5041 - 408ms/epoch - 24ms/step\n",
      "Epoch 95/3000\n",
      "17/17 - 0s - loss: 1.4394 - val_loss: 1.4950 - 406ms/epoch - 24ms/step\n",
      "Epoch 96/3000\n",
      "17/17 - 0s - loss: 1.4404 - val_loss: 1.4925 - 347ms/epoch - 20ms/step\n",
      "Epoch 97/3000\n",
      "17/17 - 0s - loss: 1.4427 - val_loss: 1.4966 - 356ms/epoch - 21ms/step\n",
      "Epoch 98/3000\n",
      "17/17 - 0s - loss: 1.4438 - val_loss: 1.4897 - 352ms/epoch - 21ms/step\n",
      "Epoch 99/3000\n",
      "17/17 - 0s - loss: 1.4448 - val_loss: 1.4957 - 360ms/epoch - 21ms/step\n",
      "Epoch 100/3000\n",
      "17/17 - 0s - loss: 1.4414 - val_loss: 1.4975 - 367ms/epoch - 22ms/step\n",
      "Epoch 101/3000\n",
      "17/17 - 0s - loss: 1.4381 - val_loss: 1.4925 - 393ms/epoch - 23ms/step\n",
      "Epoch 102/3000\n",
      "17/17 - 0s - loss: 1.4385 - val_loss: 1.5010 - 358ms/epoch - 21ms/step\n",
      "Epoch 103/3000\n",
      "17/17 - 0s - loss: 1.4407 - val_loss: 1.4966 - 369ms/epoch - 22ms/step\n",
      "Epoch 104/3000\n",
      "17/17 - 0s - loss: 1.4405 - val_loss: 1.4881 - 366ms/epoch - 22ms/step\n",
      "Epoch 105/3000\n",
      "17/17 - 0s - loss: 1.4381 - val_loss: 1.4934 - 407ms/epoch - 24ms/step\n",
      "Epoch 106/3000\n",
      "17/17 - 0s - loss: 1.4383 - val_loss: 1.5032 - 363ms/epoch - 21ms/step\n",
      "Epoch 107/3000\n",
      "17/17 - 0s - loss: 1.4393 - val_loss: 1.4932 - 363ms/epoch - 21ms/step\n",
      "Epoch 108/3000\n",
      "17/17 - 0s - loss: 1.4386 - val_loss: 1.4968 - 355ms/epoch - 21ms/step\n",
      "Epoch 109/3000\n",
      "17/17 - 0s - loss: 1.4395 - val_loss: 1.4909 - 355ms/epoch - 21ms/step\n",
      "Epoch 110/3000\n",
      "17/17 - 0s - loss: 1.4395 - val_loss: 1.4932 - 350ms/epoch - 21ms/step\n",
      "Epoch 111/3000\n",
      "17/17 - 0s - loss: 1.4391 - val_loss: 1.4930 - 359ms/epoch - 21ms/step\n",
      "Epoch 112/3000\n",
      "17/17 - 0s - loss: 1.4369 - val_loss: 1.5053 - 392ms/epoch - 23ms/step\n",
      "Epoch 113/3000\n",
      "17/17 - 0s - loss: 1.4396 - val_loss: 1.4976 - 351ms/epoch - 21ms/step\n",
      "Epoch 114/3000\n",
      "17/17 - 0s - loss: 1.4378 - val_loss: 1.4883 - 358ms/epoch - 21ms/step\n",
      "Epoch 115/3000\n",
      "17/17 - 0s - loss: 1.4390 - val_loss: 1.4893 - 366ms/epoch - 22ms/step\n",
      "Epoch 116/3000\n",
      "17/17 - 0s - loss: 1.4419 - val_loss: 1.5091 - 353ms/epoch - 21ms/step\n",
      "Epoch 117/3000\n",
      "17/17 - 0s - loss: 1.4417 - val_loss: 1.4920 - 353ms/epoch - 21ms/step\n",
      "Epoch 118/3000\n",
      "17/17 - 0s - loss: 1.4380 - val_loss: 1.5001 - 352ms/epoch - 21ms/step\n",
      "Epoch 119/3000\n",
      "17/17 - 0s - loss: 1.4363 - val_loss: 1.4940 - 400ms/epoch - 24ms/step\n",
      "Epoch 120/3000\n",
      "17/17 - 0s - loss: 1.4344 - val_loss: 1.4918 - 398ms/epoch - 23ms/step\n",
      "Epoch 121/3000\n",
      "17/17 - 0s - loss: 1.4341 - val_loss: 1.4977 - 391ms/epoch - 23ms/step\n",
      "Epoch 122/3000\n",
      "17/17 - 0s - loss: 1.4334 - val_loss: 1.4941 - 382ms/epoch - 22ms/step\n",
      "Epoch 123/3000\n",
      "17/17 - 0s - loss: 1.4334 - val_loss: 1.4959 - 352ms/epoch - 21ms/step\n",
      "Epoch 124/3000\n",
      "17/17 - 0s - loss: 1.4351 - val_loss: 1.4946 - 350ms/epoch - 21ms/step\n",
      "Epoch 125/3000\n",
      "17/17 - 0s - loss: 1.4349 - val_loss: 1.4995 - 353ms/epoch - 21ms/step\n",
      "Epoch 126/3000\n",
      "17/17 - 0s - loss: 1.4396 - val_loss: 1.4935 - 354ms/epoch - 21ms/step\n",
      "Epoch 127/3000\n",
      "17/17 - 0s - loss: 1.4397 - val_loss: 1.4945 - 353ms/epoch - 21ms/step\n",
      "Epoch 128/3000\n",
      "17/17 - 0s - loss: 1.4358 - val_loss: 1.4903 - 351ms/epoch - 21ms/step\n",
      "Epoch 129/3000\n",
      "17/17 - 0s - loss: 1.4358 - val_loss: 1.4937 - 350ms/epoch - 21ms/step\n",
      "Epoch 130/3000\n",
      "17/17 - 0s - loss: 1.4328 - val_loss: 1.4951 - 389ms/epoch - 23ms/step\n",
      "Epoch 131/3000\n",
      "17/17 - 0s - loss: 1.4343 - val_loss: 1.4965 - 351ms/epoch - 21ms/step\n",
      "Epoch 132/3000\n",
      "17/17 - 0s - loss: 1.4320 - val_loss: 1.4949 - 389ms/epoch - 23ms/step\n",
      "Epoch 133/3000\n",
      "17/17 - 0s - loss: 1.4328 - val_loss: 1.4957 - 352ms/epoch - 21ms/step\n",
      "Epoch 134/3000\n",
      "17/17 - 0s - loss: 1.4317 - val_loss: 1.4910 - 390ms/epoch - 23ms/step\n",
      "Epoch 135/3000\n",
      "17/17 - 0s - loss: 1.4327 - val_loss: 1.5104 - 363ms/epoch - 21ms/step\n",
      "Epoch 136/3000\n",
      "17/17 - 0s - loss: 1.4371 - val_loss: 1.4970 - 366ms/epoch - 22ms/step\n",
      "Epoch 137/3000\n",
      "17/17 - 0s - loss: 1.4355 - val_loss: 1.4915 - 366ms/epoch - 22ms/step\n",
      "Epoch 138/3000\n",
      "17/17 - 0s - loss: 1.4334 - val_loss: 1.4944 - 366ms/epoch - 22ms/step\n",
      "Epoch 139/3000\n",
      "17/17 - 0s - loss: 1.4304 - val_loss: 1.4983 - 401ms/epoch - 24ms/step\n",
      "Epoch 140/3000\n",
      "17/17 - 0s - loss: 1.4310 - val_loss: 1.4925 - 364ms/epoch - 21ms/step\n",
      "Epoch 141/3000\n",
      "17/17 - 0s - loss: 1.4332 - val_loss: 1.4951 - 359ms/epoch - 21ms/step\n",
      "Epoch 142/3000\n",
      "17/17 - 0s - loss: 1.4350 - val_loss: 1.4914 - 361ms/epoch - 21ms/step\n",
      "Epoch 143/3000\n",
      "17/17 - 0s - loss: 1.4354 - val_loss: 1.4888 - 356ms/epoch - 21ms/step\n",
      "Epoch 144/3000\n",
      "17/17 - 0s - loss: 1.4341 - val_loss: 1.4904 - 352ms/epoch - 21ms/step\n",
      "Epoch 145/3000\n",
      "17/17 - 0s - loss: 1.4339 - val_loss: 1.4921 - 350ms/epoch - 21ms/step\n",
      "Epoch 146/3000\n",
      "17/17 - 0s - loss: 1.4316 - val_loss: 1.4863 - 351ms/epoch - 21ms/step\n",
      "Epoch 147/3000\n",
      "17/17 - 0s - loss: 1.4305 - val_loss: 1.4878 - 346ms/epoch - 20ms/step\n",
      "Epoch 148/3000\n",
      "17/17 - 0s - loss: 1.4269 - val_loss: 1.5034 - 400ms/epoch - 24ms/step\n",
      "Epoch 149/3000\n",
      "17/17 - 0s - loss: 1.4303 - val_loss: 1.4922 - 356ms/epoch - 21ms/step\n",
      "Epoch 150/3000\n",
      "17/17 - 0s - loss: 1.4278 - val_loss: 1.4883 - 353ms/epoch - 21ms/step\n",
      "Epoch 151/3000\n",
      "17/17 - 0s - loss: 1.4276 - val_loss: 1.5092 - 353ms/epoch - 21ms/step\n",
      "Epoch 152/3000\n",
      "17/17 - 0s - loss: 1.4345 - val_loss: 1.4999 - 351ms/epoch - 21ms/step\n",
      "Epoch 153/3000\n",
      "17/17 - 0s - loss: 1.4342 - val_loss: 1.5077 - 362ms/epoch - 21ms/step\n",
      "Epoch 154/3000\n",
      "17/17 - 0s - loss: 1.4359 - val_loss: 1.4971 - 359ms/epoch - 21ms/step\n",
      "Epoch 155/3000\n",
      "17/17 - 0s - loss: 1.4373 - val_loss: 1.5036 - 358ms/epoch - 21ms/step\n",
      "Epoch 156/3000\n",
      "17/17 - 0s - loss: 1.4376 - val_loss: 1.4885 - 365ms/epoch - 21ms/step\n",
      "Epoch 157/3000\n",
      "17/17 - 0s - loss: 1.4350 - val_loss: 1.4946 - 359ms/epoch - 21ms/step\n",
      "Epoch 158/3000\n",
      "17/17 - 0s - loss: 1.4310 - val_loss: 1.4930 - 351ms/epoch - 21ms/step\n",
      "Epoch 159/3000\n",
      "17/17 - 0s - loss: 1.4308 - val_loss: 1.4983 - 351ms/epoch - 21ms/step\n",
      "Epoch 160/3000\n",
      "17/17 - 0s - loss: 1.4288 - val_loss: 1.4999 - 360ms/epoch - 21ms/step\n",
      "Epoch 161/3000\n",
      "17/17 - 0s - loss: 1.4289 - val_loss: 1.5070 - 357ms/epoch - 21ms/step\n",
      "Epoch 162/3000\n",
      "17/17 - 0s - loss: 1.4262 - val_loss: 1.4899 - 397ms/epoch - 23ms/step\n",
      "Epoch 163/3000\n",
      "17/17 - 0s - loss: 1.4253 - val_loss: 1.4975 - 392ms/epoch - 23ms/step\n",
      "Epoch 164/3000\n",
      "17/17 - 0s - loss: 1.4260 - val_loss: 1.4897 - 355ms/epoch - 21ms/step\n",
      "Epoch 165/3000\n",
      "17/17 - 0s - loss: 1.4249 - val_loss: 1.4872 - 397ms/epoch - 23ms/step\n",
      "Epoch 166/3000\n",
      "17/17 - 0s - loss: 1.4267 - val_loss: 1.4935 - 353ms/epoch - 21ms/step\n",
      "Epoch 167/3000\n",
      "17/17 - 0s - loss: 1.4272 - val_loss: 1.4988 - 352ms/epoch - 21ms/step\n",
      "Epoch 168/3000\n",
      "17/17 - 0s - loss: 1.4229 - val_loss: 1.5032 - 384ms/epoch - 23ms/step\n",
      "Epoch 169/3000\n",
      "17/17 - 0s - loss: 1.4243 - val_loss: 1.5051 - 354ms/epoch - 21ms/step\n",
      "Epoch 170/3000\n",
      "17/17 - 0s - loss: 1.4268 - val_loss: 1.4965 - 352ms/epoch - 21ms/step\n",
      "Epoch 171/3000\n",
      "17/17 - 0s - loss: 1.4284 - val_loss: 1.4959 - 351ms/epoch - 21ms/step\n",
      "Epoch 172/3000\n",
      "17/17 - 0s - loss: 1.4325 - val_loss: 1.5015 - 351ms/epoch - 21ms/step\n",
      "Epoch 173/3000\n",
      "17/17 - 0s - loss: 1.4323 - val_loss: 1.4928 - 352ms/epoch - 21ms/step\n",
      "Epoch 174/3000\n",
      "17/17 - 0s - loss: 1.4271 - val_loss: 1.4873 - 352ms/epoch - 21ms/step\n",
      "Epoch 175/3000\n",
      "17/17 - 0s - loss: 1.4205 - val_loss: 1.4920 - 395ms/epoch - 23ms/step\n",
      "Epoch 176/3000\n",
      "17/17 - 0s - loss: 1.4235 - val_loss: 1.4950 - 352ms/epoch - 21ms/step\n",
      "Epoch 177/3000\n",
      "17/17 - 0s - loss: 1.4217 - val_loss: 1.4909 - 350ms/epoch - 21ms/step\n",
      "Epoch 178/3000\n",
      "17/17 - 0s - loss: 1.4208 - val_loss: 1.4952 - 351ms/epoch - 21ms/step\n",
      "Epoch 179/3000\n",
      "17/17 - 0s - loss: 1.4190 - val_loss: 1.4926 - 397ms/epoch - 23ms/step\n",
      "Epoch 180/3000\n",
      "17/17 - 0s - loss: 1.4211 - val_loss: 1.4965 - 363ms/epoch - 21ms/step\n",
      "Epoch 181/3000\n",
      "17/17 - 0s - loss: 1.4214 - val_loss: 1.4968 - 358ms/epoch - 21ms/step\n",
      "Epoch 182/3000\n",
      "17/17 - 0s - loss: 1.4198 - val_loss: 1.5041 - 360ms/epoch - 21ms/step\n",
      "Epoch 183/3000\n",
      "17/17 - 0s - loss: 1.4178 - val_loss: 1.4941 - 405ms/epoch - 24ms/step\n",
      "Epoch 184/3000\n",
      "17/17 - 0s - loss: 1.4184 - val_loss: 1.4950 - 363ms/epoch - 21ms/step\n",
      "Epoch 185/3000\n",
      "17/17 - 0s - loss: 1.4221 - val_loss: 1.4983 - 352ms/epoch - 21ms/step\n",
      "Epoch 186/3000\n",
      "17/17 - 0s - loss: 1.4193 - val_loss: 1.4934 - 349ms/epoch - 21ms/step\n",
      "Epoch 187/3000\n",
      "17/17 - 0s - loss: 1.4185 - val_loss: 1.4956 - 354ms/epoch - 21ms/step\n",
      "Epoch 188/3000\n",
      "17/17 - 0s - loss: 1.4179 - val_loss: 1.4998 - 350ms/epoch - 21ms/step\n",
      "Epoch 189/3000\n",
      "17/17 - 0s - loss: 1.4185 - val_loss: 1.4976 - 355ms/epoch - 21ms/step\n",
      "Epoch 190/3000\n",
      "17/17 - 0s - loss: 1.4203 - val_loss: 1.5082 - 351ms/epoch - 21ms/step\n",
      "Epoch 191/3000\n",
      "17/17 - 0s - loss: 1.4233 - val_loss: 1.4874 - 352ms/epoch - 21ms/step\n",
      "Epoch 192/3000\n",
      "17/17 - 0s - loss: 1.4246 - val_loss: 1.5061 - 356ms/epoch - 21ms/step\n",
      "Epoch 193/3000\n",
      "17/17 - 0s - loss: 1.4259 - val_loss: 1.4977 - 355ms/epoch - 21ms/step\n",
      "Epoch 194/3000\n",
      "17/17 - 0s - loss: 1.4237 - val_loss: 1.5137 - 357ms/epoch - 21ms/step\n",
      "Epoch 195/3000\n",
      "17/17 - 0s - loss: 1.4259 - val_loss: 1.4929 - 322ms/epoch - 19ms/step\n",
      "Epoch 196/3000\n",
      "17/17 - 0s - loss: 1.4243 - val_loss: 1.4973 - 353ms/epoch - 21ms/step\n",
      "Epoch 197/3000\n",
      "17/17 - 0s - loss: 1.4202 - val_loss: 1.4952 - 352ms/epoch - 21ms/step\n",
      "Epoch 198/3000\n",
      "17/17 - 0s - loss: 1.4196 - val_loss: 1.4892 - 351ms/epoch - 21ms/step\n",
      "Epoch 199/3000\n",
      "17/17 - 0s - loss: 1.4199 - val_loss: 1.4942 - 353ms/epoch - 21ms/step\n",
      "Epoch 200/3000\n",
      "17/17 - 0s - loss: 1.4225 - val_loss: 1.5014 - 351ms/epoch - 21ms/step\n",
      "Epoch 201/3000\n",
      "17/17 - 0s - loss: 1.4249 - val_loss: 1.4930 - 352ms/epoch - 21ms/step\n",
      "Epoch 202/3000\n",
      "17/17 - 0s - loss: 1.4243 - val_loss: 1.4892 - 352ms/epoch - 21ms/step\n",
      "Epoch 203/3000\n",
      "17/17 - 0s - loss: 1.4189 - val_loss: 1.4940 - 354ms/epoch - 21ms/step\n",
      "Epoch 204/3000\n",
      "17/17 - 0s - loss: 1.4187 - val_loss: 1.4968 - 353ms/epoch - 21ms/step\n",
      "Epoch 205/3000\n",
      "17/17 - 0s - loss: 1.4192 - val_loss: 1.4994 - 352ms/epoch - 21ms/step\n",
      "Epoch 206/3000\n",
      "17/17 - 0s - loss: 1.4207 - val_loss: 1.4954 - 360ms/epoch - 21ms/step\n",
      "Epoch 207/3000\n",
      "17/17 - 0s - loss: 1.4159 - val_loss: 1.4951 - 398ms/epoch - 23ms/step\n",
      "Epoch 208/3000\n",
      "17/17 - 0s - loss: 1.4140 - val_loss: 1.5010 - 393ms/epoch - 23ms/step\n",
      "Epoch 209/3000\n",
      "17/17 - 0s - loss: 1.4134 - val_loss: 1.5004 - 394ms/epoch - 23ms/step\n",
      "Epoch 210/3000\n",
      "17/17 - 0s - loss: 1.4162 - val_loss: 1.4942 - 353ms/epoch - 21ms/step\n",
      "Epoch 211/3000\n",
      "17/17 - 0s - loss: 1.4160 - val_loss: 1.5091 - 357ms/epoch - 21ms/step\n",
      "Epoch 212/3000\n",
      "17/17 - 0s - loss: 1.4158 - val_loss: 1.4984 - 355ms/epoch - 21ms/step\n",
      "Epoch 213/3000\n",
      "17/17 - 0s - loss: 1.4127 - val_loss: 1.4983 - 401ms/epoch - 24ms/step\n",
      "Epoch 214/3000\n",
      "17/17 - 0s - loss: 1.4134 - val_loss: 1.5038 - 351ms/epoch - 21ms/step\n",
      "Epoch 215/3000\n",
      "17/17 - 0s - loss: 1.4162 - val_loss: 1.5070 - 350ms/epoch - 21ms/step\n",
      "Epoch 216/3000\n",
      "17/17 - 0s - loss: 1.4179 - val_loss: 1.4985 - 355ms/epoch - 21ms/step\n",
      "Epoch 217/3000\n",
      "17/17 - 0s - loss: 1.4163 - val_loss: 1.4930 - 355ms/epoch - 21ms/step\n",
      "Epoch 218/3000\n",
      "17/17 - 0s - loss: 1.4162 - val_loss: 1.4928 - 359ms/epoch - 21ms/step\n",
      "Epoch 219/3000\n",
      "17/17 - 0s - loss: 1.4143 - val_loss: 1.5008 - 353ms/epoch - 21ms/step\n",
      "Epoch 220/3000\n",
      "17/17 - 0s - loss: 1.4143 - val_loss: 1.4970 - 349ms/epoch - 21ms/step\n",
      "Epoch 221/3000\n",
      "17/17 - 0s - loss: 1.4140 - val_loss: 1.4933 - 351ms/epoch - 21ms/step\n",
      "Epoch 222/3000\n",
      "17/17 - 0s - loss: 1.4153 - val_loss: 1.5073 - 353ms/epoch - 21ms/step\n",
      "Epoch 223/3000\n",
      "17/17 - 0s - loss: 1.4145 - val_loss: 1.4947 - 349ms/epoch - 21ms/step\n",
      "Epoch 224/3000\n",
      "17/17 - 0s - loss: 1.4124 - val_loss: 1.4977 - 394ms/epoch - 23ms/step\n",
      "Epoch 225/3000\n",
      "17/17 - 0s - loss: 1.4111 - val_loss: 1.4945 - 389ms/epoch - 23ms/step\n",
      "Epoch 226/3000\n",
      "17/17 - 0s - loss: 1.4145 - val_loss: 1.4934 - 350ms/epoch - 21ms/step\n",
      "Epoch 227/3000\n",
      "17/17 - 0s - loss: 1.4150 - val_loss: 1.5025 - 351ms/epoch - 21ms/step\n",
      "Epoch 228/3000\n",
      "17/17 - 0s - loss: 1.4128 - val_loss: 1.5029 - 353ms/epoch - 21ms/step\n",
      "Epoch 229/3000\n",
      "17/17 - 0s - loss: 1.4171 - val_loss: 1.4944 - 353ms/epoch - 21ms/step\n",
      "Epoch 230/3000\n",
      "17/17 - 0s - loss: 1.4176 - val_loss: 1.5017 - 352ms/epoch - 21ms/step\n",
      "Epoch 231/3000\n",
      "17/17 - 0s - loss: 1.4196 - val_loss: 1.4943 - 350ms/epoch - 21ms/step\n",
      "Epoch 232/3000\n",
      "17/17 - 0s - loss: 1.4195 - val_loss: 1.5031 - 351ms/epoch - 21ms/step\n",
      "Epoch 233/3000\n",
      "17/17 - 0s - loss: 1.4209 - val_loss: 1.4995 - 351ms/epoch - 21ms/step\n",
      "Epoch 234/3000\n",
      "17/17 - 0s - loss: 1.4151 - val_loss: 1.4945 - 352ms/epoch - 21ms/step\n",
      "Epoch 235/3000\n",
      "17/17 - 0s - loss: 1.4124 - val_loss: 1.4951 - 350ms/epoch - 21ms/step\n",
      "Epoch 236/3000\n",
      "17/17 - 0s - loss: 1.4144 - val_loss: 1.5003 - 351ms/epoch - 21ms/step\n",
      "Epoch 237/3000\n",
      "17/17 - 0s - loss: 1.4166 - val_loss: 1.4937 - 354ms/epoch - 21ms/step\n",
      "Epoch 238/3000\n",
      "17/17 - 0s - loss: 1.4129 - val_loss: 1.5066 - 354ms/epoch - 21ms/step\n",
      "Epoch 239/3000\n",
      "17/17 - 0s - loss: 1.4113 - val_loss: 1.4957 - 351ms/epoch - 21ms/step\n",
      "Epoch 240/3000\n",
      "17/17 - 0s - loss: 1.4095 - val_loss: 1.5056 - 396ms/epoch - 23ms/step\n",
      "Epoch 241/3000\n",
      "17/17 - 0s - loss: 1.4114 - val_loss: 1.5007 - 354ms/epoch - 21ms/step\n",
      "Epoch 242/3000\n",
      "17/17 - 0s - loss: 1.4112 - val_loss: 1.4963 - 350ms/epoch - 21ms/step\n",
      "Epoch 243/3000\n",
      "17/17 - 0s - loss: 1.4113 - val_loss: 1.4922 - 349ms/epoch - 21ms/step\n",
      "Epoch 244/3000\n",
      "17/17 - 0s - loss: 1.4071 - val_loss: 1.5000 - 397ms/epoch - 23ms/step\n",
      "Epoch 245/3000\n",
      "17/17 - 0s - loss: 1.4081 - val_loss: 1.4948 - 352ms/epoch - 21ms/step\n",
      "Epoch 246/3000\n",
      "17/17 - 0s - loss: 1.4071 - val_loss: 1.5083 - 354ms/epoch - 21ms/step\n",
      "Epoch 247/3000\n",
      "17/17 - 0s - loss: 1.4083 - val_loss: 1.5010 - 353ms/epoch - 21ms/step\n",
      "Epoch 248/3000\n",
      "17/17 - 0s - loss: 1.4073 - val_loss: 1.4979 - 348ms/epoch - 20ms/step\n",
      "Epoch 249/3000\n",
      "17/17 - 0s - loss: 1.4083 - val_loss: 1.5002 - 352ms/epoch - 21ms/step\n",
      "Epoch 250/3000\n",
      "17/17 - 0s - loss: 1.4090 - val_loss: 1.4917 - 351ms/epoch - 21ms/step\n",
      "Epoch 251/3000\n",
      "17/17 - 0s - loss: 1.4087 - val_loss: 1.5008 - 352ms/epoch - 21ms/step\n",
      "Epoch 252/3000\n",
      "17/17 - 0s - loss: 1.4105 - val_loss: 1.4990 - 340ms/epoch - 20ms/step\n",
      "Epoch 253/3000\n",
      "17/17 - 0s - loss: 1.4135 - val_loss: 1.5051 - 359ms/epoch - 21ms/step\n",
      "Epoch 254/3000\n",
      "17/17 - 0s - loss: 1.4126 - val_loss: 1.5044 - 364ms/epoch - 21ms/step\n",
      "Epoch 255/3000\n",
      "17/17 - 0s - loss: 1.4130 - val_loss: 1.4990 - 364ms/epoch - 21ms/step\n",
      "Epoch 256/3000\n",
      "17/17 - 0s - loss: 1.4141 - val_loss: 1.4920 - 361ms/epoch - 21ms/step\n",
      "Epoch 257/3000\n",
      "17/17 - 0s - loss: 1.4121 - val_loss: 1.4967 - 356ms/epoch - 21ms/step\n",
      "Epoch 258/3000\n",
      "17/17 - 0s - loss: 1.4104 - val_loss: 1.4928 - 354ms/epoch - 21ms/step\n",
      "Epoch 259/3000\n",
      "17/17 - 0s - loss: 1.4067 - val_loss: 1.5014 - 393ms/epoch - 23ms/step\n",
      "Epoch 260/3000\n",
      "17/17 - 0s - loss: 1.4057 - val_loss: 1.5085 - 392ms/epoch - 23ms/step\n",
      "Epoch 261/3000\n",
      "17/17 - 0s - loss: 1.4106 - val_loss: 1.5012 - 352ms/epoch - 21ms/step\n",
      "Epoch 262/3000\n",
      "17/17 - 0s - loss: 1.4126 - val_loss: 1.5222 - 349ms/epoch - 21ms/step\n",
      "Epoch 263/3000\n",
      "17/17 - 0s - loss: 1.4114 - val_loss: 1.4934 - 350ms/epoch - 21ms/step\n",
      "Epoch 264/3000\n",
      "17/17 - 0s - loss: 1.4080 - val_loss: 1.5044 - 353ms/epoch - 21ms/step\n",
      "Epoch 265/3000\n",
      "17/17 - 0s - loss: 1.4078 - val_loss: 1.4994 - 351ms/epoch - 21ms/step\n",
      "Epoch 266/3000\n",
      "17/17 - 0s - loss: 1.4073 - val_loss: 1.5012 - 350ms/epoch - 21ms/step\n",
      "Epoch 267/3000\n",
      "17/17 - 0s - loss: 1.4066 - val_loss: 1.5111 - 360ms/epoch - 21ms/step\n",
      "Epoch 268/3000\n",
      "17/17 - 0s - loss: 1.4059 - val_loss: 1.4958 - 350ms/epoch - 21ms/step\n",
      "Epoch 269/3000\n",
      "17/17 - 0s - loss: 1.4053 - val_loss: 1.5030 - 404ms/epoch - 24ms/step\n",
      "Epoch 270/3000\n",
      "17/17 - 0s - loss: 1.4060 - val_loss: 1.5043 - 352ms/epoch - 21ms/step\n",
      "Epoch 271/3000\n",
      "17/17 - 0s - loss: 1.4055 - val_loss: 1.5018 - 353ms/epoch - 21ms/step\n",
      "Epoch 272/3000\n",
      "17/17 - 0s - loss: 1.4047 - val_loss: 1.4989 - 395ms/epoch - 23ms/step\n",
      "Epoch 273/3000\n",
      "17/17 - 0s - loss: 1.4079 - val_loss: 1.5017 - 354ms/epoch - 21ms/step\n",
      "Epoch 274/3000\n",
      "17/17 - 0s - loss: 1.4076 - val_loss: 1.5011 - 350ms/epoch - 21ms/step\n",
      "Epoch 275/3000\n",
      "17/17 - 0s - loss: 1.4084 - val_loss: 1.4998 - 352ms/epoch - 21ms/step\n",
      "Epoch 276/3000\n",
      "17/17 - 0s - loss: 1.4087 - val_loss: 1.5025 - 352ms/epoch - 21ms/step\n",
      "Epoch 277/3000\n",
      "17/17 - 0s - loss: 1.4070 - val_loss: 1.5066 - 350ms/epoch - 21ms/step\n",
      "Epoch 278/3000\n",
      "17/17 - 0s - loss: 1.4093 - val_loss: 1.5061 - 349ms/epoch - 21ms/step\n",
      "Epoch 279/3000\n",
      "17/17 - 0s - loss: 1.4100 - val_loss: 1.4946 - 350ms/epoch - 21ms/step\n",
      "Epoch 280/3000\n",
      "17/17 - 0s - loss: 1.4100 - val_loss: 1.5118 - 353ms/epoch - 21ms/step\n",
      "Epoch 281/3000\n",
      "17/17 - 0s - loss: 1.4068 - val_loss: 1.4954 - 349ms/epoch - 21ms/step\n",
      "Epoch 282/3000\n",
      "17/17 - 0s - loss: 1.4111 - val_loss: 1.5029 - 351ms/epoch - 21ms/step\n",
      "Epoch 283/3000\n",
      "17/17 - 0s - loss: 1.4117 - val_loss: 1.5095 - 350ms/epoch - 21ms/step\n",
      "Epoch 284/3000\n",
      "17/17 - 0s - loss: 1.4090 - val_loss: 1.4999 - 351ms/epoch - 21ms/step\n",
      "Epoch 285/3000\n",
      "17/17 - 0s - loss: 1.4046 - val_loss: 1.4991 - 400ms/epoch - 24ms/step\n",
      "Epoch 286/3000\n",
      "17/17 - 0s - loss: 1.4050 - val_loss: 1.4958 - 348ms/epoch - 20ms/step\n",
      "Epoch 287/3000\n",
      "17/17 - 0s - loss: 1.4023 - val_loss: 1.4979 - 392ms/epoch - 23ms/step\n",
      "Epoch 288/3000\n",
      "17/17 - 0s - loss: 1.4017 - val_loss: 1.5091 - 390ms/epoch - 23ms/step\n",
      "Epoch 289/3000\n",
      "17/17 - 0s - loss: 1.4042 - val_loss: 1.5120 - 356ms/epoch - 21ms/step\n",
      "Epoch 290/3000\n",
      "17/17 - 0s - loss: 1.4056 - val_loss: 1.5059 - 347ms/epoch - 20ms/step\n",
      "Epoch 291/3000\n",
      "17/17 - 0s - loss: 1.4054 - val_loss: 1.4997 - 345ms/epoch - 20ms/step\n",
      "Epoch 292/3000\n",
      "17/17 - 0s - loss: 1.4015 - val_loss: 1.5021 - 393ms/epoch - 23ms/step\n",
      "Epoch 293/3000\n",
      "17/17 - 0s - loss: 1.4056 - val_loss: 1.5006 - 354ms/epoch - 21ms/step\n",
      "Epoch 294/3000\n",
      "17/17 - 0s - loss: 1.4077 - val_loss: 1.4929 - 352ms/epoch - 21ms/step\n",
      "Epoch 295/3000\n",
      "17/17 - 0s - loss: 1.4053 - val_loss: 1.4970 - 351ms/epoch - 21ms/step\n",
      "Epoch 296/3000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_304239/3698743716.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# mdl_nonlinear.load_weights('./temp_training-nonlinear.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m hist0 = mdl_nonlinear.fit(u_train[0,:,:,:,:], u_train[0,:,:,:,:],\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pat = 200 # EarlyStopping\n",
    "tempfn = './temp_training-nonlinear.h5'\n",
    "model_cb=ModelCheckpoint(tempfn, monitor='loss',save_best_only=True,verbose=0,save_weights_only=True)#val_loss\n",
    "early_cb=EarlyStopping(monitor='loss', patience=pat,verbose=0)\n",
    "cb = [model_cb, early_cb]\n",
    "\n",
    "# mdl_nonlinear.load_weights('./temp_training-nonlinear.h5')\n",
    "# Training\n",
    "hist0 = mdl_nonlinear.fit(u_train[0,:,:,:,:], u_train[0,:,:,:,:],\n",
    "                epochs=nb_epoch,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(u_val[0,:,:,:,:], u_val[0,:,:,:,:]),\n",
    "                callbacks=cb,verbose=2)  \n",
    "hist_train_3.extend(hist0.history['loss'])\n",
    "hist_val_3.extend(hist0.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.title('nonlinear')\n",
    "plt.plot(hist_train_3,label='train')\n",
    "plt.plot(hist_val_3,label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ae modes from training or testing\n",
    "which = 'train'\n",
    "\n",
    "\n",
    "mdl_nonlinear.load_weights('./temp_training-nonlinear.h5')\n",
    "latent_test_3 = mdl_nonlinear.encoder.predict(u_test[0,:,:,:,:])\n",
    "latent_train_3 = mdl_nonlinear.encoder.predict(u_train[0,:,:,:,:])\n",
    "nonlinear_decoders_trained = []\n",
    "for name in mdl_nonlinear.name_decoder:\n",
    "    nonlinear_decoders_trained.append(mdl_nonlinear.get_layer(name))\n",
    "nonlinear_modes_trained = []\n",
    "\n",
    "if which == 'train':\n",
    "    z = latent_train_3\n",
    "    u_in = u_train[0,:,:,:,:]\n",
    "elif which == 'test':\n",
    "    z = latent_test_3\n",
    "    u_in = u_test[0,:,:,:,:]\n",
    "\n",
    "for i in range(latent_dim):\n",
    "    nonlinear_modes_trained.append(nonlinear_decoders_trained[i].predict(np.reshape(z[:,i],(-1,1))))\n",
    "nonlinear_modes_trained = np.array(nonlinear_modes_trained)\n",
    "y_test_sum = np.sum(nonlinear_modes_trained,axis=0)\n",
    "    \n",
    "y_test = mdl_nonlinear.predict(u_in)\n",
    "\n",
    "print('Are results calculated the two ways the same?',np.array_equal(y_test_sum,y_test))\n",
    "loss_test_1 = mdl_nonlinear.evaluate(u_in,u_in,verbose=0)\n",
    "loss_test_2 = tf.keras.losses.MeanSquaredError()(u_in,y_test)\n",
    "print('total loss:',loss_test_1,'    mse loss:',loss_test_2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(latent_dim):\n",
    "    # plt.plot(latent_test_3[:,i],label=str(i+1))\n",
    "    plt.plot(latent_train_3[:,i],label=str(i+1))\n",
    "plt.xlim([0,600])\n",
    "plt.title('latent variables after training with nonlinear ae')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get psd of latent variables\n",
    "from matplotlib import mlab\n",
    "plt.figure(figsize=(10,5))\n",
    "for which_a in range(latent_dim):\n",
    "    pxx, freqs = mlab.psd(latent_train_3[:,which_a]/(np.var(latent_train_3[:,which_a])**0.5),Fs=f_piv,NFFT=Ntrain)\n",
    "    st = ((D/1000)/U_inf)*freqs\n",
    "    plt.semilogx(st,pxx,label=\"$z_{%i}/\\sigma_{%i}$\"%(which_a+1,which_a+1),linewidth=1)    \n",
    "    pxx, freqs = mlab.psd(A_data[:,which_a]/(lam_data[which_a]**0.5),Fs=f_piv,NFFT=Ntrain)\n",
    "    plt.semilogx(st,pxx,label=\"$a_{%i}/\\sqrt{\\lambda_{%i}}$\"%(which_a+1,which_a+1),linewidth=1)\n",
    "plt.xlabel(\"$St_D$\")\n",
    "plt.ylabel(\"$P_{aa}$\")\n",
    "plt.grid(which='both')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject autoencoder modes onto data pod modes\n",
    "lam_modes = mode_eval.equivalent_pca_energy(nonlinear_modes_trained,Q_POD_data)\n",
    "lam_modes_percent = lam_modes/lam_data\n",
    "\n",
    "x_axis = np.arange(1, Nz*Ny*Nu+1)\n",
    "\n",
    "plt.figure()\n",
    "for i in range(latent_dim):\n",
    "    plt.plot(x_axis,lam_modes_percent[i,:]*100,label='AE mode '+str(i+1),linestyle='--',marker='x')\n",
    "plt.xlim([0,10])\n",
    "plt.ylim([0,100])\n",
    "plt.xticks(range(10))\n",
    "plt.legend()\n",
    "plt.xlabel('POD mode')\n",
    "plt.ylabel('% equivalent PCA energy')\n",
    "plt.title('precentage of POD modes captured')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare PSD\n",
    "which_z = 1\n",
    "which_a = 0\n",
    "plt.figure(figsize=(10,5))\n",
    "# latent\n",
    "pxx, freqs = mlab.psd(latent_train_3[:,which_z]/(np.var(latent_train_3[:,which_z])**0.5),Fs=f_piv,NFFT=Ntrain)\n",
    "st = ((D/1000)/U_inf)*freqs\n",
    "plt.semilogx(st,pxx,'k',label=\"$z_{%i}/\\sigma_{%i}$\"%(which_z+1,which_z+1),linewidth=1)\n",
    "# time coeff\n",
    "paa, freqs = mlab.psd(A_data[:,which_a]/(lam_data[which_a]**0.5),Fs=f_piv,NFFT=Ntrain)\n",
    "plt.semilogx(st,paa,'k--',label=\"$a_{%i}/\\sqrt{\\lambda_{%i}}$\"%(which_a+1,which_a+1),linewidth=1)\n",
    "plt.xlabel(\"$St_D$\")\n",
    "plt.ylabel(\"$P_{aa}$\")\n",
    "plt.grid(which='both')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_3 = z.T@z/(z.shape[0]-1)\n",
    "# cov_3 = np.cov(z.T)\n",
    "vmax = np.max(cov_3)\n",
    "plt.figure()\n",
    "plt.title('nonlinear after training')\n",
    "plt.imshow(cov_3,cmap='seismic',vmin=-vmax,vmax=vmax,extent=[0.5,latent_dim+0.5,latent_dim+0.5,0.5]) # vmin=-1,vmax=1,\n",
    "plt.xticks(np.arange(latent_dim)+1)\n",
    "plt.yticks(np.arange(latent_dim)+1)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "sort_idx = np.argsort(np.diag(cov_3))\n",
    "rank_var_3 = np.arange(1,latent_dim+1)[np.flip(sort_idx)]\n",
    "print('variance:',np.diag(cov_3),' rank:',rank_var_3)\n",
    "\n",
    "# compare two variables\n",
    "pltz = [0,1]\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(z[:,pltz[0]],z[:,pltz[1]],alpha=0.5)\n",
    "plt.xlabel(\"latent variable %i\"%(pltz[0]+1))\n",
    "plt.ylabel(\"latent variable %i\"%(pltz[1]+1))\n",
    "# plt.xlim([-1,1])\n",
    "# plt.ylim([-1,1])\n",
    "# plt.plot([0,-0.89021688],[0,-0.45553694],linewidth=3,color='k',label='new eigenvectors')\n",
    "# plt.plot([0,0.45553694],[0,-0.89021688],linewidth=3,color='k')\n",
    "# plt.plot([0,0.45391235],[0,0.89104634],linewidth=3,color='r',label='old eigenvectors')\n",
    "# plt.plot([0,-0.89104634],[0,0.45391235],linewidth=3,color='r')\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "\n",
    "det_3 = np.linalg.det(np.corrcoef(z.T))\n",
    "print('determinant of corr matrix: ',det_3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_1 = md.POD(z.T,method='classic')\n",
    "m_1,lam_1 = pod_1.get_modes()\n",
    "print(m_1,lam_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,5,figsize=(12,2))\n",
    "fig.suptitle('nonlinear ae modes after training')\n",
    "ax[0].set_ylabel('v')\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    if i < len(nonlinear_decoders):\n",
    "        m = nonlinear_decoders[i].predict(np.reshape(1,(1,1)))\n",
    "        im = ax.imshow(m[0,:,:,0],'jet')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(str(i+1))\n",
    "        ax_divider = make_axes_locatable(ax)\n",
    "        cax = ax_divider.append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
    "        plt.colorbar(im,cax=cax)\n",
    "plt.tight_layout()\n",
    "        \n",
    "\n",
    "fig,ax = plt.subplots(1,5,figsize=(12,2))\n",
    "fig.suptitle('nonlinear ae modes after training')\n",
    "ax[0].set_ylabel('w')\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    if i < len(nonlinear_decoders):\n",
    "        m = nonlinear_decoders[i].predict(np.reshape(1,(1,1)))\n",
    "        im = ax.imshow(m[0,:,:,1],'jet')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(str(i+1))\n",
    "        ax_divider = make_axes_locatable(ax)\n",
    "        cax = ax_divider.append_axes(\"right\", size=\"5%\", pad=\"2%\")\n",
    "        plt.colorbar(im,cax=cax)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_modes = []\n",
    "for i in range(latent_dim):\n",
    "    ae_modes.append(nonlinear_decoders[i].predict(z[:,[i]]))\n",
    "ae_modes = np.array(ae_modes)\n",
    "fig2,ax2 = plt.subplots(1,latent_dim,sharey='all')\n",
    "fig2.suptitle('modes (changing with time),v')\n",
    "for i in range(latent_dim):\n",
    "    im2 = ax2[i].imshow(ae_modes[i,0,:,:,0],'jet')\n",
    "    div = make_axes_locatable(ax2[i])\n",
    "    cax = div.append_axes('right',size='5%',pad='2%')\n",
    "    plt.colorbar(im2,cax=cax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decompose ae modes\n",
    "which_decoder = 1-1\n",
    "PlotWhichVelocity = 'v' \n",
    "save_img = False\n",
    "vy = nonlinear_modes_trained[which_decoder,:,:,:,0]\n",
    "vy = np.transpose(vy,[1,2,0])\n",
    "vz = nonlinear_modes_trained[which_decoder,:,:,:,1]\n",
    "vz = np.transpose(vz,[1,2,0])\n",
    "X = np.vstack((vz,vy)) # new shape [2*ny,nz,nt]\n",
    "# POD\n",
    "pod = md.POD(X)\n",
    "Q_POD,lam = pod.get_modes()\n",
    "Q_mean = pod.Q_mean\n",
    "\n",
    "\n",
    "## plot POD\n",
    "fig1 = plt.figure(figsize=(10, 3))\n",
    "title = \"POD Modes in decoder \" + str(which_decoder+1)\n",
    "plt.suptitle(title)\n",
    "\n",
    "# v\n",
    "grid0 = ImageGrid(fig1,211,share_all=True,nrows_ncols=(1, 5),axes_pad=0.4,label_mode=\"1\",cbar_location=\"right\",cbar_mode=\"each\",cbar_size=\"7%\",cbar_pad=\"3%\")\n",
    "grid0.axes_llc.set_xticks([])\n",
    "grid0.axes_llc.set_yticks([])\n",
    "grid0.axes_llc.set_ylabel('v')\n",
    "iphi = 0\n",
    "for ax, cax in zip(grid0, grid0.cbar_axes):\n",
    "    pltV = Q_POD[:,iphi];\n",
    "    pltV = np.reshape(pltV,[2*Ny,Nz])\n",
    "    im = ax.imshow(pltV[Ny:,:],'jet')\n",
    "    cb = cax.colorbar(im)\n",
    "    iphi += 1\n",
    "# w\n",
    "grid1 = ImageGrid(fig1,212,share_all=True,nrows_ncols=(1, 5),axes_pad=0.4,label_mode=\"1\",cbar_location=\"right\",cbar_mode=\"each\",cbar_size=\"7%\",cbar_pad=\"3%\")\n",
    "grid1.axes_llc.set_xticks([])\n",
    "grid1.axes_llc.set_yticks([])\n",
    "grid1.axes_llc.set_ylabel('w')\n",
    "iphi = 0\n",
    "for ax, cax in zip(grid1, grid1.cbar_axes):\n",
    "    pltV = Q_POD[:,iphi];\n",
    "    pltV = np.reshape(pltV,[2*Ny,Nz])\n",
    "    im = ax.imshow(pltV[0:Ny,:],'jet')\n",
    "    cb = cax.colorbar(im)\n",
    "    iphi += 1\n",
    "\n",
    "\n",
    "## Spectrum\n",
    "energy_1 = np.cumsum(lam/np.sum(lam))\n",
    "print('spectrum (first 5): ', energy_1[:15])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decompose ae modes\n",
    "which_decoder = 2-1\n",
    "PlotWhichVelocity = 'v' \n",
    "save_img = False\n",
    "vy = nonlinear_modes_trained[which_decoder,:,:,:,0]\n",
    "vy = np.transpose(vy,[1,2,0])\n",
    "vz = nonlinear_modes_trained[which_decoder,:,:,:,1]\n",
    "vz = np.transpose(vz,[1,2,0])\n",
    "X = np.vstack((vz,vy)) # new shape [2*ny,nz,nt]\n",
    "# POD\n",
    "pod = md.POD(X)\n",
    "Q_POD,lam = pod.get_modes()\n",
    "Q_mean = pod.Q_mean\n",
    "\n",
    "\n",
    "## plot POD\n",
    "fig1 = plt.figure(figsize=(10, 3))\n",
    "title = \"POD Modes in decoder \" + str(which_decoder+1)\n",
    "plt.suptitle(title)\n",
    "\n",
    "# v\n",
    "grid0 = ImageGrid(fig1,211,share_all=True,nrows_ncols=(1, 5),axes_pad=0.4,label_mode=\"1\",cbar_location=\"right\",cbar_mode=\"each\",cbar_size=\"7%\",cbar_pad=\"3%\")\n",
    "grid0.axes_llc.set_xticks([])\n",
    "grid0.axes_llc.set_yticks([])\n",
    "grid0.axes_llc.set_ylabel('v')\n",
    "iphi = 0\n",
    "for ax, cax in zip(grid0, grid0.cbar_axes):\n",
    "    pltV = Q_POD[:,iphi];\n",
    "    pltV = np.reshape(pltV,[2*Ny,Nz])\n",
    "    im = ax.imshow(pltV[Ny:,:],'jet')\n",
    "    cb = cax.colorbar(im)\n",
    "    iphi += 1\n",
    "# w\n",
    "grid1 = ImageGrid(fig1,212,share_all=True,nrows_ncols=(1, 5),axes_pad=0.4,label_mode=\"1\",cbar_location=\"right\",cbar_mode=\"each\",cbar_size=\"7%\",cbar_pad=\"3%\")\n",
    "grid1.axes_llc.set_xticks([])\n",
    "grid1.axes_llc.set_yticks([])\n",
    "grid1.axes_llc.set_ylabel('w')\n",
    "iphi = 0\n",
    "for ax, cax in zip(grid1, grid1.cbar_axes):\n",
    "    pltV = Q_POD[:,iphi];\n",
    "    pltV = np.reshape(pltV,[2*Ny,Nz])\n",
    "    im = ax.imshow(pltV[0:Ny,:],'jet')\n",
    "    cb = cax.colorbar(im)\n",
    "    iphi += 1\n",
    "\n",
    "\n",
    "## Spectrum\n",
    "energy_2 = np.cumsum(lam/np.sum(lam))\n",
    "print('spectrum (first 5): ', energy_2[:8])\n",
    "plt.figure()\n",
    "plt.plot(energy_1,label='1')\n",
    "plt.plot(energy_2,label='2')\n",
    "plt.title('spectrum')\n",
    "plt.xlim([0,29])\n",
    "plt.ylim([0.4,1.05])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mse loss ae mode 1:',tf.keras.losses.MeanSquaredError()(u_in,nonlinear_modes_trained[0,:,:,:,:]).numpy())\n",
    "print('mse loss ae mode 2:',tf.keras.losses.MeanSquaredError()(u_in,nonlinear_modes_trained[1,:,:,:,:]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse loss\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "modes_E = np.copy(nonlinear_modes_trained)\n",
    "rank_mse = np.zeros(latent_dim,dtype='int')-1\n",
    "for j in range(latent_dim):\n",
    "    E_ref = 0\n",
    "    for i in range(latent_dim):\n",
    "        if i not in rank_mse:\n",
    "            E = 1/mse(u_in,modes_E[i]).numpy()\n",
    "            if E > E_ref:\n",
    "                E_ref = E\n",
    "                rank_mse[j] = i\n",
    "    modes_E = nonlinear_modes_trained + modes_E[rank_mse[j]]\n",
    "rank_mse += 1\n",
    "\n",
    "i_mse = []\n",
    "for i in range(latent_dim):\n",
    "    i_mse.append(mse(u_in,nonlinear_modes_trained[i,:,:,:,:]).numpy())\n",
    "sort_idx = np.argsort(i_mse)\n",
    "rank_i_mse = np.arange(1,latent_dim+1)[sort_idx]\n",
    "\n",
    "\n",
    "ke = np.sum((nonlinear_modes_trained**2),axis=(4,3,2,1))\n",
    "sort_idx = np.argsort(ke)\n",
    "rank_ke = np.arange(1,latent_dim+1)[np.flip(sort_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of POD modes to reach 99% energy u_mean_trainD modes\n",
    "a = []\n",
    "to99 = np.zeros(latent_dim,dtype='int')\n",
    "for i in range(latent_dim):\n",
    "    vy = nonlinear_modes_trained[i,:,:,:,0].astype('float64')\n",
    "    vy = np.transpose(vy,[1,2,0])\n",
    "    vz = nonlinear_modes_trained[i,:,:,:,1].astype('float64')\n",
    "    vz = np.transpose(vz,[1,2,0]) #(ny,nz,nt)\n",
    "    X_99 = np.vstack((vz,vy)) # new shape [2*ny,nz,nt]\n",
    "    pod_99 = md.POD(X_99,method='classic')\n",
    "    Q_POD_99,lam_99 = pod_99.get_modes()\n",
    "    energy = np.cumsum(lam_99/np.sum(lam_99))\n",
    "    a.append(energy[0])\n",
    "    for j in range(len(energy)):\n",
    "        if energy[j] >= 0.9999:\n",
    "            to99[i] = j+1\n",
    "            break\n",
    "print(to99)\n",
    "sort_idx = np.argsort(to99)\n",
    "rank_to99 = np.arange(1,latent_dim+1)[np.flip(sort_idx)]\n",
    "print(rank_to99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POD of data\n",
    "vy = u_train[0,:,:,:,0] + u_mean_train[:,:,0]\n",
    "vy = np.transpose(vy,[1,2,0])\n",
    "vz = u_train[0,:,:,:,1] + u_mean_train[:,:,1]\n",
    "vz = np.transpose(vz,[1,2,0])\n",
    "X = np.vstack((vz,vy))\n",
    "\n",
    "pod_data = md.POD(X,method='classic')\n",
    "Q_POD_data,lam_data = pod_data.get_modes()\n",
    "total_energy = np.sum(lam_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear energy\n",
    "a = []\n",
    "lam_modes = []\n",
    "for i in range(latent_dim):\n",
    "    vy = nonlinear_modes_trained[i,:,:,:,0].astype('float64')\n",
    "    vy = np.transpose(vy,[1,2,0])\n",
    "    vz = nonlinear_modes_trained[i,:,:,:,1].astype('float64')\n",
    "    vz = np.transpose(vz,[1,2,0])\n",
    "    X_mode = np.vstack((vz,vy))\n",
    "    nt = X_mode.shape[-1]\n",
    "    X_mode = np.reshape(X_mode,(-1,nt))\n",
    "    A = X_mode.T @ Q_POD_data #(nt,nx)\n",
    "    lam_1 = np.sum(np.square(A),axis=0)/(nt-1)\n",
    "    lam_modes.append(lam_1)\n",
    "\n",
    "lam_modes = np.array(lam_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "for i in range(latent_dim):\n",
    "    plt.plot(lam_modes[i,:],label=i+1,linestyle='none',marker='x')\n",
    "plt.plot(lam_data,label='data')\n",
    "plt.xlim([-1,20])\n",
    "plt.ylim(bottom=0)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum of square of elements in a ae mode\n",
    "for i in range(latent_dim):\n",
    "    m = nonlinear_decoders_trained[i].predict(np.reshape(1,(1,1)))\n",
    "    ke = np.sum(m[0,:,:,:]**2)\n",
    "    print(ke/2933)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rank_variance:       ',rank_var_3)\n",
    "print('rank_mse:            ',rank_mse)\n",
    "print('rank_i_mse:          ',rank_i_mse)\n",
    "print('rank_to99:           ',rank_to99)\n",
    "print('rank_ke:             ',rank_ke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save to wandb\n",
    "config_wandb = {'latent_dim':latent_dim}\n",
    "run = wandb.init(config=config_wandb,project=\"MD-CNN-AE\",group='experiments',name='2-mode')\n",
    "with run:\n",
    "    for i in range(1008):\n",
    "        run.log({\"pod1\":lam_modes[rank_var[0]-1,i]/np.max(lam_modes[rank_var[0]-1,:]),\"pod2\":lam_modes[rank_var[1]-1,i]/np.max(lam_modes[rank_var[1]-1,:])})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('MD-CNN-AE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f85e8d5ca1c7d62daa514db97b690def00c9e189bf201b0a2c929de67d960fcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
