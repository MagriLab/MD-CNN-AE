{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Physical GPUs, 1 Logical GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 12:02:15.116427: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-19 12:02:16.762393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1024 MB memory:  -> device: 2, name: Quadro RTX 8000, pci bus id: 0000:a6:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras import backend as K\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# from MD_AE_tools.models.models import *\n",
    "from MD_AE_tools.models.models_no_bias import *\n",
    "import MD_AE_tools.mode_decomposition as md\n",
    "import MD_AE_tools.ae_mode_evaluation as mode_eval\n",
    "\n",
    "import myplot\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from numpy import einsum\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "import time\n",
    "import os\n",
    "import configparser\n",
    "import datetime\n",
    "import wandb\n",
    "\n",
    "# get system information\n",
    "config = configparser.ConfigParser()\n",
    "config.read('_system.ini')\n",
    "system_info = config['system_info']\n",
    "\n",
    "# use gpu\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[2], 'GPU')# use [] for cpu only, gpus[i] for the ith gpu\n",
    "        tf.config.set_logical_device_configuration(gpus[2],[tf.config.LogicalDeviceConfiguration(memory_limit=1024)]) # set hard memory limit\n",
    "        # tf.config.experimental.set_memory_growth(gpus[0], True) # allow memory growth\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data_file = './data/PIV4_downsampled_by8.h5'\n",
    "Ntrain = 1632 # snapshots for training\n",
    "Nval = 550 # sanpshots for validation\n",
    "Ntest = 550\n",
    "\n",
    "# Boolean \n",
    "LATENT_STATE = True # save latent state\n",
    "SHUFFLE = True # shuffle before splitting into sets, test set is extracted before shuffling\n",
    "REMOVE_MEAN = True # train on fluctuating velocity\n",
    "\n",
    "## ae configuration\n",
    "lmb = 0.000 #1e-05 #regulariser\n",
    "drop_rate = 0.0\n",
    "features_layers = [32, 64, 128]\n",
    "latent_dim = 2\n",
    "act_fct = 'linear'\n",
    "resize_meth = 'bilinear'\n",
    "filter_window= (5,5)\n",
    "batch_norm = False\n",
    "\n",
    "## training\n",
    "nb_epoch = 3000\n",
    "batch_size = 200\n",
    "learning_rate = 0.001\n",
    "\n",
    "Nz = 24 # grid size\n",
    "Ny = 21\n",
    "Nu = 2\n",
    "Nt = 2732 # number of snapshots available\n",
    "D = 196.5 # mm diameter of bluff body\n",
    "U_inf = 15 # m/s freestream velocity\n",
    "f_piv = 720.0 # Hz PIV sampling frequency  \n",
    "dt = 1.0/f_piv \n",
    "Nx = [Ny, Nz]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================= READ DATA FROM FILE ================================\n",
    "hf = h5py.File('./data/ufluc_shuffle_1632.h5','r')\n",
    "u_all = np.array(hf.get('u_all'))\n",
    "u_train = np.array(hf.get('u_train'))\n",
    "u_val = np.array(hf.get('u_val'))\n",
    "u_test = np.array(hf.get('u_test'))\n",
    "u_mean_all = np.array(hf.get('u_mean_all'))\n",
    "u_mean_train = np.array(hf.get('u_mean_train'))\n",
    "u_mean_val = np.array(hf.get('u_mean_val'))\n",
    "u_mean_test = np.array(hf.get('u_mean_test'))\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating POD ...\n",
      "User has selected classic POD\n",
      "POD done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ym917/Codes/MD-CNN-AE/MD_AE_tools/mode_decomposition.py:220: RuntimeWarning: invalid value encountered in sqrt\n",
      "  normQ = (Q_POD.T @ Q_POD*self.w).real**0.5\n"
     ]
    }
   ],
   "source": [
    "# POD data\n",
    "x = einsum('t y z u -> y z t u',np.squeeze(u_train))\n",
    "X = np.vstack((x[:,:,:,0],x[:,:,:,1]))\n",
    "pod_data = md.POD(X,method='classic')\n",
    "Q_POD_data,lam_data = pod_data.get_modes\n",
    "Q_mean = pod_data.Q_mean\n",
    "recons_data = pod_data.reconstruct(latent_dim,shape=[2,Ny,Nz,u_train.shape[1]])\n",
    "recons_data = np.transpose(recons_data,[3,1,2,0])\n",
    "\n",
    "A_data = pod_data.get_time_coefficient\n",
    "pod_modes_t = []\n",
    "for i in range(latent_dim):\n",
    "    Q_add = pod_data.Phi[:,[i]] @ A_data[:,[i]].T\n",
    "    rebuildv = np.reshape(Q_add,[2,Ny,Nz,A_data.shape[0]])\n",
    "    pod_modes_t.append(rebuildv)\n",
    "pod_modes_t = np.array(pod_modes_t) # [latent_dim,velocity,Ny,Nz,time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating POD ...\n",
      "User has selected classic POD\n",
      "POD done.\n"
     ]
    }
   ],
   "source": [
    "# prepare validation data\n",
    "vy = u_val[0,:,:,:,0] + u_mean_val[:,:,0]\n",
    "vy = np.transpose(vy,[1,2,0])\n",
    "vz = u_val[0,:,:,:,1] + u_mean_val[:,:,1]\n",
    "vz = np.transpose(vz,[1,2,0])\n",
    "X = np.vstack((vz,vy))\n",
    "\n",
    "pod_val = md.POD(X,method='classic')\n",
    "Q_POD_val,lam_val = pod_val.get_modes\n",
    "A_val = pod_val.get_time_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(Nx,Nu,features_layers,latent_dim,filter_window,act_fct='tanh',batch_norm=batch_norm,drop_rate=drop_rate,lmb=lmb)\n",
    "layer_size = encoder.get_layer_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"decoder\" (type Decoder).\n\nin user code:\n\n    File \"/home/ym917/Codes/MD-CNN-AE/MD_AE_tools/models/models_no_bias.py\", line 126, in call  *\n        x = layer(x)\n    File \"/home/ym917/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/ym917/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 247, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"dense_1\" is incompatible with the layer: expected axis -1of input shape to have value 1, but received input with shape (None, 2)\n\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 2), dtype=float32)\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1293028/4143323880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilter_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact_fct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlmb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"decoder\" (type Decoder).\n\nin user code:\n\n    File \"/home/ym917/Codes/MD-CNN-AE/MD_AE_tools/models/models_no_bias.py\", line 126, in call  *\n        x = layer(x)\n    File \"/home/ym917/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/ym917/miniconda3/envs/MD-CNN-AE/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 247, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"dense_1\" is incompatible with the layer: expected axis -1of input shape to have value 1, but received input with shape (None, 2)\n\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 2), dtype=float32)\n  • training=False"
     ]
    }
   ],
   "source": [
    "### single decoder\n",
    "\n",
    "decoders = []\n",
    "add_modes = []\n",
    "inn = Input(shape=(latent_dim))\n",
    "for i in range(latent_dim):\n",
    "    decoders.append(\n",
    "        Decoder(Nx=Nx,Nu=Nu,layer_size=layer_size,features_layers=features_layers,latent_dim=1,filter_window=filter_window,act_fct='tanh',batch_norm=batch_norm,drop_rate=drop_rate,lmb=lmb)\n",
    "        )\n",
    "out = decoders[0](inn)\n",
    "mdl = Model(inn,out)\n",
    "print(mdl.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 1)            0           ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " decoder_2 (Decoder)            (None, 21, 24, 2)    258752      ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " decoder_3 (Decoder)            (None, 21, 24, 2)    258752      ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 21, 24, 2)    0           ['decoder_2[0][0]',              \n",
      "                                                                  'decoder_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 517,504\n",
      "Trainable params: 517,504\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# set up network\n",
    "# sum(decoders) = flow\n",
    "decoders = []\n",
    "add_modes = []\n",
    "inn = Input(shape=(latent_dim))\n",
    "for i in range(latent_dim):\n",
    "    decoders.append(\n",
    "        Decoder(Nx=Nx,Nu=Nu,layer_size=layer_size,features_layers=features_layers,latent_dim=1,filter_window=filter_window,act_fct='tanh',batch_norm=batch_norm,drop_rate=drop_rate,lmb=lmb)\n",
    "        )\n",
    "    x = Lambda(lambda x,i: x[:,i:i+1],arguments={'i':i})(inn)\n",
    "    x = decoders[i](x)\n",
    "    add_modes.append(x)\n",
    "    del x\n",
    "out = Add()(add_modes)\n",
    "mdl = Model(inn,out)\n",
    "print(mdl.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.compile(optimizer=Adam(learning_rate=learning_rate),loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training control\n",
    "pat = 200 # early stopping\n",
    "tempfn = './temp_decoder.h5'\n",
    "model_cb=ModelCheckpoint(tempfn, monitor='loss',save_best_only=True,verbose=1,save_weights_only=True)\n",
    "early_cb=EarlyStopping(monitor='loss', patience=pat,verbose=1)\n",
    "cb = [model_cb, early_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear history\n",
    "hist_train = []\n",
    "hist_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify input\n",
    "A_train_in = A_data[:,:2]\n",
    "A_val_in = A_val[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(A_train_in)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-19 12:02:57.536467: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - ETA: 0s - loss: 2.2390\n",
      "Epoch 00001: loss improved from inf to 2.23898, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 7s 84ms/step - loss: 2.2390 - val_loss: 1.7060\n",
      "Epoch 2/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.7156\n",
      "Epoch 00002: loss improved from 2.23898 to 1.67245, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.6724 - val_loss: 1.5952\n",
      "Epoch 3/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.5903\n",
      "Epoch 00003: loss improved from 1.67245 to 1.59251, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.5925 - val_loss: 1.5496\n",
      "Epoch 4/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.5550\n",
      "Epoch 00004: loss improved from 1.59251 to 1.55651, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.5565 - val_loss: 1.5218\n",
      "Epoch 5/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5363\n",
      "Epoch 00005: loss improved from 1.55651 to 1.53634, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.5363 - val_loss: 1.5055\n",
      "Epoch 6/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5208\n",
      "Epoch 00006: loss improved from 1.53634 to 1.52079, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.5208 - val_loss: 1.4960\n",
      "Epoch 7/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5115\n",
      "Epoch 00007: loss improved from 1.52079 to 1.51153, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.5115 - val_loss: 1.4903\n",
      "Epoch 8/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5048\n",
      "Epoch 00008: loss improved from 1.51153 to 1.50477, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.5048 - val_loss: 1.4829\n",
      "Epoch 9/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4994\n",
      "Epoch 00009: loss improved from 1.50477 to 1.49941, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4994 - val_loss: 1.4794\n",
      "Epoch 10/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4947\n",
      "Epoch 00010: loss improved from 1.49941 to 1.49475, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4947 - val_loss: 1.4796\n",
      "Epoch 11/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4927\n",
      "Epoch 00011: loss improved from 1.49475 to 1.49266, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4927 - val_loss: 1.4744\n",
      "Epoch 12/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4908\n",
      "Epoch 00012: loss improved from 1.49266 to 1.49083, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4908 - val_loss: 1.4731\n",
      "Epoch 13/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4883\n",
      "Epoch 00013: loss improved from 1.49083 to 1.48834, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4883 - val_loss: 1.4965\n",
      "Epoch 14/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4957\n",
      "Epoch 00014: loss did not improve from 1.48834\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4957 - val_loss: 1.4785\n",
      "Epoch 15/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4874\n",
      "Epoch 00015: loss improved from 1.48834 to 1.48740, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4874 - val_loss: 1.4779\n",
      "Epoch 16/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4904\n",
      "Epoch 00016: loss did not improve from 1.48740\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4904 - val_loss: 1.4709\n",
      "Epoch 17/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4880\n",
      "Epoch 00017: loss did not improve from 1.48740\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4880 - val_loss: 1.4802\n",
      "Epoch 18/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4910\n",
      "Epoch 00018: loss did not improve from 1.48740\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4910 - val_loss: 1.4791\n",
      "Epoch 19/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4873\n",
      "Epoch 00019: loss improved from 1.48740 to 1.48729, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4873 - val_loss: 1.4706\n",
      "Epoch 20/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4844\n",
      "Epoch 00020: loss improved from 1.48729 to 1.48438, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4844 - val_loss: 1.4754\n",
      "Epoch 21/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4884\n",
      "Epoch 00021: loss did not improve from 1.48438\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4884 - val_loss: 1.4787\n",
      "Epoch 22/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4854\n",
      "Epoch 00022: loss did not improve from 1.48438\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4854 - val_loss: 1.4710\n",
      "Epoch 23/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4816\n",
      "Epoch 00023: loss improved from 1.48438 to 1.48159, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4816 - val_loss: 1.4765\n",
      "Epoch 24/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4839\n",
      "Epoch 00024: loss did not improve from 1.48159\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4839 - val_loss: 1.4673\n",
      "Epoch 25/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4832\n",
      "Epoch 00025: loss did not improve from 1.48159\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4832 - val_loss: 1.4796\n",
      "Epoch 26/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4849\n",
      "Epoch 00026: loss did not improve from 1.48159\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4849 - val_loss: 1.4730\n",
      "Epoch 27/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4807\n",
      "Epoch 00027: loss improved from 1.48159 to 1.48066, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4807 - val_loss: 1.4731\n",
      "Epoch 28/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4804\n",
      "Epoch 00028: loss improved from 1.48066 to 1.48038, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4804 - val_loss: 1.4701\n",
      "Epoch 29/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4798\n",
      "Epoch 00029: loss improved from 1.48038 to 1.47982, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4798 - val_loss: 1.4682\n",
      "Epoch 30/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4779\n",
      "Epoch 00030: loss improved from 1.47982 to 1.47788, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4779 - val_loss: 1.4671\n",
      "Epoch 31/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4778\n",
      "Epoch 00031: loss improved from 1.47788 to 1.47778, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4778 - val_loss: 1.4667\n",
      "Epoch 32/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4814\n",
      "Epoch 00032: loss did not improve from 1.47778\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4814 - val_loss: 1.4681\n",
      "Epoch 33/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4783\n",
      "Epoch 00033: loss did not improve from 1.47778\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4783 - val_loss: 1.4713\n",
      "Epoch 34/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4775\n",
      "Epoch 00034: loss improved from 1.47778 to 1.47749, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4775 - val_loss: 1.4690\n",
      "Epoch 35/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4749\n",
      "Epoch 00035: loss improved from 1.47749 to 1.47493, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4749 - val_loss: 1.4735\n",
      "Epoch 36/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4754\n",
      "Epoch 00036: loss did not improve from 1.47493\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4754 - val_loss: 1.4758\n",
      "Epoch 37/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4812\n",
      "Epoch 00037: loss did not improve from 1.47493\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4812 - val_loss: 1.4764\n",
      "Epoch 38/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4777\n",
      "Epoch 00038: loss did not improve from 1.47493\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4777 - val_loss: 1.4736\n",
      "Epoch 39/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4738\n",
      "Epoch 00039: loss improved from 1.47493 to 1.47381, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4738 - val_loss: 1.4656\n",
      "Epoch 40/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4724\n",
      "Epoch 00040: loss improved from 1.47381 to 1.47238, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4724 - val_loss: 1.4693\n",
      "Epoch 41/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4749\n",
      "Epoch 00041: loss did not improve from 1.47238\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4749 - val_loss: 1.4797\n",
      "Epoch 42/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4814\n",
      "Epoch 00042: loss did not improve from 1.47238\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4814 - val_loss: 1.4685\n",
      "Epoch 43/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4759\n",
      "Epoch 00043: loss did not improve from 1.47238\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4759 - val_loss: 1.4739\n",
      "Epoch 44/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4732\n",
      "Epoch 00044: loss did not improve from 1.47238\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4732 - val_loss: 1.4697\n",
      "Epoch 45/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4721\n",
      "Epoch 00045: loss improved from 1.47238 to 1.47212, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4721 - val_loss: 1.4691\n",
      "Epoch 46/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4740\n",
      "Epoch 00046: loss did not improve from 1.47212\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4740 - val_loss: 1.4716\n",
      "Epoch 47/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4738\n",
      "Epoch 00047: loss did not improve from 1.47212\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4738 - val_loss: 1.4682\n",
      "Epoch 48/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4750\n",
      "Epoch 00048: loss did not improve from 1.47212\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4750 - val_loss: 1.4703\n",
      "Epoch 49/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4718\n",
      "Epoch 00049: loss improved from 1.47212 to 1.47179, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4718 - val_loss: 1.4686\n",
      "Epoch 50/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4704\n",
      "Epoch 00050: loss improved from 1.47179 to 1.47040, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4704 - val_loss: 1.4744\n",
      "Epoch 51/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4717\n",
      "Epoch 00051: loss did not improve from 1.47040\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4717 - val_loss: 1.4691\n",
      "Epoch 52/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4699\n",
      "Epoch 00052: loss improved from 1.47040 to 1.46986, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4699 - val_loss: 1.4719\n",
      "Epoch 53/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4698\n",
      "Epoch 00053: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4724 - val_loss: 1.4686\n",
      "Epoch 54/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4736\n",
      "Epoch 00054: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4736 - val_loss: 1.4736\n",
      "Epoch 55/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4828\n",
      "Epoch 00055: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4725 - val_loss: 1.4690\n",
      "Epoch 56/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4631\n",
      "Epoch 00056: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4727 - val_loss: 1.4666\n",
      "Epoch 57/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4713\n",
      "Epoch 00057: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4713 - val_loss: 1.4693\n",
      "Epoch 58/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4708\n",
      "Epoch 00058: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4703 - val_loss: 1.4696\n",
      "Epoch 59/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4753\n",
      "Epoch 00059: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4706 - val_loss: 1.4735\n",
      "Epoch 60/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4728\n",
      "Epoch 00060: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4728 - val_loss: 1.4675\n",
      "Epoch 61/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4703\n",
      "Epoch 00061: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4703 - val_loss: 1.4714\n",
      "Epoch 62/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4724\n",
      "Epoch 00062: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4724 - val_loss: 1.4705\n",
      "Epoch 63/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4733\n",
      "Epoch 00063: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4733 - val_loss: 1.4685\n",
      "Epoch 64/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4705\n",
      "Epoch 00064: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4705 - val_loss: 1.4681\n",
      "Epoch 65/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4659\n",
      "Epoch 00065: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4706 - val_loss: 1.4788\n",
      "Epoch 66/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4764\n",
      "Epoch 00066: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4764 - val_loss: 1.4684\n",
      "Epoch 67/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4750\n",
      "Epoch 00067: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4738 - val_loss: 1.4729\n",
      "Epoch 68/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4788\n",
      "Epoch 00068: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4767 - val_loss: 1.4735\n",
      "Epoch 69/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4827\n",
      "Epoch 00069: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4723 - val_loss: 1.4771\n",
      "Epoch 70/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4815\n",
      "Epoch 00070: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4757 - val_loss: 1.4687\n",
      "Epoch 71/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4721\n",
      "Epoch 00071: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4721 - val_loss: 1.4755\n",
      "Epoch 72/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4718\n",
      "Epoch 00072: loss did not improve from 1.46986\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4718 - val_loss: 1.4696\n",
      "Epoch 73/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4659\n",
      "Epoch 00073: loss improved from 1.46986 to 1.46894, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4689 - val_loss: 1.4698\n",
      "Epoch 74/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4699\n",
      "Epoch 00074: loss did not improve from 1.46894\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4699 - val_loss: 1.4699\n",
      "Epoch 75/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4725\n",
      "Epoch 00075: loss did not improve from 1.46894\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4725 - val_loss: 1.4734\n",
      "Epoch 76/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4699\n",
      "Epoch 00076: loss did not improve from 1.46894\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4699 - val_loss: 1.4698\n",
      "Epoch 77/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4701\n",
      "Epoch 00077: loss did not improve from 1.46894\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4701 - val_loss: 1.4702\n",
      "Epoch 78/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4691\n",
      "Epoch 00078: loss did not improve from 1.46894\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4691 - val_loss: 1.4807\n",
      "Epoch 79/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4755\n",
      "Epoch 00079: loss did not improve from 1.46894\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4755 - val_loss: 1.4749\n",
      "Epoch 80/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4718\n",
      "Epoch 00080: loss did not improve from 1.46894\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4718 - val_loss: 1.4719\n",
      "Epoch 81/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4687\n",
      "Epoch 00081: loss improved from 1.46894 to 1.46873, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4687 - val_loss: 1.4720\n",
      "Epoch 82/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4710\n",
      "Epoch 00082: loss did not improve from 1.46873\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4710 - val_loss: 1.4774\n",
      "Epoch 83/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4752\n",
      "Epoch 00083: loss did not improve from 1.46873\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4752 - val_loss: 1.4691\n",
      "Epoch 84/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4698\n",
      "Epoch 00084: loss did not improve from 1.46873\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4698 - val_loss: 1.4740\n",
      "Epoch 85/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4708\n",
      "Epoch 00085: loss did not improve from 1.46873\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4708 - val_loss: 1.4708\n",
      "Epoch 86/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4698\n",
      "Epoch 00086: loss did not improve from 1.46873\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4698 - val_loss: 1.4659\n",
      "Epoch 87/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4686\n",
      "Epoch 00087: loss improved from 1.46873 to 1.46865, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4686 - val_loss: 1.4678\n",
      "Epoch 88/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4674\n",
      "Epoch 00088: loss improved from 1.46865 to 1.46738, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4674 - val_loss: 1.4708\n",
      "Epoch 89/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4682\n",
      "Epoch 00089: loss did not improve from 1.46738\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4682 - val_loss: 1.4726\n",
      "Epoch 90/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4701\n",
      "Epoch 00090: loss did not improve from 1.46738\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4701 - val_loss: 1.4717\n",
      "Epoch 91/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4699\n",
      "Epoch 00091: loss did not improve from 1.46738\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4699 - val_loss: 1.4676\n",
      "Epoch 92/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4630\n",
      "Epoch 00092: loss did not improve from 1.46738\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4682 - val_loss: 1.4667\n",
      "Epoch 93/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4659\n",
      "Epoch 00093: loss improved from 1.46738 to 1.46594, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4659 - val_loss: 1.4710\n",
      "Epoch 94/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4627\n",
      "Epoch 00094: loss did not improve from 1.46594\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.4680 - val_loss: 1.4704\n",
      "Epoch 95/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4702\n",
      "Epoch 00095: loss did not improve from 1.46594\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.4696 - val_loss: 1.4690\n",
      "Epoch 96/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4681\n",
      "Epoch 00096: loss improved from 1.46594 to 1.46532, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4653 - val_loss: 1.4691\n",
      "Epoch 97/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4665\n",
      "Epoch 00097: loss did not improve from 1.46532\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4665 - val_loss: 1.4675\n",
      "Epoch 98/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4685\n",
      "Epoch 00098: loss did not improve from 1.46532\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4685 - val_loss: 1.4776\n",
      "Epoch 99/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4762\n",
      "Epoch 00099: loss did not improve from 1.46532\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4762 - val_loss: 1.4780\n",
      "Epoch 100/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4803\n",
      "Epoch 00100: loss did not improve from 1.46532\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4803 - val_loss: 1.4721\n",
      "Epoch 101/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4731\n",
      "Epoch 00101: loss did not improve from 1.46532\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4731 - val_loss: 1.4725\n",
      "Epoch 102/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4691\n",
      "Epoch 00102: loss did not improve from 1.46532\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4699 - val_loss: 1.4693\n",
      "Epoch 103/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4657\n",
      "Epoch 00103: loss did not improve from 1.46532\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4657 - val_loss: 1.4713\n",
      "Epoch 104/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4684\n",
      "Epoch 00104: loss did not improve from 1.46532\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4684 - val_loss: 1.4743\n",
      "Epoch 105/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4679\n",
      "Epoch 00105: loss did not improve from 1.46532\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4679 - val_loss: 1.4634\n",
      "Epoch 106/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4641\n",
      "Epoch 00106: loss improved from 1.46532 to 1.46408, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4641 - val_loss: 1.4704\n",
      "Epoch 107/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4677\n",
      "Epoch 00107: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4677 - val_loss: 1.4691\n",
      "Epoch 108/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4652\n",
      "Epoch 00108: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4652 - val_loss: 1.4708\n",
      "Epoch 109/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4694\n",
      "Epoch 00109: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4694 - val_loss: 1.4709\n",
      "Epoch 110/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4671\n",
      "Epoch 00110: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4671 - val_loss: 1.4673\n",
      "Epoch 111/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4658\n",
      "Epoch 00111: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4658 - val_loss: 1.4694\n",
      "Epoch 112/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4655\n",
      "Epoch 00112: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4655 - val_loss: 1.4689\n",
      "Epoch 113/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4651\n",
      "Epoch 00113: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4651 - val_loss: 1.4656\n",
      "Epoch 114/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4656\n",
      "Epoch 00114: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4656 - val_loss: 1.4740\n",
      "Epoch 115/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4691\n",
      "Epoch 00115: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4691 - val_loss: 1.4690\n",
      "Epoch 116/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4652\n",
      "Epoch 00116: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4652 - val_loss: 1.4708\n",
      "Epoch 117/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4660\n",
      "Epoch 00117: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4660 - val_loss: 1.4721\n",
      "Epoch 118/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4676\n",
      "Epoch 00118: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4676 - val_loss: 1.4716\n",
      "Epoch 119/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4656\n",
      "Epoch 00119: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4656 - val_loss: 1.4681\n",
      "Epoch 120/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4638\n",
      "Epoch 00120: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4653 - val_loss: 1.4741\n",
      "Epoch 121/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4752\n",
      "Epoch 00121: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4673 - val_loss: 1.4737\n",
      "Epoch 122/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4713\n",
      "Epoch 00122: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4679 - val_loss: 1.4681\n",
      "Epoch 123/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4620\n",
      "Epoch 00123: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4672 - val_loss: 1.4716\n",
      "Epoch 124/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4701\n",
      "Epoch 00124: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4701 - val_loss: 1.4705\n",
      "Epoch 125/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4697\n",
      "Epoch 00125: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4697 - val_loss: 1.4728\n",
      "Epoch 126/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4718\n",
      "Epoch 00126: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4718 - val_loss: 1.4694\n",
      "Epoch 127/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4647\n",
      "Epoch 00127: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4664 - val_loss: 1.4687\n",
      "Epoch 128/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4661\n",
      "Epoch 00128: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4661 - val_loss: 1.4733\n",
      "Epoch 129/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4657\n",
      "Epoch 00129: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4657 - val_loss: 1.4659\n",
      "Epoch 130/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4653\n",
      "Epoch 00130: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4653 - val_loss: 1.4750\n",
      "Epoch 131/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4679\n",
      "Epoch 00131: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4679 - val_loss: 1.4720\n",
      "Epoch 132/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4683\n",
      "Epoch 00132: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4683 - val_loss: 1.4692\n",
      "Epoch 133/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4652\n",
      "Epoch 00133: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4652 - val_loss: 1.4693\n",
      "Epoch 134/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4667\n",
      "Epoch 00134: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4667 - val_loss: 1.4690\n",
      "Epoch 135/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4669\n",
      "Epoch 00135: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4669 - val_loss: 1.4716\n",
      "Epoch 136/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4646\n",
      "Epoch 00136: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4646 - val_loss: 1.4746\n",
      "Epoch 137/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4653\n",
      "Epoch 00137: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4653 - val_loss: 1.4684\n",
      "Epoch 138/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4662\n",
      "Epoch 00138: loss did not improve from 1.46408\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4662 - val_loss: 1.4702\n",
      "Epoch 139/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4638\n",
      "Epoch 00139: loss improved from 1.46408 to 1.46380, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4638 - val_loss: 1.4716\n",
      "Epoch 140/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4635\n",
      "Epoch 00140: loss improved from 1.46380 to 1.46352, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4635 - val_loss: 1.4679\n",
      "Epoch 141/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4646\n",
      "Epoch 00141: loss did not improve from 1.46352\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4646 - val_loss: 1.4688\n",
      "Epoch 142/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4660\n",
      "Epoch 00142: loss did not improve from 1.46352\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4660 - val_loss: 1.4679\n",
      "Epoch 143/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4649\n",
      "Epoch 00143: loss did not improve from 1.46352\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4649 - val_loss: 1.4676\n",
      "Epoch 144/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4634\n",
      "Epoch 00144: loss improved from 1.46352 to 1.46335, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4634 - val_loss: 1.4691\n",
      "Epoch 145/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4625\n",
      "Epoch 00145: loss improved from 1.46335 to 1.46252, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4625 - val_loss: 1.4667\n",
      "Epoch 146/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4628\n",
      "Epoch 00146: loss did not improve from 1.46252\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4628 - val_loss: 1.4720\n",
      "Epoch 147/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4628\n",
      "Epoch 00147: loss did not improve from 1.46252\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4628 - val_loss: 1.4661\n",
      "Epoch 148/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4632\n",
      "Epoch 00148: loss did not improve from 1.46252\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4632 - val_loss: 1.4690\n",
      "Epoch 149/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4632\n",
      "Epoch 00149: loss did not improve from 1.46252\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4632 - val_loss: 1.4719\n",
      "Epoch 150/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4630\n",
      "Epoch 00150: loss did not improve from 1.46252\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4630 - val_loss: 1.4787\n",
      "Epoch 151/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4584\n",
      "Epoch 00151: loss did not improve from 1.46252\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4648 - val_loss: 1.4699\n",
      "Epoch 152/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4674\n",
      "Epoch 00152: loss did not improve from 1.46252\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4674 - val_loss: 1.4687\n",
      "Epoch 153/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4670\n",
      "Epoch 00153: loss did not improve from 1.46252\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4670 - val_loss: 1.4802\n",
      "Epoch 154/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4686\n",
      "Epoch 00154: loss did not improve from 1.46252\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4686 - val_loss: 1.4729\n",
      "Epoch 155/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4628\n",
      "Epoch 00155: loss did not improve from 1.46252\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4628 - val_loss: 1.4690\n",
      "Epoch 156/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4627\n",
      "Epoch 00156: loss did not improve from 1.46252\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4627 - val_loss: 1.4689\n",
      "Epoch 157/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4613\n",
      "Epoch 00157: loss improved from 1.46252 to 1.46130, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4613 - val_loss: 1.4677\n",
      "Epoch 158/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4636\n",
      "Epoch 00158: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4636 - val_loss: 1.4715\n",
      "Epoch 159/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4626\n",
      "Epoch 00159: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4626 - val_loss: 1.4693\n",
      "Epoch 160/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4626\n",
      "Epoch 00160: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4626 - val_loss: 1.4722\n",
      "Epoch 161/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4663\n",
      "Epoch 00161: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4663 - val_loss: 1.4699\n",
      "Epoch 162/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4633\n",
      "Epoch 00162: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4633 - val_loss: 1.4784\n",
      "Epoch 163/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4649\n",
      "Epoch 00163: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4649 - val_loss: 1.4701\n",
      "Epoch 164/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4626\n",
      "Epoch 00164: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4626 - val_loss: 1.4710\n",
      "Epoch 165/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4654\n",
      "Epoch 00165: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4654 - val_loss: 1.4716\n",
      "Epoch 166/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4664\n",
      "Epoch 00166: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4664 - val_loss: 1.4673\n",
      "Epoch 167/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.475 - ETA: 0s - loss: 1.4624\n",
      "Epoch 00167: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4624 - val_loss: 1.4725\n",
      "Epoch 168/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4632\n",
      "Epoch 00168: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4632 - val_loss: 1.4681\n",
      "Epoch 169/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4627\n",
      "Epoch 00169: loss did not improve from 1.46130\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4627 - val_loss: 1.4701\n",
      "Epoch 170/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4608\n",
      "Epoch 00170: loss improved from 1.46130 to 1.46083, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4608 - val_loss: 1.4682\n",
      "Epoch 171/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4602\n",
      "Epoch 00171: loss improved from 1.46083 to 1.46022, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4602 - val_loss: 1.4664\n",
      "Epoch 172/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4590\n",
      "Epoch 00172: loss improved from 1.46022 to 1.45901, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4590 - val_loss: 1.4700\n",
      "Epoch 173/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4648\n",
      "Epoch 00173: loss did not improve from 1.45901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4648 - val_loss: 1.4676\n",
      "Epoch 174/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4637\n",
      "Epoch 00174: loss did not improve from 1.45901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4637 - val_loss: 1.4788\n",
      "Epoch 175/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4669\n",
      "Epoch 00175: loss did not improve from 1.45901\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4669 - val_loss: 1.4670\n",
      "Epoch 176/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4662\n",
      "Epoch 00176: loss did not improve from 1.45901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4662 - val_loss: 1.4697\n",
      "Epoch 177/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4612\n",
      "Epoch 00177: loss did not improve from 1.45901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4612 - val_loss: 1.4726\n",
      "Epoch 178/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4642\n",
      "Epoch 00178: loss did not improve from 1.45901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4642 - val_loss: 1.4720\n",
      "Epoch 179/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4623\n",
      "Epoch 00179: loss did not improve from 1.45901\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4623 - val_loss: 1.4703\n",
      "Epoch 180/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4626\n",
      "Epoch 00180: loss did not improve from 1.45901\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4626 - val_loss: 1.4698\n",
      "Epoch 181/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4628\n",
      "Epoch 00181: loss did not improve from 1.45901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4628 - val_loss: 1.4715\n",
      "Epoch 182/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4644\n",
      "Epoch 00182: loss did not improve from 1.45901\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4644 - val_loss: 1.4710\n",
      "Epoch 183/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4612\n",
      "Epoch 00183: loss did not improve from 1.45901\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4612 - val_loss: 1.4673\n",
      "Epoch 184/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4589\n",
      "Epoch 00184: loss improved from 1.45901 to 1.45889, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4589 - val_loss: 1.4665\n",
      "Epoch 185/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4597\n",
      "Epoch 00185: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4597 - val_loss: 1.4678\n",
      "Epoch 186/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4622\n",
      "Epoch 00186: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4622 - val_loss: 1.4679\n",
      "Epoch 187/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00187: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.4637 - val_loss: 1.4717\n",
      "Epoch 188/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4638\n",
      "Epoch 00188: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4638 - val_loss: 1.4744\n",
      "Epoch 189/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4654\n",
      "Epoch 00189: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4654 - val_loss: 1.4770\n",
      "Epoch 190/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4658\n",
      "Epoch 00190: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4658 - val_loss: 1.4709\n",
      "Epoch 191/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4622\n",
      "Epoch 00191: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4622 - val_loss: 1.4689\n",
      "Epoch 192/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4622\n",
      "Epoch 00192: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.4599 - val_loss: 1.4698\n",
      "Epoch 193/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4616\n",
      "Epoch 00193: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4616 - val_loss: 1.4670\n",
      "Epoch 194/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4616\n",
      "Epoch 00194: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4616 - val_loss: 1.4702\n",
      "Epoch 195/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4620\n",
      "Epoch 00195: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4620 - val_loss: 1.4689\n",
      "Epoch 196/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4601\n",
      "Epoch 00196: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4601 - val_loss: 1.4676\n",
      "Epoch 197/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4607\n",
      "Epoch 00197: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4607 - val_loss: 1.4782\n",
      "Epoch 198/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4641\n",
      "Epoch 00198: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4641 - val_loss: 1.4755\n",
      "Epoch 199/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4610\n",
      "Epoch 00199: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4610 - val_loss: 1.4743\n",
      "Epoch 200/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4629\n",
      "Epoch 00200: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4629 - val_loss: 1.4708\n",
      "Epoch 201/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4611\n",
      "Epoch 00201: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4611 - val_loss: 1.4707\n",
      "Epoch 202/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4629\n",
      "Epoch 00202: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4629 - val_loss: 1.4674\n",
      "Epoch 203/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4618\n",
      "Epoch 00203: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4618 - val_loss: 1.4738\n",
      "Epoch 204/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4677\n",
      "Epoch 00204: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4692 - val_loss: 1.4742\n",
      "Epoch 205/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4638\n",
      "Epoch 00205: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4638 - val_loss: 1.4757\n",
      "Epoch 206/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4635\n",
      "Epoch 00206: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4635 - val_loss: 1.4726\n",
      "Epoch 207/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4652\n",
      "Epoch 00207: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4652 - val_loss: 1.4700\n",
      "Epoch 208/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4600\n",
      "Epoch 00208: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4600 - val_loss: 1.4722\n",
      "Epoch 209/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4600\n",
      "Epoch 00209: loss did not improve from 1.45889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4600 - val_loss: 1.4642\n",
      "Epoch 210/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4585\n",
      "Epoch 00210: loss improved from 1.45889 to 1.45850, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4585 - val_loss: 1.4713\n",
      "Epoch 211/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4617\n",
      "Epoch 00211: loss did not improve from 1.45850\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4617 - val_loss: 1.4692\n",
      "Epoch 212/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4607\n",
      "Epoch 00212: loss did not improve from 1.45850\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4607 - val_loss: 1.4683\n",
      "Epoch 213/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4584\n",
      "Epoch 00213: loss improved from 1.45850 to 1.45844, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4584 - val_loss: 1.4653\n",
      "Epoch 214/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4584\n",
      "Epoch 00214: loss improved from 1.45844 to 1.45836, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4584 - val_loss: 1.4671\n",
      "Epoch 215/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4569\n",
      "Epoch 00215: loss improved from 1.45836 to 1.45691, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4569 - val_loss: 1.4684\n",
      "Epoch 216/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4572\n",
      "Epoch 00216: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4572 - val_loss: 1.4692\n",
      "Epoch 217/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4585\n",
      "Epoch 00217: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4585 - val_loss: 1.4654\n",
      "Epoch 218/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4582\n",
      "Epoch 00218: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4582 - val_loss: 1.4664\n",
      "Epoch 219/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4598\n",
      "Epoch 00219: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4598 - val_loss: 1.4654\n",
      "Epoch 220/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4605\n",
      "Epoch 00220: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4605 - val_loss: 1.4708\n",
      "Epoch 221/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00221: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4603 - val_loss: 1.4714\n",
      "Epoch 222/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4581\n",
      "Epoch 00222: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4581 - val_loss: 1.4687\n",
      "Epoch 223/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4584\n",
      "Epoch 00223: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4584 - val_loss: 1.4665\n",
      "Epoch 224/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4593\n",
      "Epoch 00224: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4593 - val_loss: 1.4675\n",
      "Epoch 225/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4587\n",
      "Epoch 00225: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4587 - val_loss: 1.4692\n",
      "Epoch 226/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4591\n",
      "Epoch 00226: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4591 - val_loss: 1.4686\n",
      "Epoch 227/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4576\n",
      "Epoch 00227: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4576 - val_loss: 1.4716\n",
      "Epoch 228/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4609\n",
      "Epoch 00228: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4609 - val_loss: 1.4659\n",
      "Epoch 229/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4589\n",
      "Epoch 00229: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4589 - val_loss: 1.4655\n",
      "Epoch 230/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4590\n",
      "Epoch 00230: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4590 - val_loss: 1.4764\n",
      "Epoch 231/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4674\n",
      "Epoch 00231: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 1.4604 - val_loss: 1.4691\n",
      "Epoch 232/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4616\n",
      "Epoch 00232: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4616 - val_loss: 1.4654\n",
      "Epoch 233/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4596\n",
      "Epoch 00233: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4596 - val_loss: 1.4696\n",
      "Epoch 234/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4565\n",
      "Epoch 00234: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4607 - val_loss: 1.4712\n",
      "Epoch 235/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4593\n",
      "Epoch 00235: loss did not improve from 1.45691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4593 - val_loss: 1.4643\n",
      "Epoch 236/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4569\n",
      "Epoch 00236: loss improved from 1.45691 to 1.45686, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4569 - val_loss: 1.4642\n",
      "Epoch 237/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4588\n",
      "Epoch 00237: loss did not improve from 1.45686\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4588 - val_loss: 1.4687\n",
      "Epoch 238/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4575\n",
      "Epoch 00238: loss did not improve from 1.45686\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4575 - val_loss: 1.4690\n",
      "Epoch 239/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4592\n",
      "Epoch 00239: loss did not improve from 1.45686\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4592 - val_loss: 1.4664\n",
      "Epoch 240/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4579\n",
      "Epoch 00240: loss did not improve from 1.45686\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4579 - val_loss: 1.4732\n",
      "Epoch 241/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4617\n",
      "Epoch 00241: loss did not improve from 1.45686\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4617 - val_loss: 1.4689\n",
      "Epoch 242/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4630\n",
      "Epoch 00242: loss did not improve from 1.45686\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4630 - val_loss: 1.4678\n",
      "Epoch 243/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4589\n",
      "Epoch 00243: loss did not improve from 1.45686\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4589 - val_loss: 1.4731\n",
      "Epoch 244/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4579\n",
      "Epoch 00244: loss did not improve from 1.45686\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4579 - val_loss: 1.4675\n",
      "Epoch 245/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4602\n",
      "Epoch 00245: loss did not improve from 1.45686\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4602 - val_loss: 1.4641\n",
      "Epoch 246/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4580\n",
      "Epoch 00246: loss did not improve from 1.45686\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4580 - val_loss: 1.4684\n",
      "Epoch 247/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4569\n",
      "Epoch 00247: loss did not improve from 1.45686\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4569 - val_loss: 1.4671\n",
      "Epoch 248/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4566\n",
      "Epoch 00248: loss improved from 1.45686 to 1.45656, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4566 - val_loss: 1.4696\n",
      "Epoch 249/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4605\n",
      "Epoch 00249: loss did not improve from 1.45656\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4605 - val_loss: 1.4698\n",
      "Epoch 250/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4595\n",
      "Epoch 00250: loss did not improve from 1.45656\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4595 - val_loss: 1.4661\n",
      "Epoch 251/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4570\n",
      "Epoch 00251: loss did not improve from 1.45656\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4570 - val_loss: 1.4703\n",
      "Epoch 252/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4581\n",
      "Epoch 00252: loss did not improve from 1.45656\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4581 - val_loss: 1.4682\n",
      "Epoch 253/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4554\n",
      "Epoch 00253: loss improved from 1.45656 to 1.45538, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4554 - val_loss: 1.4674\n",
      "Epoch 254/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4574\n",
      "Epoch 00254: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4574 - val_loss: 1.4732\n",
      "Epoch 255/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4587\n",
      "Epoch 00255: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4587 - val_loss: 1.4669\n",
      "Epoch 256/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4596\n",
      "Epoch 00256: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4596 - val_loss: 1.4692\n",
      "Epoch 257/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4584\n",
      "Epoch 00257: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4584 - val_loss: 1.4723\n",
      "Epoch 258/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4632\n",
      "Epoch 00258: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4632 - val_loss: 1.4721\n",
      "Epoch 259/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4631\n",
      "Epoch 00259: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4631 - val_loss: 1.4820\n",
      "Epoch 260/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4604\n",
      "Epoch 00260: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4604 - val_loss: 1.4672\n",
      "Epoch 261/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4601\n",
      "Epoch 00261: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4601 - val_loss: 1.4688\n",
      "Epoch 262/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4576\n",
      "Epoch 00262: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4576 - val_loss: 1.4686\n",
      "Epoch 263/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4566\n",
      "Epoch 00263: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4583 - val_loss: 1.4702\n",
      "Epoch 264/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4610\n",
      "Epoch 00264: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4610 - val_loss: 1.4684\n",
      "Epoch 265/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4600\n",
      "Epoch 00265: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4600 - val_loss: 1.4700\n",
      "Epoch 266/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4577\n",
      "Epoch 00266: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4577 - val_loss: 1.4731\n",
      "Epoch 267/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4585\n",
      "Epoch 00267: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4585 - val_loss: 1.4719\n",
      "Epoch 268/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4579\n",
      "Epoch 00268: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4579 - val_loss: 1.4706\n",
      "Epoch 269/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4583\n",
      "Epoch 00269: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4583 - val_loss: 1.4708\n",
      "Epoch 270/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4591\n",
      "Epoch 00270: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4591 - val_loss: 1.4668\n",
      "Epoch 271/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4568\n",
      "Epoch 00271: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4568 - val_loss: 1.4672\n",
      "Epoch 272/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4565\n",
      "Epoch 00272: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4565 - val_loss: 1.4682\n",
      "Epoch 273/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4504\n",
      "Epoch 00273: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4568 - val_loss: 1.4716\n",
      "Epoch 274/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4601\n",
      "Epoch 00274: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4601 - val_loss: 1.4740\n",
      "Epoch 275/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4628\n",
      "Epoch 00275: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4628 - val_loss: 1.4706\n",
      "Epoch 276/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4605\n",
      "Epoch 00276: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4605 - val_loss: 1.4714\n",
      "Epoch 277/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4580\n",
      "Epoch 00277: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4580 - val_loss: 1.4695\n",
      "Epoch 278/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4584\n",
      "Epoch 00278: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4584 - val_loss: 1.4700\n",
      "Epoch 279/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4581\n",
      "Epoch 00279: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4581 - val_loss: 1.4684\n",
      "Epoch 280/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4562\n",
      "Epoch 00280: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4562 - val_loss: 1.4689\n",
      "Epoch 281/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4587\n",
      "Epoch 00281: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4587 - val_loss: 1.4785\n",
      "Epoch 282/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4621\n",
      "Epoch 00282: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4621 - val_loss: 1.4691\n",
      "Epoch 283/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4575\n",
      "Epoch 00283: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4575 - val_loss: 1.4719\n",
      "Epoch 284/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4617\n",
      "Epoch 00284: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4572 - val_loss: 1.4690\n",
      "Epoch 285/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4558\n",
      "Epoch 00285: loss did not improve from 1.45538\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4558 - val_loss: 1.4657\n",
      "Epoch 286/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4553\n",
      "Epoch 00286: loss improved from 1.45538 to 1.45528, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4553 - val_loss: 1.4693\n",
      "Epoch 287/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4560\n",
      "Epoch 00287: loss did not improve from 1.45528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4560 - val_loss: 1.4671\n",
      "Epoch 288/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4553\n",
      "Epoch 00288: loss did not improve from 1.45528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4553 - val_loss: 1.4678\n",
      "Epoch 289/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4536\n",
      "Epoch 00289: loss improved from 1.45528 to 1.45365, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4536 - val_loss: 1.4664\n",
      "Epoch 290/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4572\n",
      "Epoch 00290: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4572 - val_loss: 1.4703\n",
      "Epoch 291/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4575\n",
      "Epoch 00291: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4575 - val_loss: 1.4697\n",
      "Epoch 292/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4601\n",
      "Epoch 00292: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4601 - val_loss: 1.4724\n",
      "Epoch 293/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4596\n",
      "Epoch 00293: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4596 - val_loss: 1.4659\n",
      "Epoch 294/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4563\n",
      "Epoch 00294: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4563 - val_loss: 1.4669\n",
      "Epoch 295/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4550\n",
      "Epoch 00295: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4550 - val_loss: 1.4743\n",
      "Epoch 296/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4573\n",
      "Epoch 00296: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4573 - val_loss: 1.4666\n",
      "Epoch 297/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4567\n",
      "Epoch 00297: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4567 - val_loss: 1.4721\n",
      "Epoch 298/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4580\n",
      "Epoch 00298: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4580 - val_loss: 1.4733\n",
      "Epoch 299/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4625\n",
      "Epoch 00299: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4625 - val_loss: 1.4698\n",
      "Epoch 300/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4585\n",
      "Epoch 00300: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4585 - val_loss: 1.4738\n",
      "Epoch 301/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4537\n",
      "Epoch 00301: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4582 - val_loss: 1.4710\n",
      "Epoch 302/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4552\n",
      "Epoch 00302: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4552 - val_loss: 1.4677\n",
      "Epoch 303/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4545\n",
      "Epoch 00303: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4545 - val_loss: 1.4699\n",
      "Epoch 304/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4561\n",
      "Epoch 00304: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4561 - val_loss: 1.4665\n",
      "Epoch 305/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4579\n",
      "Epoch 00305: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4579 - val_loss: 1.4741\n",
      "Epoch 306/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4576\n",
      "Epoch 00306: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4576 - val_loss: 1.4696\n",
      "Epoch 307/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4544\n",
      "Epoch 00307: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4544 - val_loss: 1.4678\n",
      "Epoch 308/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4557\n",
      "Epoch 00308: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4557 - val_loss: 1.4712\n",
      "Epoch 309/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4562\n",
      "Epoch 00309: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4562 - val_loss: 1.4699\n",
      "Epoch 310/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4560\n",
      "Epoch 00310: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4560 - val_loss: 1.4781\n",
      "Epoch 311/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4650\n",
      "Epoch 00311: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4650 - val_loss: 1.4696\n",
      "Epoch 312/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4604\n",
      "Epoch 00312: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4604 - val_loss: 1.4661\n",
      "Epoch 313/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4572\n",
      "Epoch 00313: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4572 - val_loss: 1.4718\n",
      "Epoch 314/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4573\n",
      "Epoch 00314: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4573 - val_loss: 1.4740\n",
      "Epoch 315/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4637\n",
      "Epoch 00315: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4637 - val_loss: 1.4790\n",
      "Epoch 316/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4668\n",
      "Epoch 00316: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4668 - val_loss: 1.4733\n",
      "Epoch 317/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4610\n",
      "Epoch 00317: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4610 - val_loss: 1.4672\n",
      "Epoch 318/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4576\n",
      "Epoch 00318: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4576 - val_loss: 1.4715\n",
      "Epoch 319/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4710\n",
      "Epoch 00319: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4557 - val_loss: 1.4691\n",
      "Epoch 320/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4558\n",
      "Epoch 00320: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4558 - val_loss: 1.4720\n",
      "Epoch 321/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00321: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4544 - val_loss: 1.4735\n",
      "Epoch 322/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4503\n",
      "Epoch 00322: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4588 - val_loss: 1.4669\n",
      "Epoch 323/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4546\n",
      "Epoch 00323: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4546 - val_loss: 1.4699\n",
      "Epoch 324/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4600\n",
      "Epoch 00324: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4543 - val_loss: 1.4681\n",
      "Epoch 325/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4659\n",
      "Epoch 00325: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4659 - val_loss: 1.4714\n",
      "Epoch 326/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4513\n",
      "Epoch 00326: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4628 - val_loss: 1.4682\n",
      "Epoch 327/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4594\n",
      "Epoch 00327: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4594 - val_loss: 1.4707\n",
      "Epoch 328/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4602\n",
      "Epoch 00328: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4602 - val_loss: 1.4756\n",
      "Epoch 329/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4568\n",
      "Epoch 00329: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4568 - val_loss: 1.4679\n",
      "Epoch 330/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00330: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4540 - val_loss: 1.4641\n",
      "Epoch 331/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4528\n",
      "Epoch 00331: loss improved from 1.45365 to 1.45275, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4528 - val_loss: 1.4700\n",
      "Epoch 332/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4567\n",
      "Epoch 00332: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4567 - val_loss: 1.4716\n",
      "Epoch 333/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4569\n",
      "Epoch 00333: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4569 - val_loss: 1.4714\n",
      "Epoch 334/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4620\n",
      "Epoch 00334: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4620 - val_loss: 1.4727\n",
      "Epoch 335/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4574\n",
      "Epoch 00335: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4574 - val_loss: 1.4752\n",
      "Epoch 336/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4574\n",
      "Epoch 00336: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4574 - val_loss: 1.4689\n",
      "Epoch 337/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00337: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4551 - val_loss: 1.4665\n",
      "Epoch 338/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4557\n",
      "Epoch 00338: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4557 - val_loss: 1.4659\n",
      "Epoch 339/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4573\n",
      "Epoch 00339: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4573 - val_loss: 1.4739\n",
      "Epoch 340/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4578\n",
      "Epoch 00340: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4578 - val_loss: 1.4675\n",
      "Epoch 341/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4550\n",
      "Epoch 00341: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4550 - val_loss: 1.4676\n",
      "Epoch 342/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4541\n",
      "Epoch 00342: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4541 - val_loss: 1.4661\n",
      "Epoch 343/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4536\n",
      "Epoch 00343: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4536 - val_loss: 1.4678\n",
      "Epoch 344/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00344: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4549 - val_loss: 1.4716\n",
      "Epoch 345/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4604\n",
      "Epoch 00345: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4604 - val_loss: 1.4736\n",
      "Epoch 346/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4571\n",
      "Epoch 00346: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4571 - val_loss: 1.4691\n",
      "Epoch 347/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4548\n",
      "Epoch 00347: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4548 - val_loss: 1.4703\n",
      "Epoch 348/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4570\n",
      "Epoch 00348: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4570 - val_loss: 1.4708\n",
      "Epoch 349/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00349: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4549 - val_loss: 1.4690\n",
      "Epoch 350/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00350: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4549 - val_loss: 1.4697\n",
      "Epoch 351/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4711\n",
      "Epoch 00351: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4711 - val_loss: 1.4734\n",
      "Epoch 352/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4633\n",
      "Epoch 00352: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4633 - val_loss: 1.4718\n",
      "Epoch 353/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4619\n",
      "Epoch 00353: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4619 - val_loss: 1.4731\n",
      "Epoch 354/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4601\n",
      "Epoch 00354: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4601 - val_loss: 1.4659\n",
      "Epoch 355/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4572\n",
      "Epoch 00355: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4572 - val_loss: 1.4732\n",
      "Epoch 356/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4599\n",
      "Epoch 00356: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4599 - val_loss: 1.4721\n",
      "Epoch 357/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4586\n",
      "Epoch 00357: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4586 - val_loss: 1.4704\n",
      "Epoch 358/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4597\n",
      "Epoch 00358: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4597 - val_loss: 1.4726\n",
      "Epoch 359/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4560\n",
      "Epoch 00359: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4560 - val_loss: 1.4728\n",
      "Epoch 360/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4559\n",
      "Epoch 00360: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4559 - val_loss: 1.4689\n",
      "Epoch 361/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4543\n",
      "Epoch 00361: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4543 - val_loss: 1.4666\n",
      "Epoch 362/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4535\n",
      "Epoch 00362: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4535 - val_loss: 1.4678\n",
      "Epoch 363/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4538\n",
      "Epoch 00363: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4538 - val_loss: 1.4719\n",
      "Epoch 364/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4560\n",
      "Epoch 00364: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4560 - val_loss: 1.4693\n",
      "Epoch 365/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4534\n",
      "Epoch 00365: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4534 - val_loss: 1.4680\n",
      "Epoch 366/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4528\n",
      "Epoch 00366: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4528 - val_loss: 1.4695\n",
      "Epoch 367/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4536\n",
      "Epoch 00367: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4536 - val_loss: 1.4696\n",
      "Epoch 368/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00368: loss did not improve from 1.45275\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4555 - val_loss: 1.4698\n",
      "Epoch 369/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4526\n",
      "Epoch 00369: loss improved from 1.45275 to 1.45256, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4526 - val_loss: 1.4655\n",
      "Epoch 370/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4521\n",
      "Epoch 00370: loss improved from 1.45256 to 1.45209, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4521 - val_loss: 1.4656\n",
      "Epoch 371/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00371: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4540 - val_loss: 1.4696\n",
      "Epoch 372/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4545\n",
      "Epoch 00372: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4545 - val_loss: 1.4744\n",
      "Epoch 373/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4548\n",
      "Epoch 00373: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4548 - val_loss: 1.4681\n",
      "Epoch 374/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4571\n",
      "Epoch 00374: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4571 - val_loss: 1.4746\n",
      "Epoch 375/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4568\n",
      "Epoch 00375: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4568 - val_loss: 1.4686\n",
      "Epoch 376/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4564\n",
      "Epoch 00376: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4564 - val_loss: 1.4686\n",
      "Epoch 377/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4566\n",
      "Epoch 00377: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4566 - val_loss: 1.4706\n",
      "Epoch 378/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4571\n",
      "Epoch 00378: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4571 - val_loss: 1.4727\n",
      "Epoch 379/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00379: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4555 - val_loss: 1.4695\n",
      "Epoch 380/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4557\n",
      "Epoch 00380: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4557 - val_loss: 1.4698\n",
      "Epoch 381/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00381: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4542 - val_loss: 1.4709\n",
      "Epoch 382/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4542\n",
      "Epoch 00382: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4542 - val_loss: 1.4733\n",
      "Epoch 383/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00383: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4540 - val_loss: 1.4699\n",
      "Epoch 384/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4484\n",
      "Epoch 00384: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4535 - val_loss: 1.4695\n",
      "Epoch 385/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4543\n",
      "Epoch 00385: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4543 - val_loss: 1.4675\n",
      "Epoch 386/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4544\n",
      "Epoch 00386: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4544 - val_loss: 1.4721\n",
      "Epoch 387/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4563\n",
      "Epoch 00387: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4563 - val_loss: 1.4667\n",
      "Epoch 388/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4544\n",
      "Epoch 00388: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4544 - val_loss: 1.4676\n",
      "Epoch 389/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4552\n",
      "Epoch 00389: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4552 - val_loss: 1.4716\n",
      "Epoch 390/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4552\n",
      "Epoch 00390: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4552 - val_loss: 1.4693\n",
      "Epoch 391/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4560\n",
      "Epoch 00391: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4560 - val_loss: 1.4698\n",
      "Epoch 392/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4557\n",
      "Epoch 00392: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4557 - val_loss: 1.4648\n",
      "Epoch 393/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4530\n",
      "Epoch 00393: loss did not improve from 1.45209\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4530 - val_loss: 1.4677\n",
      "Epoch 394/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4575\n",
      "Epoch 00394: loss improved from 1.45209 to 1.45156, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4516 - val_loss: 1.4674\n",
      "Epoch 395/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4533\n",
      "Epoch 00395: loss did not improve from 1.45156\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4533 - val_loss: 1.4696\n",
      "Epoch 396/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4514\n",
      "Epoch 00396: loss improved from 1.45156 to 1.45141, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4514 - val_loss: 1.4694\n",
      "Epoch 397/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00397: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4538 - val_loss: 1.4696\n",
      "Epoch 398/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00398: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4525 - val_loss: 1.4715\n",
      "Epoch 399/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00399: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4549 - val_loss: 1.4664\n",
      "Epoch 400/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4582\n",
      "Epoch 00400: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4551 - val_loss: 1.4712\n",
      "Epoch 401/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4545\n",
      "Epoch 00401: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4545 - val_loss: 1.4719\n",
      "Epoch 402/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4528\n",
      "Epoch 00402: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4528 - val_loss: 1.4669\n",
      "Epoch 403/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4530\n",
      "Epoch 00403: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4530 - val_loss: 1.4701\n",
      "Epoch 404/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00404: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4555 - val_loss: 1.4664\n",
      "Epoch 405/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4548\n",
      "Epoch 00405: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4548 - val_loss: 1.4700\n",
      "Epoch 406/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4523\n",
      "Epoch 00406: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4523 - val_loss: 1.4675\n",
      "Epoch 407/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4544\n",
      "Epoch 00407: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4544 - val_loss: 1.4752\n",
      "Epoch 408/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4541\n",
      "Epoch 00408: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4541 - val_loss: 1.4693\n",
      "Epoch 409/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4542\n",
      "Epoch 00409: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4542 - val_loss: 1.4746\n",
      "Epoch 410/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4617\n",
      "Epoch 00410: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4617 - val_loss: 1.4701\n",
      "Epoch 411/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4568\n",
      "Epoch 00411: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4568 - val_loss: 1.4734\n",
      "Epoch 412/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4561\n",
      "Epoch 00412: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4561 - val_loss: 1.4715\n",
      "Epoch 413/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4559\n",
      "Epoch 00413: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4559 - val_loss: 1.4693\n",
      "Epoch 414/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4550\n",
      "Epoch 00414: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4550 - val_loss: 1.4668\n",
      "Epoch 415/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00415: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4529 - val_loss: 1.4668\n",
      "Epoch 416/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4521\n",
      "Epoch 00416: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4521 - val_loss: 1.4701\n",
      "Epoch 417/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00417: loss did not improve from 1.45141\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4525 - val_loss: 1.4668\n",
      "Epoch 418/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4512\n",
      "Epoch 00418: loss improved from 1.45141 to 1.45118, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4512 - val_loss: 1.4710\n",
      "Epoch 419/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00419: loss did not improve from 1.45118\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4529 - val_loss: 1.4672\n",
      "Epoch 420/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4552\n",
      "Epoch 00420: loss did not improve from 1.45118\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4552 - val_loss: 1.4768\n",
      "Epoch 421/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4513\n",
      "Epoch 00421: loss did not improve from 1.45118\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4542 - val_loss: 1.4686\n",
      "Epoch 422/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4579\n",
      "Epoch 00422: loss did not improve from 1.45118\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4525 - val_loss: 1.4732\n",
      "Epoch 423/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00423: loss did not improve from 1.45118\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4540 - val_loss: 1.4706\n",
      "Epoch 424/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4501\n",
      "Epoch 00424: loss improved from 1.45118 to 1.45013, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4501 - val_loss: 1.4674\n",
      "Epoch 425/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4564\n",
      "Epoch 00425: loss improved from 1.45013 to 1.44963, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4496 - val_loss: 1.4667\n",
      "Epoch 426/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4493\n",
      "Epoch 00426: loss improved from 1.44963 to 1.44925, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4493 - val_loss: 1.4678\n",
      "Epoch 427/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4513\n",
      "Epoch 00427: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4513 - val_loss: 1.4729\n",
      "Epoch 428/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00428: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4549 - val_loss: 1.4701\n",
      "Epoch 429/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4518\n",
      "Epoch 00429: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4518 - val_loss: 1.4681\n",
      "Epoch 430/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4509\n",
      "Epoch 00430: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4509 - val_loss: 1.4665\n",
      "Epoch 431/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00431: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4519 - val_loss: 1.4707\n",
      "Epoch 432/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4552\n",
      "Epoch 00432: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4552 - val_loss: 1.4725\n",
      "Epoch 433/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4536\n",
      "Epoch 00433: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4536 - val_loss: 1.4724\n",
      "Epoch 434/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4537\n",
      "Epoch 00434: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4537 - val_loss: 1.4727\n",
      "Epoch 435/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4522\n",
      "Epoch 00435: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4522 - val_loss: 1.4680\n",
      "Epoch 436/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4521\n",
      "Epoch 00436: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4521 - val_loss: 1.4670\n",
      "Epoch 437/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00437: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4515 - val_loss: 1.4718\n",
      "Epoch 438/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00438: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4549 - val_loss: 1.4749\n",
      "Epoch 439/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4552\n",
      "Epoch 00439: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4552 - val_loss: 1.4694\n",
      "Epoch 440/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00440: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4529 - val_loss: 1.4681\n",
      "Epoch 441/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00441: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4529 - val_loss: 1.4692\n",
      "Epoch 442/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00442: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4555 - val_loss: 1.4739\n",
      "Epoch 443/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4553\n",
      "Epoch 00443: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4553 - val_loss: 1.4743\n",
      "Epoch 444/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4517\n",
      "Epoch 00444: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4517 - val_loss: 1.4653\n",
      "Epoch 445/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4501\n",
      "Epoch 00445: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4501 - val_loss: 1.4702\n",
      "Epoch 446/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4388\n",
      "Epoch 00446: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4502 - val_loss: 1.4668\n",
      "Epoch 447/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4497\n",
      "Epoch 00447: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4497 - val_loss: 1.4683\n",
      "Epoch 448/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4517\n",
      "Epoch 00448: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4517 - val_loss: 1.4736\n",
      "Epoch 449/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4542\n",
      "Epoch 00449: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4542 - val_loss: 1.4752\n",
      "Epoch 450/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00450: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4529 - val_loss: 1.4678\n",
      "Epoch 451/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00451: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4515 - val_loss: 1.4690\n",
      "Epoch 452/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4504\n",
      "Epoch 00452: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4504 - val_loss: 1.4686\n",
      "Epoch 453/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4532\n",
      "Epoch 00453: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4532 - val_loss: 1.4659\n",
      "Epoch 454/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4537\n",
      "Epoch 00454: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4537 - val_loss: 1.4706\n",
      "Epoch 455/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4516\n",
      "Epoch 00455: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4516 - val_loss: 1.4705\n",
      "Epoch 456/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4507\n",
      "Epoch 00456: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4507 - val_loss: 1.4704\n",
      "Epoch 457/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00457: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4519 - val_loss: 1.4683\n",
      "Epoch 458/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4531\n",
      "Epoch 00458: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4531 - val_loss: 1.4735\n",
      "Epoch 459/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00459: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4525 - val_loss: 1.4689\n",
      "Epoch 460/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4518\n",
      "Epoch 00460: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4518 - val_loss: 1.4733\n",
      "Epoch 461/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4537\n",
      "Epoch 00461: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4537 - val_loss: 1.4750\n",
      "Epoch 462/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4543\n",
      "Epoch 00462: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4543 - val_loss: 1.4718\n",
      "Epoch 463/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4528\n",
      "Epoch 00463: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4528 - val_loss: 1.4679\n",
      "Epoch 464/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4526\n",
      "Epoch 00464: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4526 - val_loss: 1.4724\n",
      "Epoch 465/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4511\n",
      "Epoch 00465: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4511 - val_loss: 1.4692\n",
      "Epoch 466/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00466: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4525 - val_loss: 1.4707\n",
      "Epoch 467/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4510\n",
      "Epoch 00467: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4510 - val_loss: 1.4686\n",
      "Epoch 468/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4494\n",
      "Epoch 00468: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4494 - val_loss: 1.4704\n",
      "Epoch 469/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00469: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4502 - val_loss: 1.4679\n",
      "Epoch 470/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4512\n",
      "Epoch 00470: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4512 - val_loss: 1.4730\n",
      "Epoch 471/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4533\n",
      "Epoch 00471: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4533 - val_loss: 1.4744\n",
      "Epoch 472/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4550\n",
      "Epoch 00472: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4550 - val_loss: 1.4690\n",
      "Epoch 473/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4558\n",
      "Epoch 00473: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4558 - val_loss: 1.4723\n",
      "Epoch 474/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4537\n",
      "Epoch 00474: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4537 - val_loss: 1.4733\n",
      "Epoch 475/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00475: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4525 - val_loss: 1.4676\n",
      "Epoch 476/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4523\n",
      "Epoch 00476: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4523 - val_loss: 1.4723\n",
      "Epoch 477/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00477: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4519 - val_loss: 1.4725\n",
      "Epoch 478/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00478: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4519 - val_loss: 1.4699\n",
      "Epoch 479/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4539\n",
      "Epoch 00479: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4539 - val_loss: 1.4725\n",
      "Epoch 480/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4506\n",
      "Epoch 00480: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4506 - val_loss: 1.4686\n",
      "Epoch 481/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4494\n",
      "Epoch 00481: loss did not improve from 1.44925\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4494 - val_loss: 1.4675\n",
      "Epoch 482/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4490\n",
      "Epoch 00482: loss improved from 1.44925 to 1.44900, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4490 - val_loss: 1.4780\n",
      "Epoch 483/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4561\n",
      "Epoch 00483: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4561 - val_loss: 1.4730\n",
      "Epoch 484/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4547\n",
      "Epoch 00484: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4547 - val_loss: 1.4719\n",
      "Epoch 485/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00485: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4555 - val_loss: 1.4712\n",
      "Epoch 486/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4563\n",
      "Epoch 00486: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4563 - val_loss: 1.4702\n",
      "Epoch 487/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4534\n",
      "Epoch 00487: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4534 - val_loss: 1.4690\n",
      "Epoch 488/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4498\n",
      "Epoch 00488: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4498 - val_loss: 1.4669\n",
      "Epoch 489/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4503\n",
      "Epoch 00489: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4503 - val_loss: 1.4704\n",
      "Epoch 490/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4511\n",
      "Epoch 00490: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4511 - val_loss: 1.4681\n",
      "Epoch 491/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4520\n",
      "Epoch 00491: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4520 - val_loss: 1.4722\n",
      "Epoch 492/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4511\n",
      "Epoch 00492: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4511 - val_loss: 1.4674\n",
      "Epoch 493/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4505\n",
      "Epoch 00493: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4505 - val_loss: 1.4724\n",
      "Epoch 494/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4497\n",
      "Epoch 00494: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4497 - val_loss: 1.4685\n",
      "Epoch 495/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4505\n",
      "Epoch 00495: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4505 - val_loss: 1.4713\n",
      "Epoch 496/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4493\n",
      "Epoch 00496: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4493 - val_loss: 1.4698\n",
      "Epoch 497/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4500\n",
      "Epoch 00497: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4500 - val_loss: 1.4709\n",
      "Epoch 498/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00498: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4502 - val_loss: 1.4755\n",
      "Epoch 499/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4499\n",
      "Epoch 00499: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4499 - val_loss: 1.4679\n",
      "Epoch 500/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4497\n",
      "Epoch 00500: loss did not improve from 1.44900\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4497 - val_loss: 1.4684\n",
      "Epoch 501/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00501: loss improved from 1.44900 to 1.44818, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4482 - val_loss: 1.4740\n",
      "Epoch 502/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4511\n",
      "Epoch 00502: loss did not improve from 1.44818\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4511 - val_loss: 1.4712\n",
      "Epoch 503/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4517\n",
      "Epoch 00503: loss did not improve from 1.44818\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4517 - val_loss: 1.4754\n",
      "Epoch 504/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4520\n",
      "Epoch 00504: loss did not improve from 1.44818\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4520 - val_loss: 1.4709\n",
      "Epoch 505/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4521\n",
      "Epoch 00505: loss did not improve from 1.44818\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4521 - val_loss: 1.4714\n",
      "Epoch 506/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4553\n",
      "Epoch 00506: loss did not improve from 1.44818\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4553 - val_loss: 1.4765\n",
      "Epoch 507/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00507: loss did not improve from 1.44818\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4551 - val_loss: 1.4754\n",
      "Epoch 508/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4531\n",
      "Epoch 00508: loss did not improve from 1.44818\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4531 - val_loss: 1.4718\n",
      "Epoch 509/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4489\n",
      "Epoch 00509: loss did not improve from 1.44818\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4489 - val_loss: 1.4690\n",
      "Epoch 510/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4489\n",
      "Epoch 00510: loss did not improve from 1.44818\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4489 - val_loss: 1.4690\n",
      "Epoch 511/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4480\n",
      "Epoch 00511: loss improved from 1.44818 to 1.44804, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4480 - val_loss: 1.4714\n",
      "Epoch 512/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4499\n",
      "Epoch 00512: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4499 - val_loss: 1.4749\n",
      "Epoch 513/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4509\n",
      "Epoch 00513: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4509 - val_loss: 1.4720\n",
      "Epoch 514/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4504\n",
      "Epoch 00514: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4504 - val_loss: 1.4714\n",
      "Epoch 515/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4543\n",
      "Epoch 00515: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4543 - val_loss: 1.4704\n",
      "Epoch 516/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4538\n",
      "Epoch 00516: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4538 - val_loss: 1.4731\n",
      "Epoch 517/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4522\n",
      "Epoch 00517: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4522 - val_loss: 1.4695\n",
      "Epoch 518/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00518: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4502 - val_loss: 1.4738\n",
      "Epoch 519/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4495\n",
      "Epoch 00519: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4495 - val_loss: 1.4724\n",
      "Epoch 520/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4495\n",
      "Epoch 00520: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4495 - val_loss: 1.4695\n",
      "Epoch 521/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4505\n",
      "Epoch 00521: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4505 - val_loss: 1.4747\n",
      "Epoch 522/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4533\n",
      "Epoch 00522: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4533 - val_loss: 1.4787\n",
      "Epoch 523/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4541\n",
      "Epoch 00523: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4541 - val_loss: 1.4721\n",
      "Epoch 524/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4532\n",
      "Epoch 00524: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4532 - val_loss: 1.4688\n",
      "Epoch 525/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4484\n",
      "Epoch 00525: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4484 - val_loss: 1.4747\n",
      "Epoch 526/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4521\n",
      "Epoch 00526: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4521 - val_loss: 1.4737\n",
      "Epoch 527/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4495\n",
      "Epoch 00527: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4495 - val_loss: 1.4706\n",
      "Epoch 528/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4491\n",
      "Epoch 00528: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4491 - val_loss: 1.4741\n",
      "Epoch 529/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4471\n",
      "Epoch 00529: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4503 - val_loss: 1.4746\n",
      "Epoch 530/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4517\n",
      "Epoch 00530: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4517 - val_loss: 1.4685\n",
      "Epoch 531/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00531: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4502 - val_loss: 1.4685\n",
      "Epoch 532/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4508\n",
      "Epoch 00532: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4508 - val_loss: 1.4666\n",
      "Epoch 533/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4504\n",
      "Epoch 00533: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4504 - val_loss: 1.4734\n",
      "Epoch 534/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4556\n",
      "Epoch 00534: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4556 - val_loss: 1.4747\n",
      "Epoch 535/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4558\n",
      "Epoch 00535: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4558 - val_loss: 1.4681\n",
      "Epoch 536/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4511\n",
      "Epoch 00536: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4523 - val_loss: 1.4723\n",
      "Epoch 537/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00537: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4519 - val_loss: 1.4774\n",
      "Epoch 538/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00538: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4551 - val_loss: 1.4692\n",
      "Epoch 539/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4508\n",
      "Epoch 00539: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4508 - val_loss: 1.4729\n",
      "Epoch 540/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4511\n",
      "Epoch 00540: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4511 - val_loss: 1.4744\n",
      "Epoch 541/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4512\n",
      "Epoch 00541: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4512 - val_loss: 1.4705\n",
      "Epoch 542/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4501\n",
      "Epoch 00542: loss did not improve from 1.44804\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4501 - val_loss: 1.4688\n",
      "Epoch 543/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4469\n",
      "Epoch 00543: loss improved from 1.44804 to 1.44691, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4469 - val_loss: 1.4688\n",
      "Epoch 544/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4481\n",
      "Epoch 00544: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4481 - val_loss: 1.4710\n",
      "Epoch 545/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00545: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4482 - val_loss: 1.4730\n",
      "Epoch 546/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4499\n",
      "Epoch 00546: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4499 - val_loss: 1.4775\n",
      "Epoch 547/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4510\n",
      "Epoch 00547: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4510 - val_loss: 1.4703\n",
      "Epoch 548/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4503\n",
      "Epoch 00548: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4503 - val_loss: 1.4708\n",
      "Epoch 549/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4497\n",
      "Epoch 00549: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4483 - val_loss: 1.4733\n",
      "Epoch 550/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00550: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4502 - val_loss: 1.4689\n",
      "Epoch 551/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4485\n",
      "Epoch 00551: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4485 - val_loss: 1.4714\n",
      "Epoch 552/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00552: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4482 - val_loss: 1.4736\n",
      "Epoch 553/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4524\n",
      "Epoch 00553: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4524 - val_loss: 1.4694\n",
      "Epoch 554/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00554: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4502 - val_loss: 1.4748\n",
      "Epoch 555/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4503\n",
      "Epoch 00555: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4503 - val_loss: 1.4724\n",
      "Epoch 556/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4503\n",
      "Epoch 00556: loss did not improve from 1.44691\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4503 - val_loss: 1.4697\n",
      "Epoch 557/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4462\n",
      "Epoch 00557: loss improved from 1.44691 to 1.44615, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4462 - val_loss: 1.4695\n",
      "Epoch 558/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00558: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4482 - val_loss: 1.4695\n",
      "Epoch 559/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4484\n",
      "Epoch 00559: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4484 - val_loss: 1.4735\n",
      "Epoch 560/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4499\n",
      "Epoch 00560: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4499 - val_loss: 1.4735\n",
      "Epoch 561/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4527\n",
      "Epoch 00561: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4527 - val_loss: 1.4786\n",
      "Epoch 562/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4510\n",
      "Epoch 00562: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4510 - val_loss: 1.4732\n",
      "Epoch 563/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4494\n",
      "Epoch 00563: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4494 - val_loss: 1.4699\n",
      "Epoch 564/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00564: loss improved from 1.44615 to 1.44615, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4461 - val_loss: 1.4693\n",
      "Epoch 565/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4488\n",
      "Epoch 00565: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4488 - val_loss: 1.4713\n",
      "Epoch 566/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4486\n",
      "Epoch 00566: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4486 - val_loss: 1.4722\n",
      "Epoch 567/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4514\n",
      "Epoch 00567: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4514 - val_loss: 1.4772\n",
      "Epoch 568/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4521\n",
      "Epoch 00568: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4521 - val_loss: 1.4735\n",
      "Epoch 569/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00569: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4502 - val_loss: 1.4708\n",
      "Epoch 570/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4497\n",
      "Epoch 00570: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4497 - val_loss: 1.4763\n",
      "Epoch 571/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4499\n",
      "Epoch 00571: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4499 - val_loss: 1.4727\n",
      "Epoch 572/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00572: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4515 - val_loss: 1.4700\n",
      "Epoch 573/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4516\n",
      "Epoch 00573: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4516 - val_loss: 1.4718\n",
      "Epoch 574/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00574: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4515 - val_loss: 1.4716\n",
      "Epoch 575/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4496\n",
      "Epoch 00575: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4496 - val_loss: 1.4683\n",
      "Epoch 576/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4542\n",
      "Epoch 00576: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4495 - val_loss: 1.4749\n",
      "Epoch 577/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4526\n",
      "Epoch 00577: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4526 - val_loss: 1.4721\n",
      "Epoch 578/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4507\n",
      "Epoch 00578: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4507 - val_loss: 1.4678\n",
      "Epoch 579/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4724\n",
      "Epoch 00579: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4585 - val_loss: 1.4723\n",
      "Epoch 580/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4536\n",
      "Epoch 00580: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4536 - val_loss: 1.4704\n",
      "Epoch 581/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4508\n",
      "Epoch 00581: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4508 - val_loss: 1.4716\n",
      "Epoch 582/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4498\n",
      "Epoch 00582: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4498 - val_loss: 1.4716\n",
      "Epoch 583/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4491\n",
      "Epoch 00583: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4491 - val_loss: 1.4705\n",
      "Epoch 584/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00584: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4492 - val_loss: 1.4783\n",
      "Epoch 585/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4532\n",
      "Epoch 00585: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4532 - val_loss: 1.4763\n",
      "Epoch 586/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4506\n",
      "Epoch 00586: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4506 - val_loss: 1.4722\n",
      "Epoch 587/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4486\n",
      "Epoch 00587: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4486 - val_loss: 1.4766\n",
      "Epoch 588/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4605\n",
      "Epoch 00588: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4533 - val_loss: 1.4758\n",
      "Epoch 589/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4546\n",
      "Epoch 00589: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4546 - val_loss: 1.4732\n",
      "Epoch 590/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4568\n",
      "Epoch 00590: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4568 - val_loss: 1.4748\n",
      "Epoch 591/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4494\n",
      "Epoch 00591: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4494 - val_loss: 1.4703\n",
      "Epoch 592/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00592: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4470 - val_loss: 1.4691\n",
      "Epoch 593/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4507\n",
      "Epoch 00593: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4507 - val_loss: 1.4685\n",
      "Epoch 594/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4494\n",
      "Epoch 00594: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4494 - val_loss: 1.4746\n",
      "Epoch 595/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00595: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4482 - val_loss: 1.4713\n",
      "Epoch 596/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4475\n",
      "Epoch 00596: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4475 - val_loss: 1.4710\n",
      "Epoch 597/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4471\n",
      "Epoch 00597: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4471 - val_loss: 1.4697\n",
      "Epoch 598/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4468\n",
      "Epoch 00598: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4468 - val_loss: 1.4701\n",
      "Epoch 599/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4469\n",
      "Epoch 00599: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4469 - val_loss: 1.4707\n",
      "Epoch 600/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4473\n",
      "Epoch 00600: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4473 - val_loss: 1.4682\n",
      "Epoch 601/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00601: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4470 - val_loss: 1.4704\n",
      "Epoch 602/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4466\n",
      "Epoch 00602: loss did not improve from 1.44615\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4466 - val_loss: 1.4690\n",
      "Epoch 603/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4457\n",
      "Epoch 00603: loss improved from 1.44615 to 1.44573, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4457 - val_loss: 1.4679\n",
      "Epoch 604/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00604: loss did not improve from 1.44573\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4467 - val_loss: 1.4698\n",
      "Epoch 605/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4451\n",
      "Epoch 00605: loss improved from 1.44573 to 1.44511, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4451 - val_loss: 1.4674\n",
      "Epoch 606/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4456\n",
      "Epoch 00606: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4456 - val_loss: 1.4685\n",
      "Epoch 607/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4475\n",
      "Epoch 00607: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4475 - val_loss: 1.4702\n",
      "Epoch 608/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4473\n",
      "Epoch 00608: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4473 - val_loss: 1.4711\n",
      "Epoch 609/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4459\n",
      "Epoch 00609: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4459 - val_loss: 1.4722\n",
      "Epoch 610/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4480\n",
      "Epoch 00610: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4480 - val_loss: 1.4710\n",
      "Epoch 611/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4499\n",
      "Epoch 00611: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4499 - val_loss: 1.4708\n",
      "Epoch 612/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4510\n",
      "Epoch 00612: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4510 - val_loss: 1.4752\n",
      "Epoch 613/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4473\n",
      "Epoch 00613: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4473 - val_loss: 1.4713\n",
      "Epoch 614/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4500\n",
      "Epoch 00614: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4500 - val_loss: 1.4708\n",
      "Epoch 615/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4485\n",
      "Epoch 00615: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4485 - val_loss: 1.4708\n",
      "Epoch 616/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4485\n",
      "Epoch 00616: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4485 - val_loss: 1.4705\n",
      "Epoch 617/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4591\n",
      "Epoch 00617: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4591 - val_loss: 1.4803\n",
      "Epoch 618/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4541\n",
      "Epoch 00618: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4541 - val_loss: 1.4737\n",
      "Epoch 619/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4507\n",
      "Epoch 00619: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4507 - val_loss: 1.4746\n",
      "Epoch 620/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4497\n",
      "Epoch 00620: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4497 - val_loss: 1.4734\n",
      "Epoch 621/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4494\n",
      "Epoch 00621: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4494 - val_loss: 1.4750\n",
      "Epoch 622/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4484\n",
      "Epoch 00622: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4484 - val_loss: 1.4726\n",
      "Epoch 623/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4499\n",
      "Epoch 00623: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4499 - val_loss: 1.4700\n",
      "Epoch 624/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00624: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4502 - val_loss: 1.4732\n",
      "Epoch 625/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4480\n",
      "Epoch 00625: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4480 - val_loss: 1.4771\n",
      "Epoch 626/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4503\n",
      "Epoch 00626: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4503 - val_loss: 1.4749\n",
      "Epoch 627/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4477\n",
      "Epoch 00627: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4477 - val_loss: 1.4682\n",
      "Epoch 628/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4483\n",
      "Epoch 00628: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4483 - val_loss: 1.4705\n",
      "Epoch 629/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4472\n",
      "Epoch 00629: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4472 - val_loss: 1.4701\n",
      "Epoch 630/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4478\n",
      "Epoch 00630: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4478 - val_loss: 1.4678\n",
      "Epoch 631/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00631: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4461 - val_loss: 1.4768\n",
      "Epoch 632/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4479\n",
      "Epoch 00632: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4479 - val_loss: 1.4734\n",
      "Epoch 633/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00633: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4464 - val_loss: 1.4689\n",
      "Epoch 634/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4485\n",
      "Epoch 00634: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4485 - val_loss: 1.4716\n",
      "Epoch 635/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00635: loss did not improve from 1.44511\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4461 - val_loss: 1.4675\n",
      "Epoch 636/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4445\n",
      "Epoch 00636: loss improved from 1.44511 to 1.44454, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4445 - val_loss: 1.4714\n",
      "Epoch 637/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4469\n",
      "Epoch 00637: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4469 - val_loss: 1.4728\n",
      "Epoch 638/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4514\n",
      "Epoch 00638: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4514 - val_loss: 1.4717\n",
      "Epoch 639/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00639: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4515 - val_loss: 1.4755\n",
      "Epoch 640/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4513\n",
      "Epoch 00640: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4513 - val_loss: 1.4715\n",
      "Epoch 641/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4487\n",
      "Epoch 00641: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4487 - val_loss: 1.4722\n",
      "Epoch 642/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00642: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4551 - val_loss: 1.4806\n",
      "Epoch 643/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4532\n",
      "Epoch 00643: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4532 - val_loss: 1.4738\n",
      "Epoch 644/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4514\n",
      "Epoch 00644: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4514 - val_loss: 1.4779\n",
      "Epoch 645/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4514\n",
      "Epoch 00645: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4514 - val_loss: 1.4741\n",
      "Epoch 646/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4465\n",
      "Epoch 00646: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4465 - val_loss: 1.4739\n",
      "Epoch 647/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4486\n",
      "Epoch 00647: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4486 - val_loss: 1.4761\n",
      "Epoch 648/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4487\n",
      "Epoch 00648: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4487 - val_loss: 1.4741\n",
      "Epoch 649/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4474\n",
      "Epoch 00649: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4474 - val_loss: 1.4709\n",
      "Epoch 650/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4447\n",
      "Epoch 00650: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4447 - val_loss: 1.4666\n",
      "Epoch 651/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00651: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4529 - val_loss: 1.4745\n",
      "Epoch 652/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4539\n",
      "Epoch 00652: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4539 - val_loss: 1.4749\n",
      "Epoch 653/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4505\n",
      "Epoch 00653: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4505 - val_loss: 1.4720\n",
      "Epoch 654/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4498\n",
      "Epoch 00654: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4498 - val_loss: 1.4729\n",
      "Epoch 655/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 00655: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4463 - val_loss: 1.4695\n",
      "Epoch 656/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00656: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4467 - val_loss: 1.4707\n",
      "Epoch 657/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4526\n",
      "Epoch 00657: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4526 - val_loss: 1.4727\n",
      "Epoch 658/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4508\n",
      "Epoch 00658: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4508 - val_loss: 1.4756\n",
      "Epoch 659/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4489\n",
      "Epoch 00659: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4489 - val_loss: 1.4722\n",
      "Epoch 660/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4516\n",
      "Epoch 00660: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4516 - val_loss: 1.4740\n",
      "Epoch 661/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4486\n",
      "Epoch 00661: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4486 - val_loss: 1.4749\n",
      "Epoch 662/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4478\n",
      "Epoch 00662: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4478 - val_loss: 1.4734\n",
      "Epoch 663/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4479\n",
      "Epoch 00663: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4479 - val_loss: 1.4700\n",
      "Epoch 664/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4477\n",
      "Epoch 00664: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4477 - val_loss: 1.4760\n",
      "Epoch 665/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4469\n",
      "Epoch 00665: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4469 - val_loss: 1.4688\n",
      "Epoch 666/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 00666: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4463 - val_loss: 1.4692\n",
      "Epoch 667/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00667: loss did not improve from 1.44454\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4448 - val_loss: 1.4717\n",
      "Epoch 668/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00668: loss improved from 1.44454 to 1.44417, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4442 - val_loss: 1.4731\n",
      "Epoch 669/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4485\n",
      "Epoch 00669: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4485 - val_loss: 1.4698\n",
      "Epoch 670/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4446\n",
      "Epoch 00670: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4446 - val_loss: 1.4686\n",
      "Epoch 671/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4458\n",
      "Epoch 00671: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4458 - val_loss: 1.4723\n",
      "Epoch 672/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4491\n",
      "Epoch 00672: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4491 - val_loss: 1.4785\n",
      "Epoch 673/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4499\n",
      "Epoch 00673: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4499 - val_loss: 1.4717\n",
      "Epoch 674/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4466\n",
      "Epoch 00674: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4466 - val_loss: 1.4756\n",
      "Epoch 675/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4508\n",
      "Epoch 00675: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4508 - val_loss: 1.4764\n",
      "Epoch 676/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4446\n",
      "Epoch 00676: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4488 - val_loss: 1.4736\n",
      "Epoch 677/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4494\n",
      "Epoch 00677: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4494 - val_loss: 1.4710\n",
      "Epoch 678/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4478\n",
      "Epoch 00678: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4478 - val_loss: 1.4701\n",
      "Epoch 679/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00679: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4464 - val_loss: 1.4702\n",
      "Epoch 680/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4501\n",
      "Epoch 00680: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4501 - val_loss: 1.4731\n",
      "Epoch 681/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4534\n",
      "Epoch 00681: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4534 - val_loss: 1.4789\n",
      "Epoch 682/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4505\n",
      "Epoch 00682: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4505 - val_loss: 1.4745\n",
      "Epoch 683/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4489\n",
      "Epoch 00683: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4489 - val_loss: 1.4737\n",
      "Epoch 684/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00684: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4467 - val_loss: 1.4757\n",
      "Epoch 685/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00685: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4467 - val_loss: 1.4717\n",
      "Epoch 686/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4495\n",
      "Epoch 00686: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4495 - val_loss: 1.4734\n",
      "Epoch 687/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4491\n",
      "Epoch 00687: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4491 - val_loss: 1.4757\n",
      "Epoch 688/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4466\n",
      "Epoch 00688: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4466 - val_loss: 1.4741\n",
      "Epoch 689/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00689: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4482 - val_loss: 1.4734\n",
      "Epoch 690/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4459\n",
      "Epoch 00690: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4459 - val_loss: 1.4701\n",
      "Epoch 691/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00691: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4442 - val_loss: 1.4700\n",
      "Epoch 692/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00692: loss did not improve from 1.44417\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4450 - val_loss: 1.4719\n",
      "Epoch 693/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 00693: loss improved from 1.44417 to 1.44370, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4437 - val_loss: 1.4748\n",
      "Epoch 694/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4471\n",
      "Epoch 00694: loss did not improve from 1.44370\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4471 - val_loss: 1.4704\n",
      "Epoch 695/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4462\n",
      "Epoch 00695: loss did not improve from 1.44370\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4462 - val_loss: 1.4716\n",
      "Epoch 696/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4468\n",
      "Epoch 00696: loss did not improve from 1.44370\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4468 - val_loss: 1.4732\n",
      "Epoch 697/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 00697: loss improved from 1.44370 to 1.44367, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4437 - val_loss: 1.4681\n",
      "Epoch 698/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4449\n",
      "Epoch 00698: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4449 - val_loss: 1.4697\n",
      "Epoch 699/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00699: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4448 - val_loss: 1.4718\n",
      "Epoch 700/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4441\n",
      "Epoch 00700: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4441 - val_loss: 1.4700\n",
      "Epoch 701/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4438\n",
      "Epoch 00701: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4438 - val_loss: 1.4716\n",
      "Epoch 702/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4438\n",
      "Epoch 00702: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4438 - val_loss: 1.4732\n",
      "Epoch 703/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00703: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4464 - val_loss: 1.4712\n",
      "Epoch 704/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00704: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4448 - val_loss: 1.4732\n",
      "Epoch 705/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4459\n",
      "Epoch 00705: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4459 - val_loss: 1.4697\n",
      "Epoch 706/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4455\n",
      "Epoch 00706: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4455 - val_loss: 1.4721\n",
      "Epoch 707/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4465\n",
      "Epoch 00707: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4465 - val_loss: 1.4702\n",
      "Epoch 708/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4478\n",
      "Epoch 00708: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4478 - val_loss: 1.4719\n",
      "Epoch 709/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00709: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4461 - val_loss: 1.4708\n",
      "Epoch 710/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4476\n",
      "Epoch 00710: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4476 - val_loss: 1.4813\n",
      "Epoch 711/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4501\n",
      "Epoch 00711: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4501 - val_loss: 1.4802\n",
      "Epoch 712/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4462\n",
      "Epoch 00712: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4462 - val_loss: 1.4735\n",
      "Epoch 713/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4454\n",
      "Epoch 00713: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4454 - val_loss: 1.4716\n",
      "Epoch 714/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4438\n",
      "Epoch 00714: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4438 - val_loss: 1.4716\n",
      "Epoch 715/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4455\n",
      "Epoch 00715: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4455 - val_loss: 1.4714\n",
      "Epoch 716/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4457\n",
      "Epoch 00716: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4457 - val_loss: 1.4733\n",
      "Epoch 717/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4455\n",
      "Epoch 00717: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4455 - val_loss: 1.4694\n",
      "Epoch 718/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4556\n",
      "Epoch 00718: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4556 - val_loss: 1.4758\n",
      "Epoch 719/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4528\n",
      "Epoch 00719: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4528 - val_loss: 1.4773\n",
      "Epoch 720/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00720: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4492 - val_loss: 1.4709\n",
      "Epoch 721/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4501\n",
      "Epoch 00721: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4501 - val_loss: 1.4744\n",
      "Epoch 722/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4514\n",
      "Epoch 00722: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4514 - val_loss: 1.4765\n",
      "Epoch 723/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4504\n",
      "Epoch 00723: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4504 - val_loss: 1.4756\n",
      "Epoch 724/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4477\n",
      "Epoch 00724: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4477 - val_loss: 1.4696\n",
      "Epoch 725/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00725: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4444 - val_loss: 1.4762\n",
      "Epoch 726/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4466\n",
      "Epoch 00726: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4466 - val_loss: 1.4757\n",
      "Epoch 727/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4457\n",
      "Epoch 00727: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4457 - val_loss: 1.4737\n",
      "Epoch 728/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4454\n",
      "Epoch 00728: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4454 - val_loss: 1.4784\n",
      "Epoch 729/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4530\n",
      "Epoch 00729: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4530 - val_loss: 1.4750\n",
      "Epoch 730/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4532\n",
      "Epoch 00730: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4532 - val_loss: 1.4714\n",
      "Epoch 731/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4509\n",
      "Epoch 00731: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4509 - val_loss: 1.4719\n",
      "Epoch 732/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4453\n",
      "Epoch 00732: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4453 - val_loss: 1.4744\n",
      "Epoch 733/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4457\n",
      "Epoch 00733: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4448 - val_loss: 1.4736\n",
      "Epoch 734/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00734: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4444 - val_loss: 1.4724\n",
      "Epoch 735/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4432\n",
      "Epoch 00735: loss improved from 1.44367 to 1.44324, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4432 - val_loss: 1.4726\n",
      "Epoch 736/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4434\n",
      "Epoch 00736: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4434 - val_loss: 1.4719\n",
      "Epoch 737/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00737: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4467 - val_loss: 1.4713\n",
      "Epoch 738/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4433\n",
      "Epoch 00738: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4450 - val_loss: 1.4722\n",
      "Epoch 739/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4593\n",
      "Epoch 00739: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4474 - val_loss: 1.4763\n",
      "Epoch 740/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4535\n",
      "Epoch 00740: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4535 - val_loss: 1.4744\n",
      "Epoch 741/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4504\n",
      "Epoch 00741: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4504 - val_loss: 1.4763\n",
      "Epoch 742/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00742: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4470 - val_loss: 1.4752\n",
      "Epoch 743/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4475\n",
      "Epoch 00743: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4475 - val_loss: 1.4730\n",
      "Epoch 744/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 00744: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4439 - val_loss: 1.4727\n",
      "Epoch 745/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4452\n",
      "Epoch 00745: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4452 - val_loss: 1.4751\n",
      "Epoch 746/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4445\n",
      "Epoch 00746: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4445 - val_loss: 1.4742\n",
      "Epoch 747/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4445\n",
      "Epoch 00747: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4445 - val_loss: 1.4699\n",
      "Epoch 748/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4512\n",
      "Epoch 00748: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4466 - val_loss: 1.4701\n",
      "Epoch 749/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 00749: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4439 - val_loss: 1.4714\n",
      "Epoch 750/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4471\n",
      "Epoch 00750: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4474 - val_loss: 1.4747\n",
      "Epoch 751/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4453\n",
      "Epoch 00751: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4453 - val_loss: 1.4736\n",
      "Epoch 752/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4466\n",
      "Epoch 00752: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4466 - val_loss: 1.4724\n",
      "Epoch 753/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4472\n",
      "Epoch 00753: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4472 - val_loss: 1.4710\n",
      "Epoch 754/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4447\n",
      "Epoch 00754: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4447 - val_loss: 1.4749\n",
      "Epoch 755/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4486\n",
      "Epoch 00755: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4479 - val_loss: 1.4794\n",
      "Epoch 756/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4484\n",
      "Epoch 00756: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4484 - val_loss: 1.4764\n",
      "Epoch 757/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4471\n",
      "Epoch 00757: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4471 - val_loss: 1.4708\n",
      "Epoch 758/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00758: loss did not improve from 1.44324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4444 - val_loss: 1.4702\n",
      "Epoch 759/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4431\n",
      "Epoch 00759: loss improved from 1.44324 to 1.44311, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4431 - val_loss: 1.4739\n",
      "Epoch 760/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00760: loss improved from 1.44311 to 1.44297, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4430 - val_loss: 1.4725\n",
      "Epoch 761/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4456\n",
      "Epoch 00761: loss did not improve from 1.44297\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4456 - val_loss: 1.4715\n",
      "Epoch 762/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00762: loss did not improve from 1.44297\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4450 - val_loss: 1.4735\n",
      "Epoch 763/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 00763: loss did not improve from 1.44297\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4435 - val_loss: 1.4724\n",
      "Epoch 764/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4431\n",
      "Epoch 00764: loss did not improve from 1.44297\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4431 - val_loss: 1.4707\n",
      "Epoch 765/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00765: loss did not improve from 1.44297\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4448 - val_loss: 1.4701\n",
      "Epoch 766/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4449\n",
      "Epoch 00766: loss improved from 1.44297 to 1.44296, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4430 - val_loss: 1.4764\n",
      "Epoch 767/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4570\n",
      "Epoch 00767: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4460 - val_loss: 1.4786\n",
      "Epoch 768/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4462\n",
      "Epoch 00768: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4462 - val_loss: 1.4703\n",
      "Epoch 769/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00769: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4444 - val_loss: 1.4745\n",
      "Epoch 770/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4449\n",
      "Epoch 00770: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4449 - val_loss: 1.4756\n",
      "Epoch 771/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4436\n",
      "Epoch 00771: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4436 - val_loss: 1.4743\n",
      "Epoch 772/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4384\n",
      "Epoch 00772: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4447 - val_loss: 1.4717\n",
      "Epoch 773/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00773: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4444 - val_loss: 1.4779\n",
      "Epoch 774/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4458\n",
      "Epoch 00774: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4458 - val_loss: 1.4729\n",
      "Epoch 775/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00775: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4446 - val_loss: 1.4729\n",
      "Epoch 776/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4446\n",
      "Epoch 00776: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4446 - val_loss: 1.4756\n",
      "Epoch 777/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4480\n",
      "Epoch 00777: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4480 - val_loss: 1.4773\n",
      "Epoch 778/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4397\n",
      "Epoch 00778: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4492 - val_loss: 1.4805\n",
      "Epoch 779/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4478\n",
      "Epoch 00779: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4478 - val_loss: 1.4735\n",
      "Epoch 780/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4488\n",
      "Epoch 00780: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4488 - val_loss: 1.4747\n",
      "Epoch 781/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00781: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4470 - val_loss: 1.4763\n",
      "Epoch 782/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00782: loss did not improve from 1.44296\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4450 - val_loss: 1.4701\n",
      "Epoch 783/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 00783: loss improved from 1.44296 to 1.44285, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4429 - val_loss: 1.4743\n",
      "Epoch 784/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 00784: loss improved from 1.44285 to 1.44214, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4421 - val_loss: 1.4719\n",
      "Epoch 785/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4477\n",
      "Epoch 00785: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4477 - val_loss: 1.4712\n",
      "Epoch 786/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4459\n",
      "Epoch 00786: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4459 - val_loss: 1.4747\n",
      "Epoch 787/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4459\n",
      "Epoch 00787: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4459 - val_loss: 1.4726\n",
      "Epoch 788/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4443\n",
      "Epoch 00788: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4443 - val_loss: 1.4719\n",
      "Epoch 789/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4422\n",
      "Epoch 00789: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4438 - val_loss: 1.4736\n",
      "Epoch 790/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00790: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4430 - val_loss: 1.4706\n",
      "Epoch 791/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4443\n",
      "Epoch 00791: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4443 - val_loss: 1.4718\n",
      "Epoch 792/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4466\n",
      "Epoch 00792: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4466 - val_loss: 1.4764\n",
      "Epoch 793/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4447\n",
      "Epoch 00793: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4447 - val_loss: 1.4781\n",
      "Epoch 794/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4441\n",
      "Epoch 00794: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4441 - val_loss: 1.4722\n",
      "Epoch 795/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4426\n",
      "Epoch 00795: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4426 - val_loss: 1.4726\n",
      "Epoch 796/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4451\n",
      "Epoch 00796: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4451 - val_loss: 1.4720\n",
      "Epoch 797/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4446\n",
      "Epoch 00797: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4446 - val_loss: 1.4751\n",
      "Epoch 798/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00798: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4464 - val_loss: 1.4737\n",
      "Epoch 799/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00799: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4482 - val_loss: 1.4764\n",
      "Epoch 800/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00800: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4444 - val_loss: 1.4691\n",
      "Epoch 801/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00801: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4430 - val_loss: 1.4702\n",
      "Epoch 802/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 00802: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4437 - val_loss: 1.4746\n",
      "Epoch 803/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4434\n",
      "Epoch 00803: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4434 - val_loss: 1.4753\n",
      "Epoch 804/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4443\n",
      "Epoch 00804: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4443 - val_loss: 1.4778\n",
      "Epoch 805/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00805: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4464 - val_loss: 1.4710\n",
      "Epoch 806/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4431\n",
      "Epoch 00806: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4422 - val_loss: 1.4737\n",
      "Epoch 807/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4503\n",
      "Epoch 00807: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4445 - val_loss: 1.4712\n",
      "Epoch 808/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4446\n",
      "Epoch 00808: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4446 - val_loss: 1.4762\n",
      "Epoch 809/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4527\n",
      "Epoch 00809: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4458 - val_loss: 1.4713\n",
      "Epoch 810/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 00810: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4427 - val_loss: 1.4760\n",
      "Epoch 811/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4489\n",
      "Epoch 00811: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4489 - val_loss: 1.4738\n",
      "Epoch 812/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00812: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4450 - val_loss: 1.4770\n",
      "Epoch 813/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4330\n",
      "Epoch 00813: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4441 - val_loss: 1.4728\n",
      "Epoch 814/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00814: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4467 - val_loss: 1.4742\n",
      "Epoch 815/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4446\n",
      "Epoch 00815: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4446 - val_loss: 1.4722\n",
      "Epoch 816/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4428\n",
      "Epoch 00816: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4428 - val_loss: 1.4744\n",
      "Epoch 817/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4446\n",
      "Epoch 00817: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4446 - val_loss: 1.4751\n",
      "Epoch 818/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4451\n",
      "Epoch 00818: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4451 - val_loss: 1.4712\n",
      "Epoch 819/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4451\n",
      "Epoch 00819: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4451 - val_loss: 1.4748\n",
      "Epoch 820/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 00820: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4437 - val_loss: 1.4741\n",
      "Epoch 821/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00821: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4464 - val_loss: 1.4705\n",
      "Epoch 822/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 00822: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4439 - val_loss: 1.4734\n",
      "Epoch 823/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4445\n",
      "Epoch 00823: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4445 - val_loss: 1.4747\n",
      "Epoch 824/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 00824: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4435 - val_loss: 1.4723\n",
      "Epoch 825/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00825: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4430 - val_loss: 1.4768\n",
      "Epoch 826/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00826: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4442 - val_loss: 1.4734\n",
      "Epoch 827/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4367\n",
      "Epoch 00827: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4455 - val_loss: 1.4696\n",
      "Epoch 828/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00828: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4461 - val_loss: 1.4745\n",
      "Epoch 829/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4441\n",
      "Epoch 00829: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4441 - val_loss: 1.4696\n",
      "Epoch 830/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00830: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4450 - val_loss: 1.4735\n",
      "Epoch 831/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00831: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4448 - val_loss: 1.4740\n",
      "Epoch 832/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 00832: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4439 - val_loss: 1.4749\n",
      "Epoch 833/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00833: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4442 - val_loss: 1.4760\n",
      "Epoch 834/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00834: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4450 - val_loss: 1.4733\n",
      "Epoch 835/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4343\n",
      "Epoch 00835: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4440 - val_loss: 1.4701\n",
      "Epoch 836/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 00836: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4429 - val_loss: 1.4749\n",
      "Epoch 837/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4449\n",
      "Epoch 00837: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4449 - val_loss: 1.4716\n",
      "Epoch 838/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4455\n",
      "Epoch 00838: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4455 - val_loss: 1.4744\n",
      "Epoch 839/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4454\n",
      "Epoch 00839: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4454 - val_loss: 1.4735\n",
      "Epoch 840/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4478\n",
      "Epoch 00840: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4442 - val_loss: 1.4740\n",
      "Epoch 841/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4458\n",
      "Epoch 00841: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4458 - val_loss: 1.4698\n",
      "Epoch 842/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4432\n",
      "Epoch 00842: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4432 - val_loss: 1.4771\n",
      "Epoch 843/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4452\n",
      "Epoch 00843: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4452 - val_loss: 1.4767\n",
      "Epoch 844/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 00844: loss did not improve from 1.44214\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4435 - val_loss: 1.4758\n",
      "Epoch 845/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4415\n",
      "Epoch 00845: loss improved from 1.44214 to 1.44149, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4415 - val_loss: 1.4738\n",
      "Epoch 846/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 00846: loss did not improve from 1.44149\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4437 - val_loss: 1.4792\n",
      "Epoch 847/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00847: loss did not improve from 1.44149\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4464 - val_loss: 1.4794\n",
      "Epoch 848/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4466\n",
      "Epoch 00848: loss did not improve from 1.44149\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4466 - val_loss: 1.4741\n",
      "Epoch 849/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4445\n",
      "Epoch 00849: loss did not improve from 1.44149\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4445 - val_loss: 1.4745\n",
      "Epoch 850/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4441\n",
      "Epoch 00850: loss did not improve from 1.44149\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4441 - val_loss: 1.4723\n",
      "Epoch 851/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4416\n",
      "Epoch 00851: loss did not improve from 1.44149\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4416 - val_loss: 1.4737\n",
      "Epoch 852/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 00852: loss did not improve from 1.44149\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4429 - val_loss: 1.4738\n",
      "Epoch 853/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4403\n",
      "Epoch 00853: loss improved from 1.44149 to 1.44028, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4403 - val_loss: 1.4730\n",
      "Epoch 854/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4425\n",
      "Epoch 00854: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4425 - val_loss: 1.4735\n",
      "Epoch 855/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 00855: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4411 - val_loss: 1.4717\n",
      "Epoch 856/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4433\n",
      "Epoch 00856: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4433 - val_loss: 1.4720\n",
      "Epoch 857/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4410\n",
      "Epoch 00857: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4410 - val_loss: 1.4757\n",
      "Epoch 858/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4434\n",
      "Epoch 00858: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4434 - val_loss: 1.4763\n",
      "Epoch 859/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00859: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4450 - val_loss: 1.4758\n",
      "Epoch 860/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4438\n",
      "Epoch 00860: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4438 - val_loss: 1.4738\n",
      "Epoch 861/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 00861: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4435 - val_loss: 1.4720\n",
      "Epoch 862/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4475\n",
      "Epoch 00862: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4475 - val_loss: 1.4740\n",
      "Epoch 863/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4466\n",
      "Epoch 00863: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4466 - val_loss: 1.4729\n",
      "Epoch 864/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4447\n",
      "Epoch 00864: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4447 - val_loss: 1.4753\n",
      "Epoch 865/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4447\n",
      "Epoch 00865: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4447 - val_loss: 1.4738\n",
      "Epoch 866/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 00866: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4439 - val_loss: 1.4794\n",
      "Epoch 867/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4447\n",
      "Epoch 00867: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4447 - val_loss: 1.4719\n",
      "Epoch 868/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 00868: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4420 - val_loss: 1.4763\n",
      "Epoch 869/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4474\n",
      "Epoch 00869: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4474 - val_loss: 1.4756\n",
      "Epoch 870/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 00870: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4427 - val_loss: 1.4751\n",
      "Epoch 871/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4487\n",
      "Epoch 00871: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4487 - val_loss: 1.4752\n",
      "Epoch 872/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4465\n",
      "Epoch 00872: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4465 - val_loss: 1.4739\n",
      "Epoch 873/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00873: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4448 - val_loss: 1.4715\n",
      "Epoch 874/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4451\n",
      "Epoch 00874: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4451 - val_loss: 1.4757\n",
      "Epoch 875/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00875: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4450 - val_loss: 1.4728\n",
      "Epoch 876/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4441\n",
      "Epoch 00876: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4441 - val_loss: 1.4781\n",
      "Epoch 877/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4351\n",
      "Epoch 00877: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4452 - val_loss: 1.4755\n",
      "Epoch 878/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 00878: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4418 - val_loss: 1.4728\n",
      "Epoch 879/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4422\n",
      "Epoch 00879: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4422 - val_loss: 1.4731\n",
      "Epoch 880/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4416\n",
      "Epoch 00880: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4416 - val_loss: 1.4715\n",
      "Epoch 881/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4480\n",
      "Epoch 00881: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4423 - val_loss: 1.4747\n",
      "Epoch 882/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4422\n",
      "Epoch 00882: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4422 - val_loss: 1.4714\n",
      "Epoch 883/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00883: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4515 - val_loss: 1.4755\n",
      "Epoch 884/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4486\n",
      "Epoch 00884: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4486 - val_loss: 1.4770\n",
      "Epoch 885/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4433\n",
      "Epoch 00885: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4433 - val_loss: 1.4703\n",
      "Epoch 886/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00886: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4424 - val_loss: 1.4733\n",
      "Epoch 887/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4428\n",
      "Epoch 00887: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4428 - val_loss: 1.4721\n",
      "Epoch 888/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4436\n",
      "Epoch 00888: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4436 - val_loss: 1.4713\n",
      "Epoch 889/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 00889: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4414 - val_loss: 1.4747\n",
      "Epoch 890/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4406\n",
      "Epoch 00890: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4406 - val_loss: 1.4740\n",
      "Epoch 891/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4419\n",
      "Epoch 00891: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4419 - val_loss: 1.4713\n",
      "Epoch 892/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4419\n",
      "Epoch 00892: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4419 - val_loss: 1.4719\n",
      "Epoch 893/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 00893: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4427 - val_loss: 1.4740\n",
      "Epoch 894/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4403\n",
      "Epoch 00894: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4403 - val_loss: 1.4751\n",
      "Epoch 895/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00895: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4423 - val_loss: 1.4720\n",
      "Epoch 896/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 00896: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4421 - val_loss: 1.4766\n",
      "Epoch 897/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 00897: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4407 - val_loss: 1.4739\n",
      "Epoch 898/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4426\n",
      "Epoch 00898: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4426 - val_loss: 1.4758\n",
      "Epoch 899/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 00899: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4429 - val_loss: 1.4754\n",
      "Epoch 900/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4416\n",
      "Epoch 00900: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4416 - val_loss: 1.4798\n",
      "Epoch 901/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4434\n",
      "Epoch 00901: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4434 - val_loss: 1.4714\n",
      "Epoch 902/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4509\n",
      "Epoch 00902: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4447 - val_loss: 1.4772\n",
      "Epoch 903/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4428\n",
      "Epoch 00903: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4428 - val_loss: 1.4752\n",
      "Epoch 904/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4422\n",
      "Epoch 00904: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4422 - val_loss: 1.4720\n",
      "Epoch 905/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4361\n",
      "Epoch 00905: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4433 - val_loss: 1.4758\n",
      "Epoch 906/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4431\n",
      "Epoch 00906: loss did not improve from 1.44028\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4431 - val_loss: 1.4737\n",
      "Epoch 907/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4274\n",
      "Epoch 00907: loss improved from 1.44028 to 1.43941, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4394 - val_loss: 1.4730\n",
      "Epoch 908/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4436\n",
      "Epoch 00908: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4436 - val_loss: 1.4737\n",
      "Epoch 909/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00909: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4424 - val_loss: 1.4741\n",
      "Epoch 910/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4526\n",
      "Epoch 00910: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4526 - val_loss: 1.4747\n",
      "Epoch 911/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00911: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4464 - val_loss: 1.4768\n",
      "Epoch 912/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4396\n",
      "Epoch 00912: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4434 - val_loss: 1.4728\n",
      "Epoch 913/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00913: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4482 - val_loss: 1.4773\n",
      "Epoch 914/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4453\n",
      "Epoch 00914: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4453 - val_loss: 1.4746\n",
      "Epoch 915/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00915: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4424 - val_loss: 1.4713\n",
      "Epoch 916/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4352\n",
      "Epoch 00916: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4431 - val_loss: 1.4731\n",
      "Epoch 917/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 00917: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4407 - val_loss: 1.4747\n",
      "Epoch 918/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 00918: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4411 - val_loss: 1.4731\n",
      "Epoch 919/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4410\n",
      "Epoch 00919: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4410 - val_loss: 1.4787\n",
      "Epoch 920/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4431\n",
      "Epoch 00920: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4431 - val_loss: 1.4768\n",
      "Epoch 921/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4425\n",
      "Epoch 00921: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4425 - val_loss: 1.4728\n",
      "Epoch 922/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4508\n",
      "Epoch 00922: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4508 - val_loss: 1.4779\n",
      "Epoch 923/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4465\n",
      "Epoch 00923: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4465 - val_loss: 1.4758\n",
      "Epoch 924/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00924: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4450 - val_loss: 1.4756\n",
      "Epoch 925/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4433\n",
      "Epoch 00925: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4433 - val_loss: 1.4742\n",
      "Epoch 926/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4422\n",
      "Epoch 00926: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4422 - val_loss: 1.4732\n",
      "Epoch 927/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00927: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4423 - val_loss: 1.4750\n",
      "Epoch 928/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4403\n",
      "Epoch 00928: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4403 - val_loss: 1.4739\n",
      "Epoch 929/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4397\n",
      "Epoch 00929: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4397 - val_loss: 1.4706\n",
      "Epoch 930/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 00930: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4414 - val_loss: 1.4756\n",
      "Epoch 931/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4434\n",
      "Epoch 00931: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4434 - val_loss: 1.4789\n",
      "Epoch 932/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4431\n",
      "Epoch 00932: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4431 - val_loss: 1.4754\n",
      "Epoch 933/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4410\n",
      "Epoch 00933: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4410 - val_loss: 1.4710\n",
      "Epoch 934/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4415\n",
      "Epoch 00934: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4415 - val_loss: 1.4757\n",
      "Epoch 935/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4413\n",
      "Epoch 00935: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4413 - val_loss: 1.4734\n",
      "Epoch 936/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00936: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4423 - val_loss: 1.4712\n",
      "Epoch 937/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4496\n",
      "Epoch 00937: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4496 - val_loss: 1.4781\n",
      "Epoch 938/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4474\n",
      "Epoch 00938: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4474 - val_loss: 1.4788\n",
      "Epoch 939/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4491\n",
      "Epoch 00939: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4491 - val_loss: 1.4729\n",
      "Epoch 940/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00940: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4424 - val_loss: 1.4766\n",
      "Epoch 941/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 00941: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4420 - val_loss: 1.4740\n",
      "Epoch 942/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4412\n",
      "Epoch 00942: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4412 - val_loss: 1.4732\n",
      "Epoch 943/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 00943: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4421 - val_loss: 1.4773\n",
      "Epoch 944/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00944: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4424 - val_loss: 1.4781\n",
      "Epoch 945/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4412\n",
      "Epoch 00945: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4412 - val_loss: 1.4739\n",
      "Epoch 946/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4398\n",
      "Epoch 00946: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4398 - val_loss: 1.4703\n",
      "Epoch 947/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 00947: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4399 - val_loss: 1.4750\n",
      "Epoch 948/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 00948: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4427 - val_loss: 1.4724\n",
      "Epoch 949/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 00949: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4404 - val_loss: 1.4798\n",
      "Epoch 950/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4413\n",
      "Epoch 00950: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4413 - val_loss: 1.4756\n",
      "Epoch 951/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 00951: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4418 - val_loss: 1.4761\n",
      "Epoch 952/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 00952: loss did not improve from 1.43941\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4421 - val_loss: 1.4730\n",
      "Epoch 953/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4394\n",
      "Epoch 00953: loss improved from 1.43941 to 1.43940, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4394 - val_loss: 1.4766\n",
      "Epoch 954/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00954: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4430 - val_loss: 1.4743\n",
      "Epoch 955/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4409\n",
      "Epoch 00955: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4409 - val_loss: 1.4781\n",
      "Epoch 956/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00956: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4417 - val_loss: 1.4730\n",
      "Epoch 957/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4508\n",
      "Epoch 00957: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4508 - val_loss: 1.4730\n",
      "Epoch 958/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00958: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4492 - val_loss: 1.4733\n",
      "Epoch 959/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 00959: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4437 - val_loss: 1.4784\n",
      "Epoch 960/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4433\n",
      "Epoch 00960: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4433 - val_loss: 1.4773\n",
      "Epoch 961/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4426\n",
      "Epoch 00961: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4426 - val_loss: 1.4739\n",
      "Epoch 962/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4409\n",
      "Epoch 00962: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4409 - val_loss: 1.4722\n",
      "Epoch 963/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 00963: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4411 - val_loss: 1.4750\n",
      "Epoch 964/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4406\n",
      "Epoch 00964: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4406 - val_loss: 1.4710\n",
      "Epoch 965/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4410\n",
      "Epoch 00965: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4410 - val_loss: 1.4720\n",
      "Epoch 966/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4419\n",
      "Epoch 00966: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4419 - val_loss: 1.4741\n",
      "Epoch 967/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 00967: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4400 - val_loss: 1.4747\n",
      "Epoch 968/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 00968: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4414 - val_loss: 1.4810\n",
      "Epoch 969/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4422\n",
      "Epoch 00969: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4422 - val_loss: 1.4770\n",
      "Epoch 970/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00970: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4442 - val_loss: 1.4740\n",
      "Epoch 971/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00971: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4417 - val_loss: 1.4742\n",
      "Epoch 972/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4419\n",
      "Epoch 00972: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4419 - val_loss: 1.4753\n",
      "Epoch 973/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4401\n",
      "Epoch 00973: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4401 - val_loss: 1.4775\n",
      "Epoch 974/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00974: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4423 - val_loss: 1.4715\n",
      "Epoch 975/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4504\n",
      "Epoch 00975: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4504 - val_loss: 1.4791\n",
      "Epoch 976/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4474\n",
      "Epoch 00976: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4474 - val_loss: 1.4804\n",
      "Epoch 977/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4451\n",
      "Epoch 00977: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4451 - val_loss: 1.4783\n",
      "Epoch 978/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4397\n",
      "Epoch 00978: loss did not improve from 1.43940\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4411 - val_loss: 1.4739\n",
      "Epoch 979/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 00979: loss improved from 1.43940 to 1.43901, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4390 - val_loss: 1.4827\n",
      "Epoch 980/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4422\n",
      "Epoch 00980: loss did not improve from 1.43901\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4422 - val_loss: 1.4822\n",
      "Epoch 981/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4462\n",
      "Epoch 00981: loss did not improve from 1.43901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4462 - val_loss: 1.4736\n",
      "Epoch 982/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4405\n",
      "Epoch 00982: loss did not improve from 1.43901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4405 - val_loss: 1.4762\n",
      "Epoch 983/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4416\n",
      "Epoch 00983: loss did not improve from 1.43901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4416 - val_loss: 1.4715\n",
      "Epoch 984/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4412\n",
      "Epoch 00984: loss did not improve from 1.43901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4412 - val_loss: 1.4752\n",
      "Epoch 985/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4406\n",
      "Epoch 00985: loss did not improve from 1.43901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4406 - val_loss: 1.4785\n",
      "Epoch 986/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00986: loss did not improve from 1.43901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4450 - val_loss: 1.4750\n",
      "Epoch 987/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00987: loss did not improve from 1.43901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4430 - val_loss: 1.4784\n",
      "Epoch 988/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4422\n",
      "Epoch 00988: loss did not improve from 1.43901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4422 - val_loss: 1.4808\n",
      "Epoch 989/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4602\n",
      "Epoch 00989: loss did not improve from 1.43901\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4420 - val_loss: 1.4765\n",
      "Epoch 990/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4389\n",
      "Epoch 00990: loss improved from 1.43901 to 1.43887, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4389 - val_loss: 1.4704\n",
      "Epoch 991/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 00991: loss improved from 1.43887 to 1.43871, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4387 - val_loss: 1.4737\n",
      "Epoch 992/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 00992: loss did not improve from 1.43871\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4393 - val_loss: 1.4766\n",
      "Epoch 993/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4486\n",
      "Epoch 00993: loss did not improve from 1.43871\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4486 - val_loss: 1.4785\n",
      "Epoch 994/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00994: loss did not improve from 1.43871\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4450 - val_loss: 1.4755\n",
      "Epoch 995/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00995: loss did not improve from 1.43871\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4423 - val_loss: 1.4732\n",
      "Epoch 996/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 00996: loss did not improve from 1.43871\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4404 - val_loss: 1.4753\n",
      "Epoch 997/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00997: loss did not improve from 1.43871\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4423 - val_loss: 1.4735\n",
      "Epoch 998/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4432\n",
      "Epoch 00998: loss did not improve from 1.43871\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4432 - val_loss: 1.4750\n",
      "Epoch 999/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4403\n",
      "Epoch 00999: loss did not improve from 1.43871\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4403 - val_loss: 1.4729\n",
      "Epoch 1000/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 01000: loss improved from 1.43871 to 1.43852, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4385 - val_loss: 1.4744\n",
      "Epoch 1001/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4396\n",
      "Epoch 01001: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4396 - val_loss: 1.4751\n",
      "Epoch 1002/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 01002: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4418 - val_loss: 1.4816\n",
      "Epoch 1003/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 01003: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4427 - val_loss: 1.4782\n",
      "Epoch 1004/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4408\n",
      "Epoch 01004: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4408 - val_loss: 1.4760\n",
      "Epoch 1005/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4413\n",
      "Epoch 01005: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4413 - val_loss: 1.4742\n",
      "Epoch 1006/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4403\n",
      "Epoch 01006: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4403 - val_loss: 1.4747\n",
      "Epoch 1007/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 01007: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4418 - val_loss: 1.4747\n",
      "Epoch 1008/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 01008: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4420 - val_loss: 1.4746\n",
      "Epoch 1009/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4401\n",
      "Epoch 01009: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.4421 - val_loss: 1.4738\n",
      "Epoch 1010/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 01010: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4424 - val_loss: 1.4767\n",
      "Epoch 1011/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 01011: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4399 - val_loss: 1.4758\n",
      "Epoch 1012/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01012: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4388 - val_loss: 1.4724\n",
      "Epoch 1013/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01013: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4388 - val_loss: 1.4722\n",
      "Epoch 1014/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01014: loss did not improve from 1.43852\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4390 - val_loss: 1.4736\n",
      "Epoch 1015/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4376\n",
      "Epoch 01015: loss improved from 1.43852 to 1.43757, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4376 - val_loss: 1.4761\n",
      "Epoch 1016/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01016: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4390 - val_loss: 1.4720\n",
      "Epoch 1017/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01017: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4392 - val_loss: 1.4766\n",
      "Epoch 1018/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 01018: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4395 - val_loss: 1.4715\n",
      "Epoch 1019/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4391\n",
      "Epoch 01019: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4391 - val_loss: 1.4739\n",
      "Epoch 1020/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 01020: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4418 - val_loss: 1.4729\n",
      "Epoch 1021/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 01021: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4400 - val_loss: 1.4770\n",
      "Epoch 1022/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 01022: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4411 - val_loss: 1.4739\n",
      "Epoch 1023/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4394\n",
      "Epoch 01023: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4394 - val_loss: 1.4797\n",
      "Epoch 1024/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 01024: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4399 - val_loss: 1.4762\n",
      "Epoch 1025/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4398\n",
      "Epoch 01025: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4398 - val_loss: 1.4805\n",
      "Epoch 1026/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 01026: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4429 - val_loss: 1.4771\n",
      "Epoch 1027/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 01027: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4444 - val_loss: 1.4730\n",
      "Epoch 1028/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 01028: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4429 - val_loss: 1.4757\n",
      "Epoch 1029/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01029: loss did not improve from 1.43757\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4390 - val_loss: 1.4731\n",
      "Epoch 1030/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01030: loss improved from 1.43757 to 1.43696, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4370 - val_loss: 1.4737\n",
      "Epoch 1031/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01031: loss improved from 1.43696 to 1.43645, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4365 - val_loss: 1.4721\n",
      "Epoch 1032/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01032: loss improved from 1.43645 to 1.43644, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4364 - val_loss: 1.4736\n",
      "Epoch 1033/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01033: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4365 - val_loss: 1.4753\n",
      "Epoch 1034/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 01034: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4395 - val_loss: 1.4749\n",
      "Epoch 1035/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4415\n",
      "Epoch 01035: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4415 - val_loss: 1.4739\n",
      "Epoch 1036/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01036: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4392 - val_loss: 1.4770\n",
      "Epoch 1037/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4451\n",
      "Epoch 01037: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4451 - val_loss: 1.4739\n",
      "Epoch 1038/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 01038: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4414 - val_loss: 1.4796\n",
      "Epoch 1039/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4413\n",
      "Epoch 01039: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4413 - val_loss: 1.4716\n",
      "Epoch 1040/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 01040: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4400 - val_loss: 1.4784\n",
      "Epoch 1041/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 01041: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4395 - val_loss: 1.4751\n",
      "Epoch 1042/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 01042: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4395 - val_loss: 1.4750\n",
      "Epoch 1043/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4391\n",
      "Epoch 01043: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4391 - val_loss: 1.4753\n",
      "Epoch 1044/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4405\n",
      "Epoch 01044: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4405 - val_loss: 1.4771\n",
      "Epoch 1045/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 01045: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4421 - val_loss: 1.4765\n",
      "Epoch 1046/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01046: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4390 - val_loss: 1.4727\n",
      "Epoch 1047/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01047: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4387 - val_loss: 1.4784\n",
      "Epoch 1048/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 01048: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4430 - val_loss: 1.4825\n",
      "Epoch 1049/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4425\n",
      "Epoch 01049: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4425 - val_loss: 1.4784\n",
      "Epoch 1050/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01050: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4733\n",
      "Epoch 1051/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01051: loss did not improve from 1.43644\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4367 - val_loss: 1.4742\n",
      "Epoch 1052/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01052: loss improved from 1.43644 to 1.43638, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4364 - val_loss: 1.4783\n",
      "Epoch 1053/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01053: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4370 - val_loss: 1.4757\n",
      "Epoch 1054/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 01054: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4429 - val_loss: 1.4762\n",
      "Epoch 1055/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4451\n",
      "Epoch 01055: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4404 - val_loss: 1.4768\n",
      "Epoch 1056/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4402\n",
      "Epoch 01056: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4402 - val_loss: 1.4749\n",
      "Epoch 1057/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01057: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4387 - val_loss: 1.4730\n",
      "Epoch 1058/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4375\n",
      "Epoch 01058: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4375 - val_loss: 1.4778\n",
      "Epoch 1059/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01059: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4392 - val_loss: 1.4748\n",
      "Epoch 1060/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4391\n",
      "Epoch 01060: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4391 - val_loss: 1.4736\n",
      "Epoch 1061/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4396\n",
      "Epoch 01061: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4396 - val_loss: 1.4764\n",
      "Epoch 1062/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01062: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4382 - val_loss: 1.4760\n",
      "Epoch 1063/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4394\n",
      "Epoch 01063: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4372 - val_loss: 1.4751\n",
      "Epoch 1064/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01064: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4371 - val_loss: 1.4747\n",
      "Epoch 1065/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4381\n",
      "Epoch 01065: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4381 - val_loss: 1.4766\n",
      "Epoch 1066/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 01066: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4417 - val_loss: 1.4827\n",
      "Epoch 1067/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 01067: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4414 - val_loss: 1.4791\n",
      "Epoch 1068/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 01068: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4399 - val_loss: 1.4761\n",
      "Epoch 1069/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4405\n",
      "Epoch 01069: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4405 - val_loss: 1.4738\n",
      "Epoch 1070/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 01070: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4399 - val_loss: 1.4781\n",
      "Epoch 1071/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 01071: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4393 - val_loss: 1.4778\n",
      "Epoch 1072/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01072: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4392 - val_loss: 1.4757\n",
      "Epoch 1073/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4379\n",
      "Epoch 01073: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4379 - val_loss: 1.4754\n",
      "Epoch 1074/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4386\n",
      "Epoch 01074: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4386 - val_loss: 1.4791\n",
      "Epoch 1075/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 01075: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4395 - val_loss: 1.4800\n",
      "Epoch 1076/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 01076: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4418 - val_loss: 1.4809\n",
      "Epoch 1077/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 01077: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4444 - val_loss: 1.4797\n",
      "Epoch 1078/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 01078: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4420 - val_loss: 1.4744\n",
      "Epoch 1079/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01079: loss did not improve from 1.43638\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4390 - val_loss: 1.4718\n",
      "Epoch 1080/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01080: loss improved from 1.43638 to 1.43570, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4357 - val_loss: 1.4772\n",
      "Epoch 1081/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4398\n",
      "Epoch 01081: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4398 - val_loss: 1.4780\n",
      "Epoch 1082/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01082: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4417 - val_loss: 1.4739\n",
      "Epoch 1083/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4403\n",
      "Epoch 01083: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4403 - val_loss: 1.4740\n",
      "Epoch 1084/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 01084: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4420 - val_loss: 1.4794\n",
      "Epoch 1085/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4399\n",
      "Epoch 01085: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4389 - val_loss: 1.4749\n",
      "Epoch 1086/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01086: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4382 - val_loss: 1.4749\n",
      "Epoch 1087/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01087: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4367 - val_loss: 1.4743\n",
      "Epoch 1088/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4362\n",
      "Epoch 01088: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4362 - val_loss: 1.4783\n",
      "Epoch 1089/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 01089: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4429 - val_loss: 1.4831\n",
      "Epoch 1090/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4405\n",
      "Epoch 01090: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4405 - val_loss: 1.4787\n",
      "Epoch 1091/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4402\n",
      "Epoch 01091: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4402 - val_loss: 1.4762\n",
      "Epoch 1092/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4403\n",
      "Epoch 01092: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4403 - val_loss: 1.4764\n",
      "Epoch 1093/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01093: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4377 - val_loss: 1.4760\n",
      "Epoch 1094/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4379\n",
      "Epoch 01094: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4379 - val_loss: 1.4798\n",
      "Epoch 1095/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 01095: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4393 - val_loss: 1.4774\n",
      "Epoch 1096/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4403\n",
      "Epoch 01096: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4403 - val_loss: 1.4729\n",
      "Epoch 1097/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4415\n",
      "Epoch 01097: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4415 - val_loss: 1.4793\n",
      "Epoch 1098/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4381\n",
      "Epoch 01098: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4381 - val_loss: 1.4735\n",
      "Epoch 1099/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4373\n",
      "Epoch 01099: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4373 - val_loss: 1.4750\n",
      "Epoch 1100/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4396\n",
      "Epoch 01100: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4396 - val_loss: 1.4732\n",
      "Epoch 1101/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4375\n",
      "Epoch 01101: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4375 - val_loss: 1.4724\n",
      "Epoch 1102/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01102: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4371 - val_loss: 1.4773\n",
      "Epoch 1103/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4457\n",
      "Epoch 01103: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4368 - val_loss: 1.4827\n",
      "Epoch 1104/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 01104: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4411 - val_loss: 1.4771\n",
      "Epoch 1105/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01105: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4380 - val_loss: 1.4782\n",
      "Epoch 1106/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4360\n",
      "Epoch 01106: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4400 - val_loss: 1.4822\n",
      "Epoch 1107/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4395\n",
      "Epoch 01107: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4440 - val_loss: 1.4744\n",
      "Epoch 1108/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 01108: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4407 - val_loss: 1.4762\n",
      "Epoch 1109/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01109: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4418 - val_loss: 1.4785\n",
      "Epoch 1110/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 01110: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4400 - val_loss: 1.4761\n",
      "Epoch 1111/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4178\n",
      "Epoch 01111: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4384 - val_loss: 1.4771\n",
      "Epoch 1112/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4400\n",
      "Epoch 01112: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4381 - val_loss: 1.4778\n",
      "Epoch 1113/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01113: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4378 - val_loss: 1.4793\n",
      "Epoch 1114/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 01114: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4393 - val_loss: 1.4742\n",
      "Epoch 1115/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4389\n",
      "Epoch 01115: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4389 - val_loss: 1.4759\n",
      "Epoch 1116/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 01116: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4414 - val_loss: 1.4796\n",
      "Epoch 1117/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4421\n",
      "Epoch 01117: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4393 - val_loss: 1.4740\n",
      "Epoch 1118/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4557\n",
      "Epoch 01118: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4373 - val_loss: 1.4748\n",
      "Epoch 1119/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4378\n",
      "Epoch 01119: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4378 - val_loss: 1.4788\n",
      "Epoch 1120/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 01120: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4400 - val_loss: 1.4793\n",
      "Epoch 1121/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4383\n",
      "Epoch 01121: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4383 - val_loss: 1.4730\n",
      "Epoch 1122/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01122: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4363 - val_loss: 1.4800\n",
      "Epoch 1123/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4416\n",
      "Epoch 01123: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4416 - val_loss: 1.4749\n",
      "Epoch 1124/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 01124: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4414 - val_loss: 1.4730\n",
      "Epoch 1125/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 01125: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4404 - val_loss: 1.4744\n",
      "Epoch 1126/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01126: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4387 - val_loss: 1.4782\n",
      "Epoch 1127/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 01127: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4393 - val_loss: 1.4753\n",
      "Epoch 1128/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4386\n",
      "Epoch 01128: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4386 - val_loss: 1.4757\n",
      "Epoch 1129/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4445\n",
      "Epoch 01129: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4445 - val_loss: 1.4807\n",
      "Epoch 1130/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 01130: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4439 - val_loss: 1.4808\n",
      "Epoch 1131/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01131: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4405 - val_loss: 1.4808\n",
      "Epoch 1132/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4410\n",
      "Epoch 01132: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4410 - val_loss: 1.4775\n",
      "Epoch 1133/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01133: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4377 - val_loss: 1.4791\n",
      "Epoch 1134/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 01134: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4407 - val_loss: 1.4808\n",
      "Epoch 1135/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01135: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4389 - val_loss: 1.4753\n",
      "Epoch 1136/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4280\n",
      "Epoch 01136: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.4397 - val_loss: 1.4760\n",
      "Epoch 1137/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01137: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4384 - val_loss: 1.4786\n",
      "Epoch 1138/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4455\n",
      "Epoch 01138: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4455 - val_loss: 1.4797\n",
      "Epoch 1139/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01139: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4392 - val_loss: 1.4753\n",
      "Epoch 1140/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01140: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4377 - val_loss: 1.4758\n",
      "Epoch 1141/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 01141: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4404 - val_loss: 1.4769\n",
      "Epoch 1142/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4396\n",
      "Epoch 01142: loss did not improve from 1.43570\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4396 - val_loss: 1.4741\n",
      "Epoch 1143/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01143: loss improved from 1.43570 to 1.43553, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4355 - val_loss: 1.4786\n",
      "Epoch 1144/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01144: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4357 - val_loss: 1.4797\n",
      "Epoch 1145/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4394\n",
      "Epoch 01145: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4394 - val_loss: 1.4757\n",
      "Epoch 1146/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4381\n",
      "Epoch 01146: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4381 - val_loss: 1.4784\n",
      "Epoch 1147/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4379\n",
      "Epoch 01147: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4379 - val_loss: 1.4800\n",
      "Epoch 1148/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4383\n",
      "Epoch 01148: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4383 - val_loss: 1.4768\n",
      "Epoch 1149/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4380\n",
      "Epoch 01149: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4380 - val_loss: 1.4767\n",
      "Epoch 1150/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4375\n",
      "Epoch 01150: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4375 - val_loss: 1.4806\n",
      "Epoch 1151/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 01151: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4400 - val_loss: 1.4784\n",
      "Epoch 1152/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 01152: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4374 - val_loss: 1.4770\n",
      "Epoch 1153/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4379\n",
      "Epoch 01153: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4379 - val_loss: 1.4735\n",
      "Epoch 1154/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4384\n",
      "Epoch 01154: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4384 - val_loss: 1.4770\n",
      "Epoch 1155/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4380\n",
      "Epoch 01155: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4380 - val_loss: 1.4742\n",
      "Epoch 1156/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01156: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4359 - val_loss: 1.4743\n",
      "Epoch 1157/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01157: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4355 - val_loss: 1.4740\n",
      "Epoch 1158/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01158: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4365 - val_loss: 1.4780\n",
      "Epoch 1159/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 01159: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4421 - val_loss: 1.4713\n",
      "Epoch 1160/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4384\n",
      "Epoch 01160: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4384 - val_loss: 1.4750\n",
      "Epoch 1161/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01161: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4371 - val_loss: 1.4749\n",
      "Epoch 1162/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4313\n",
      "Epoch 01162: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4368 - val_loss: 1.4803\n",
      "Epoch 1163/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4373\n",
      "Epoch 01163: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4373 - val_loss: 1.4743\n",
      "Epoch 1164/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4222\n",
      "Epoch 01164: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4357 - val_loss: 1.4760\n",
      "Epoch 1165/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01165: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4356 - val_loss: 1.4743\n",
      "Epoch 1166/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01166: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4375 - val_loss: 1.4743\n",
      "Epoch 1167/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01167: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4364 - val_loss: 1.4759\n",
      "Epoch 1168/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01168: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4359 - val_loss: 1.4743\n",
      "Epoch 1169/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 01169: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4374 - val_loss: 1.4789\n",
      "Epoch 1170/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4376\n",
      "Epoch 01170: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4395 - val_loss: 1.4800\n",
      "Epoch 1171/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01171: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4387 - val_loss: 1.4763\n",
      "Epoch 1172/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01172: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4361 - val_loss: 1.4781\n",
      "Epoch 1173/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4366\n",
      "Epoch 01173: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4366 - val_loss: 1.4794\n",
      "Epoch 1174/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01174: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4382 - val_loss: 1.4781\n",
      "Epoch 1175/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4519\n",
      "Epoch 01175: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4379 - val_loss: 1.4788\n",
      "Epoch 1176/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4398\n",
      "Epoch 01176: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4398 - val_loss: 1.4758\n",
      "Epoch 1177/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4389\n",
      "Epoch 01177: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4389 - val_loss: 1.4824\n",
      "Epoch 1178/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4378\n",
      "Epoch 01178: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4378 - val_loss: 1.4801\n",
      "Epoch 1179/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4391\n",
      "Epoch 01179: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4391 - val_loss: 1.4745\n",
      "Epoch 1180/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4381\n",
      "Epoch 01180: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4381 - val_loss: 1.4738\n",
      "Epoch 1181/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 01181: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4385 - val_loss: 1.4799\n",
      "Epoch 1182/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01182: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4369 - val_loss: 1.4773\n",
      "Epoch 1183/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01183: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4361 - val_loss: 1.4820\n",
      "Epoch 1184/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4461\n",
      "Epoch 01184: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4427 - val_loss: 1.4774\n",
      "Epoch 1185/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 01185: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4385 - val_loss: 1.4765\n",
      "Epoch 1186/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01186: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4358 - val_loss: 1.4778\n",
      "Epoch 1187/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01187: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4370 - val_loss: 1.4766\n",
      "Epoch 1188/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01188: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4360 - val_loss: 1.4762\n",
      "Epoch 1189/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01189: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4357 - val_loss: 1.4761\n",
      "Epoch 1190/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01190: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4361 - val_loss: 1.4816\n",
      "Epoch 1191/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 01191: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4399 - val_loss: 1.4739\n",
      "Epoch 1192/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4397\n",
      "Epoch 01192: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4397 - val_loss: 1.4773\n",
      "Epoch 1193/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4437\n",
      "Epoch 01193: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4397 - val_loss: 1.4816\n",
      "Epoch 1194/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 01194: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4399 - val_loss: 1.4815\n",
      "Epoch 1195/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 01195: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4385 - val_loss: 1.4790\n",
      "Epoch 1196/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01196: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4387 - val_loss: 1.4762\n",
      "Epoch 1197/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01197: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4363 - val_loss: 1.4792\n",
      "Epoch 1198/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01198: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4372 - val_loss: 1.4789\n",
      "Epoch 1199/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4369\n",
      "Epoch 01199: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4369 - val_loss: 1.4725\n",
      "Epoch 1200/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01200: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4365 - val_loss: 1.4763\n",
      "Epoch 1201/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4366\n",
      "Epoch 01201: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4366 - val_loss: 1.4796\n",
      "Epoch 1202/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 01202: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4385 - val_loss: 1.4792\n",
      "Epoch 1203/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 01203: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4393 - val_loss: 1.4739\n",
      "Epoch 1204/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4379\n",
      "Epoch 01204: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4379 - val_loss: 1.4771\n",
      "Epoch 1205/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 01205: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4374 - val_loss: 1.4774\n",
      "Epoch 1206/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01206: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4390 - val_loss: 1.4771\n",
      "Epoch 1207/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01207: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4363 - val_loss: 1.4784\n",
      "Epoch 1208/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4486\n",
      "Epoch 01208: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.4387 - val_loss: 1.4763\n",
      "Epoch 1209/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4451\n",
      "Epoch 01209: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4372 - val_loss: 1.4770\n",
      "Epoch 1210/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01210: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4372 - val_loss: 1.4758\n",
      "Epoch 1211/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01211: loss did not improve from 1.43553\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4365 - val_loss: 1.4770\n",
      "Epoch 1212/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01212: loss improved from 1.43553 to 1.43434, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4343 - val_loss: 1.4739\n",
      "Epoch 1213/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01213: loss improved from 1.43434 to 1.43406, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4341 - val_loss: 1.4743\n",
      "Epoch 1214/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01214: loss did not improve from 1.43406\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4382 - val_loss: 1.4748\n",
      "Epoch 1215/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4465\n",
      "Epoch 01215: loss did not improve from 1.43406\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.4407 - val_loss: 1.4858\n",
      "Epoch 1216/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4366\n",
      "Epoch 01216: loss did not improve from 1.43406\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4395 - val_loss: 1.4838\n",
      "Epoch 1217/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4424\n",
      "Epoch 01217: loss did not improve from 1.43406\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4373 - val_loss: 1.4741\n",
      "Epoch 1218/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01218: loss did not improve from 1.43406\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4358 - val_loss: 1.4794\n",
      "Epoch 1219/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01219: loss did not improve from 1.43406\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4358 - val_loss: 1.4738\n",
      "Epoch 1220/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 01220: loss improved from 1.43406 to 1.43367, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4337 - val_loss: 1.4779\n",
      "Epoch 1221/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4394\n",
      "Epoch 01221: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4394 - val_loss: 1.4746\n",
      "Epoch 1222/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 01222: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4395 - val_loss: 1.4792\n",
      "Epoch 1223/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01223: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4356 - val_loss: 1.4803\n",
      "Epoch 1224/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01224: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4377 - val_loss: 1.4791\n",
      "Epoch 1225/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4380\n",
      "Epoch 01225: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4380 - val_loss: 1.4752\n",
      "Epoch 1226/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4369\n",
      "Epoch 01226: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4369 - val_loss: 1.4798\n",
      "Epoch 1227/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4381\n",
      "Epoch 01227: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4381 - val_loss: 1.4759\n",
      "Epoch 1228/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4354\n",
      "Epoch 01228: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4354 - val_loss: 1.4777\n",
      "Epoch 1229/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01229: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4355 - val_loss: 1.4786\n",
      "Epoch 1230/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01230: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4353 - val_loss: 1.4763\n",
      "Epoch 1231/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4360\n",
      "Epoch 01231: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4360 - val_loss: 1.4771\n",
      "Epoch 1232/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4376\n",
      "Epoch 01232: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4376 - val_loss: 1.4745\n",
      "Epoch 1233/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4376\n",
      "Epoch 01233: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4376 - val_loss: 1.4771\n",
      "Epoch 1234/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01234: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4367 - val_loss: 1.4779\n",
      "Epoch 1235/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01235: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4364 - val_loss: 1.4790\n",
      "Epoch 1236/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01236: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4361 - val_loss: 1.4763\n",
      "Epoch 1237/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4354\n",
      "Epoch 01237: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4354 - val_loss: 1.4779\n",
      "Epoch 1238/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01238: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4388 - val_loss: 1.4835\n",
      "Epoch 1239/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 01239: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4399 - val_loss: 1.4810\n",
      "Epoch 1240/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01240: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4387 - val_loss: 1.4878\n",
      "Epoch 1241/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 01241: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4417 - val_loss: 1.4748\n",
      "Epoch 1242/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01242: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4366 - val_loss: 1.4814\n",
      "Epoch 1243/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4386\n",
      "Epoch 01243: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4386 - val_loss: 1.4832\n",
      "Epoch 1244/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01244: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4372 - val_loss: 1.4786\n",
      "Epoch 1245/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01245: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4355 - val_loss: 1.4759\n",
      "Epoch 1246/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4340\n",
      "Epoch 01246: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4340 - val_loss: 1.4738\n",
      "Epoch 1247/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01247: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4351 - val_loss: 1.4778\n",
      "Epoch 1248/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4391\n",
      "Epoch 01248: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4391 - val_loss: 1.4766\n",
      "Epoch 1249/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01249: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4363 - val_loss: 1.4760\n",
      "Epoch 1250/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01250: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4350 - val_loss: 1.4729\n",
      "Epoch 1251/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01251: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4358 - val_loss: 1.4756\n",
      "Epoch 1252/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01252: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4363 - val_loss: 1.4776\n",
      "Epoch 1253/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01253: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4359 - val_loss: 1.4791\n",
      "Epoch 1254/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4415\n",
      "Epoch 01254: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4353 - val_loss: 1.4783\n",
      "Epoch 1255/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01255: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4346 - val_loss: 1.4752\n",
      "Epoch 1256/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4348\n",
      "Epoch 01256: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4348 - val_loss: 1.4769\n",
      "Epoch 1257/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01257: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4358 - val_loss: 1.4784\n",
      "Epoch 1258/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01258: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4363 - val_loss: 1.4823\n",
      "Epoch 1259/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4386\n",
      "Epoch 01259: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4386 - val_loss: 1.4836\n",
      "Epoch 1260/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01260: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4361 - val_loss: 1.4773\n",
      "Epoch 1261/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01261: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4348 - val_loss: 1.4767\n",
      "Epoch 1262/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01262: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4344 - val_loss: 1.4765\n",
      "Epoch 1263/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4389\n",
      "Epoch 01263: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4368 - val_loss: 1.4789\n",
      "Epoch 1264/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4348\n",
      "Epoch 01264: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4348 - val_loss: 1.4759\n",
      "Epoch 1265/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01265: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4361 - val_loss: 1.4772\n",
      "Epoch 1266/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01266: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4353 - val_loss: 1.4759\n",
      "Epoch 1267/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4321\n",
      "Epoch 01267: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4365 - val_loss: 1.4792\n",
      "Epoch 1268/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01268: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4370 - val_loss: 1.4756\n",
      "Epoch 1269/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01269: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4353 - val_loss: 1.4796\n",
      "Epoch 1270/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4362\n",
      "Epoch 01270: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4362 - val_loss: 1.4792\n",
      "Epoch 1271/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01271: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4351 - val_loss: 1.4773\n",
      "Epoch 1272/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01272: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4350 - val_loss: 1.4778\n",
      "Epoch 1273/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01273: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4346 - val_loss: 1.4766\n",
      "Epoch 1274/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01274: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4353 - val_loss: 1.4811\n",
      "Epoch 1275/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4391\n",
      "Epoch 01275: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4391 - val_loss: 1.4769\n",
      "Epoch 1276/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01276: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4358 - val_loss: 1.4793\n",
      "Epoch 1277/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01277: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4364 - val_loss: 1.4774\n",
      "Epoch 1278/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4352\n",
      "Epoch 01278: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4352 - val_loss: 1.4762\n",
      "Epoch 1279/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4340\n",
      "Epoch 01279: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4340 - val_loss: 1.4767\n",
      "Epoch 1280/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4360\n",
      "Epoch 01280: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4360 - val_loss: 1.4774\n",
      "Epoch 1281/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01281: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4355 - val_loss: 1.4757\n",
      "Epoch 1282/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01282: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4361 - val_loss: 1.4769\n",
      "Epoch 1283/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4366\n",
      "Epoch 01283: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4366 - val_loss: 1.4803\n",
      "Epoch 1284/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01284: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4388 - val_loss: 1.4799\n",
      "Epoch 1285/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01285: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4387 - val_loss: 1.4763\n",
      "Epoch 1286/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01286: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4355 - val_loss: 1.4811\n",
      "Epoch 1287/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4379\n",
      "Epoch 01287: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4379 - val_loss: 1.4792\n",
      "Epoch 1288/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4373\n",
      "Epoch 01288: loss did not improve from 1.43367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4373 - val_loss: 1.4773\n",
      "Epoch 1289/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01289: loss improved from 1.43367 to 1.43349, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4335 - val_loss: 1.4781\n",
      "Epoch 1290/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01290: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4359 - val_loss: 1.4847\n",
      "Epoch 1291/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01291: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4370 - val_loss: 1.4782\n",
      "Epoch 1292/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4384\n",
      "Epoch 01292: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4384 - val_loss: 1.4793\n",
      "Epoch 1293/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4397\n",
      "Epoch 01293: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4397 - val_loss: 1.4785\n",
      "Epoch 1294/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4402\n",
      "Epoch 01294: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4402 - val_loss: 1.4804\n",
      "Epoch 1295/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4384\n",
      "Epoch 01295: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4384 - val_loss: 1.4795\n",
      "Epoch 1296/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01296: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4382 - val_loss: 1.4809\n",
      "Epoch 1297/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01297: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4377 - val_loss: 1.4762\n",
      "Epoch 1298/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01298: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4350 - val_loss: 1.4777\n",
      "Epoch 1299/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 01299: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4347 - val_loss: 1.4790\n",
      "Epoch 1300/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4360\n",
      "Epoch 01300: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4360 - val_loss: 1.4774\n",
      "Epoch 1301/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4340\n",
      "Epoch 01301: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4340 - val_loss: 1.4784\n",
      "Epoch 1302/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4349\n",
      "Epoch 01302: loss did not improve from 1.43349\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4349 - val_loss: 1.4775\n",
      "Epoch 1303/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01303: loss improved from 1.43349 to 1.43335, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4334 - val_loss: 1.4769\n",
      "Epoch 1304/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01304: loss improved from 1.43335 to 1.43319, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4332 - val_loss: 1.4761\n",
      "Epoch 1305/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01305: loss improved from 1.43319 to 1.43279, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4328 - val_loss: 1.4757\n",
      "Epoch 1306/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4394\n",
      "Epoch 01306: loss did not improve from 1.43279\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4394 - val_loss: 1.4791\n",
      "Epoch 1307/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01307: loss did not improve from 1.43279\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4367 - val_loss: 1.4793\n",
      "Epoch 1308/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4340\n",
      "Epoch 01308: loss did not improve from 1.43279\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4340 - val_loss: 1.4764\n",
      "Epoch 1309/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01309: loss did not improve from 1.43279\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4334 - val_loss: 1.4756\n",
      "Epoch 1310/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01310: loss did not improve from 1.43279\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4357 - val_loss: 1.4768\n",
      "Epoch 1311/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4349\n",
      "Epoch 01311: loss did not improve from 1.43279\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4349 - val_loss: 1.4784\n",
      "Epoch 1312/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01312: loss did not improve from 1.43279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4330 - val_loss: 1.4757\n",
      "Epoch 1313/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01313: loss improved from 1.43279 to 1.43201, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4320 - val_loss: 1.4783\n",
      "Epoch 1314/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01314: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4350 - val_loss: 1.4756\n",
      "Epoch 1315/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01315: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4342 - val_loss: 1.4826\n",
      "Epoch 1316/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01316: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4355 - val_loss: 1.4782\n",
      "Epoch 1317/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01317: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4338 - val_loss: 1.4749\n",
      "Epoch 1318/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4348\n",
      "Epoch 01318: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4348 - val_loss: 1.4769\n",
      "Epoch 1319/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01319: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4334 - val_loss: 1.4762\n",
      "Epoch 1320/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01320: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4326 - val_loss: 1.4778\n",
      "Epoch 1321/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 01321: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4337 - val_loss: 1.4844\n",
      "Epoch 1322/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4373\n",
      "Epoch 01322: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4373 - val_loss: 1.4819\n",
      "Epoch 1323/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4360\n",
      "Epoch 01323: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4360 - val_loss: 1.4770\n",
      "Epoch 1324/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01324: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4330 - val_loss: 1.4758\n",
      "Epoch 1325/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01325: loss did not improve from 1.43201\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4323 - val_loss: 1.4748\n",
      "Epoch 1326/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01326: loss improved from 1.43201 to 1.43191, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4319 - val_loss: 1.4770\n",
      "Epoch 1327/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4333\n",
      "Epoch 01327: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4333 - val_loss: 1.4766\n",
      "Epoch 1328/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01328: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4344 - val_loss: 1.4774\n",
      "Epoch 1329/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01329: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4344 - val_loss: 1.4751\n",
      "Epoch 1330/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01330: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4346 - val_loss: 1.4786\n",
      "Epoch 1331/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01331: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4377 - val_loss: 1.4775\n",
      "Epoch 1332/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01332: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4359 - val_loss: 1.4784\n",
      "Epoch 1333/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4397\n",
      "Epoch 01333: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4397 - val_loss: 1.4812\n",
      "Epoch 1334/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4376\n",
      "Epoch 01334: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4376 - val_loss: 1.4753\n",
      "Epoch 1335/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01335: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4358 - val_loss: 1.4780\n",
      "Epoch 1336/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01336: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4343 - val_loss: 1.4831\n",
      "Epoch 1337/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01337: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4344 - val_loss: 1.4758\n",
      "Epoch 1338/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4321\n",
      "Epoch 01338: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4321 - val_loss: 1.4742\n",
      "Epoch 1339/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01339: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4338 - val_loss: 1.4769\n",
      "Epoch 1340/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4354\n",
      "Epoch 01340: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4354 - val_loss: 1.4842\n",
      "Epoch 1341/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01341: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4367 - val_loss: 1.4789\n",
      "Epoch 1342/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01342: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4356 - val_loss: 1.4773\n",
      "Epoch 1343/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01343: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4342 - val_loss: 1.4778\n",
      "Epoch 1344/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01344: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4338 - val_loss: 1.4781\n",
      "Epoch 1345/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01345: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4338 - val_loss: 1.4773\n",
      "Epoch 1346/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01346: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4344 - val_loss: 1.4824\n",
      "Epoch 1347/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01347: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4359 - val_loss: 1.4765\n",
      "Epoch 1348/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01348: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4331 - val_loss: 1.4783\n",
      "Epoch 1349/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01349: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4782\n",
      "Epoch 1350/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4339\n",
      "Epoch 01350: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4339 - val_loss: 1.4803\n",
      "Epoch 1351/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4360\n",
      "Epoch 01351: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4360 - val_loss: 1.4798\n",
      "Epoch 1352/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 01352: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4347 - val_loss: 1.4822\n",
      "Epoch 1353/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01353: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4833\n",
      "Epoch 1354/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4378\n",
      "Epoch 01354: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4378 - val_loss: 1.4817\n",
      "Epoch 1355/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01355: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4392 - val_loss: 1.4785\n",
      "Epoch 1356/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01356: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4371 - val_loss: 1.4759\n",
      "Epoch 1357/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01357: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4353 - val_loss: 1.4783\n",
      "Epoch 1358/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4352\n",
      "Epoch 01358: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4352 - val_loss: 1.4786\n",
      "Epoch 1359/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01359: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4342 - val_loss: 1.4793\n",
      "Epoch 1360/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4366\n",
      "Epoch 01360: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4366 - val_loss: 1.4799\n",
      "Epoch 1361/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01361: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4345 - val_loss: 1.4800\n",
      "Epoch 1362/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01362: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4359 - val_loss: 1.4877\n",
      "Epoch 1363/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01363: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4372 - val_loss: 1.4797\n",
      "Epoch 1364/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 01364: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4347 - val_loss: 1.4772\n",
      "Epoch 1365/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01365: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4355 - val_loss: 1.4782\n",
      "Epoch 1366/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 01366: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4337 - val_loss: 1.4820\n",
      "Epoch 1367/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 01367: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4374 - val_loss: 1.4815\n",
      "Epoch 1368/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01368: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4371 - val_loss: 1.4806\n",
      "Epoch 1369/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01369: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4370 - val_loss: 1.4772\n",
      "Epoch 1370/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01370: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4342 - val_loss: 1.4783\n",
      "Epoch 1371/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01371: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4342 - val_loss: 1.4760\n",
      "Epoch 1372/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01372: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4336 - val_loss: 1.4817\n",
      "Epoch 1373/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01373: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4353 - val_loss: 1.4761\n",
      "Epoch 1374/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4398\n",
      "Epoch 01374: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4398 - val_loss: 1.4794\n",
      "Epoch 1375/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4373\n",
      "Epoch 01375: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4373 - val_loss: 1.4788\n",
      "Epoch 1376/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01376: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4346 - val_loss: 1.4762\n",
      "Epoch 1377/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 01377: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4337 - val_loss: 1.4806\n",
      "Epoch 1378/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01378: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4342 - val_loss: 1.4749\n",
      "Epoch 1379/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01379: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4330 - val_loss: 1.4791\n",
      "Epoch 1380/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01380: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4338 - val_loss: 1.4768\n",
      "Epoch 1381/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4333\n",
      "Epoch 01381: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4333 - val_loss: 1.4792\n",
      "Epoch 1382/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01382: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4344 - val_loss: 1.4761\n",
      "Epoch 1383/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01383: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4770\n",
      "Epoch 1384/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01384: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4351 - val_loss: 1.4805\n",
      "Epoch 1385/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4560\n",
      "Epoch 01385: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4560 - val_loss: 1.4799\n",
      "Epoch 1386/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4495\n",
      "Epoch 01386: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4495 - val_loss: 1.4817\n",
      "Epoch 1387/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 01387: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4442 - val_loss: 1.4833\n",
      "Epoch 1388/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4380\n",
      "Epoch 01388: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4380 - val_loss: 1.4770\n",
      "Epoch 1389/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01389: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4368 - val_loss: 1.4788\n",
      "Epoch 1390/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01390: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4372 - val_loss: 1.4798\n",
      "Epoch 1391/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01391: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4342 - val_loss: 1.4822\n",
      "Epoch 1392/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01392: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4368 - val_loss: 1.4777\n",
      "Epoch 1393/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4389\n",
      "Epoch 01393: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4389 - val_loss: 1.4800\n",
      "Epoch 1394/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01394: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4361 - val_loss: 1.4771\n",
      "Epoch 1395/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01395: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4363 - val_loss: 1.4783\n",
      "Epoch 1396/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01396: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4341 - val_loss: 1.4816\n",
      "Epoch 1397/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01397: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4371 - val_loss: 1.4771\n",
      "Epoch 1398/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01398: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4345 - val_loss: 1.4803\n",
      "Epoch 1399/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01399: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4351 - val_loss: 1.4777\n",
      "Epoch 1400/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01400: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4343 - val_loss: 1.4846\n",
      "Epoch 1401/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4340\n",
      "Epoch 01401: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4340 - val_loss: 1.4769\n",
      "Epoch 1402/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01402: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4335 - val_loss: 1.4782\n",
      "Epoch 1403/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4360\n",
      "Epoch 01403: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4360 - val_loss: 1.4765\n",
      "Epoch 1404/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01404: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4351 - val_loss: 1.4779\n",
      "Epoch 1405/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01405: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4336 - val_loss: 1.4781\n",
      "Epoch 1406/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4352\n",
      "Epoch 01406: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4352 - val_loss: 1.4809\n",
      "Epoch 1407/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01407: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4351 - val_loss: 1.4795\n",
      "Epoch 1408/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01408: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4784\n",
      "Epoch 1409/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01409: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4335 - val_loss: 1.4755\n",
      "Epoch 1410/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01410: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4330 - val_loss: 1.4793\n",
      "Epoch 1411/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4339\n",
      "Epoch 01411: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4339 - val_loss: 1.4770\n",
      "Epoch 1412/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01412: loss did not improve from 1.43191\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4334 - val_loss: 1.4768\n",
      "Epoch 1413/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01413: loss improved from 1.43191 to 1.43100, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4310 - val_loss: 1.4769\n",
      "Epoch 1414/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01414: loss improved from 1.43100 to 1.43061, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4306 - val_loss: 1.4867\n",
      "Epoch 1415/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4373\n",
      "Epoch 01415: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4373 - val_loss: 1.4814\n",
      "Epoch 1416/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4373\n",
      "Epoch 01416: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4373 - val_loss: 1.4810\n",
      "Epoch 1417/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4339\n",
      "Epoch 01417: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4339 - val_loss: 1.4756\n",
      "Epoch 1418/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01418: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4344 - val_loss: 1.4798\n",
      "Epoch 1419/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4354\n",
      "Epoch 01419: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4354 - val_loss: 1.4788\n",
      "Epoch 1420/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4408\n",
      "Epoch 01420: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4408 - val_loss: 1.4791\n",
      "Epoch 1421/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 01421: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4418 - val_loss: 1.4787\n",
      "Epoch 1422/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01422: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4365 - val_loss: 1.4772\n",
      "Epoch 1423/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01423: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4356 - val_loss: 1.4844\n",
      "Epoch 1424/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01424: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4356 - val_loss: 1.4790\n",
      "Epoch 1425/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01425: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4326 - val_loss: 1.4789\n",
      "Epoch 1426/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01426: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4326 - val_loss: 1.4768\n",
      "Epoch 1427/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01427: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4318 - val_loss: 1.4817\n",
      "Epoch 1428/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01428: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4357 - val_loss: 1.4800\n",
      "Epoch 1429/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4383\n",
      "Epoch 01429: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4383 - val_loss: 1.4790\n",
      "Epoch 1430/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01430: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4382 - val_loss: 1.4779\n",
      "Epoch 1431/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4373\n",
      "Epoch 01431: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4373 - val_loss: 1.4771\n",
      "Epoch 1432/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01432: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4370 - val_loss: 1.4767\n",
      "Epoch 1433/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01433: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4359 - val_loss: 1.4797\n",
      "Epoch 1434/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01434: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4328 - val_loss: 1.4778\n",
      "Epoch 1435/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01435: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4315 - val_loss: 1.4780\n",
      "Epoch 1436/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01436: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4784\n",
      "Epoch 1437/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01437: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4330 - val_loss: 1.4813\n",
      "Epoch 1438/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01438: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4332 - val_loss: 1.4793\n",
      "Epoch 1439/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01439: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4323 - val_loss: 1.4761\n",
      "Epoch 1440/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01440: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4319 - val_loss: 1.4772\n",
      "Epoch 1441/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4321\n",
      "Epoch 01441: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4321 - val_loss: 1.4775\n",
      "Epoch 1442/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01442: loss did not improve from 1.43061\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4323 - val_loss: 1.4776\n",
      "Epoch 1443/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01443: loss improved from 1.43061 to 1.43029, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4303 - val_loss: 1.4756\n",
      "Epoch 1444/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01444: loss did not improve from 1.43029\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4318 - val_loss: 1.4770\n",
      "Epoch 1445/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01445: loss did not improve from 1.43029\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4319 - val_loss: 1.4756\n",
      "Epoch 1446/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01446: loss did not improve from 1.43029\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4315 - val_loss: 1.4770\n",
      "Epoch 1447/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01447: loss did not improve from 1.43029\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4310 - val_loss: 1.4752\n",
      "Epoch 1448/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 01448: loss did not improve from 1.43029\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4337 - val_loss: 1.4786\n",
      "Epoch 1449/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 01449: loss did not improve from 1.43029\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4337 - val_loss: 1.4762\n",
      "Epoch 1450/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01450: loss did not improve from 1.43029\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4314 - val_loss: 1.4758\n",
      "Epoch 1451/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01451: loss did not improve from 1.43029\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4320 - val_loss: 1.4774\n",
      "Epoch 1452/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01452: loss did not improve from 1.43029\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4346 - val_loss: 1.4847\n",
      "Epoch 1453/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01453: loss did not improve from 1.43029\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4336 - val_loss: 1.4767\n",
      "Epoch 1454/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01454: loss improved from 1.43029 to 1.43026, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4303 - val_loss: 1.4742\n",
      "Epoch 1455/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01455: loss did not improve from 1.43026\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4306 - val_loss: 1.4810\n",
      "Epoch 1456/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01456: loss did not improve from 1.43026\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4326 - val_loss: 1.4789\n",
      "Epoch 1457/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01457: loss did not improve from 1.43026\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4343 - val_loss: 1.4775\n",
      "Epoch 1458/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01458: loss did not improve from 1.43026\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4330 - val_loss: 1.4823\n",
      "Epoch 1459/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4324\n",
      "Epoch 01459: loss did not improve from 1.43026\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4324 - val_loss: 1.4826\n",
      "Epoch 1460/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01460: loss did not improve from 1.43026\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4353 - val_loss: 1.4785\n",
      "Epoch 1461/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4313\n",
      "Epoch 01461: loss did not improve from 1.43026\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4313 - val_loss: 1.4793\n",
      "Epoch 1462/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4324\n",
      "Epoch 01462: loss did not improve from 1.43026\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4324 - val_loss: 1.4825\n",
      "Epoch 1463/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01463: loss did not improve from 1.43026\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4323 - val_loss: 1.4778\n",
      "Epoch 1464/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01464: loss did not improve from 1.43026\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4309 - val_loss: 1.4759\n",
      "Epoch 1465/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4301\n",
      "Epoch 01465: loss improved from 1.43026 to 1.43010, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4301 - val_loss: 1.4745\n",
      "Epoch 1466/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01466: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4310 - val_loss: 1.4781\n",
      "Epoch 1467/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01467: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4309 - val_loss: 1.4789\n",
      "Epoch 1468/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01468: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4303 - val_loss: 1.4767\n",
      "Epoch 1469/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4301\n",
      "Epoch 01469: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4301 - val_loss: 1.4768\n",
      "Epoch 1470/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01470: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4306 - val_loss: 1.4796\n",
      "Epoch 1471/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01471: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4323 - val_loss: 1.4818\n",
      "Epoch 1472/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4397\n",
      "Epoch 01472: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4349 - val_loss: 1.4754\n",
      "Epoch 1473/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01473: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4335 - val_loss: 1.4784\n",
      "Epoch 1474/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4262\n",
      "Epoch 01474: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4351 - val_loss: 1.4825\n",
      "Epoch 1475/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4380\n",
      "Epoch 01475: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4375 - val_loss: 1.4791\n",
      "Epoch 1476/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01476: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4343 - val_loss: 1.4818\n",
      "Epoch 1477/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01477: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4331 - val_loss: 1.4816\n",
      "Epoch 1478/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01478: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4323 - val_loss: 1.4790\n",
      "Epoch 1479/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01479: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4325 - val_loss: 1.4753\n",
      "Epoch 1480/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01480: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4302 - val_loss: 1.4793\n",
      "Epoch 1481/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01481: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4317 - val_loss: 1.4779\n",
      "Epoch 1482/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4321\n",
      "Epoch 01482: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4321 - val_loss: 1.4773\n",
      "Epoch 1483/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4301\n",
      "Epoch 01483: loss did not improve from 1.43010\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4301 - val_loss: 1.4774\n",
      "Epoch 1484/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01484: loss improved from 1.43010 to 1.42936, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4294 - val_loss: 1.4784\n",
      "Epoch 1485/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01485: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4298 - val_loss: 1.4791\n",
      "Epoch 1486/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01486: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4328 - val_loss: 1.4773\n",
      "Epoch 1487/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4316\n",
      "Epoch 01487: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4316 - val_loss: 1.4775\n",
      "Epoch 1488/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01488: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4333 - val_loss: 1.4820\n",
      "Epoch 1489/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4431\n",
      "Epoch 01489: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4431 - val_loss: 1.4804\n",
      "Epoch 1490/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01490: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4382 - val_loss: 1.4801\n",
      "Epoch 1491/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01491: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4351 - val_loss: 1.4786\n",
      "Epoch 1492/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01492: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4315 - val_loss: 1.4811\n",
      "Epoch 1493/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 01493: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4347 - val_loss: 1.4816\n",
      "Epoch 1494/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01494: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4356 - val_loss: 1.4757\n",
      "Epoch 1495/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4354\n",
      "Epoch 01495: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4354 - val_loss: 1.4754\n",
      "Epoch 1496/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4352\n",
      "Epoch 01496: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4352 - val_loss: 1.4850\n",
      "Epoch 1497/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4433\n",
      "Epoch 01497: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4338 - val_loss: 1.4771\n",
      "Epoch 1498/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01498: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4345 - val_loss: 1.4808\n",
      "Epoch 1499/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4354\n",
      "Epoch 01499: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4354 - val_loss: 1.4820\n",
      "Epoch 1500/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 01500: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4347 - val_loss: 1.4813\n",
      "Epoch 1501/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01501: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4341 - val_loss: 1.4788\n",
      "Epoch 1502/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01502: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4351 - val_loss: 1.4758\n",
      "Epoch 1503/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01503: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4330 - val_loss: 1.4796\n",
      "Epoch 1504/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4267\n",
      "Epoch 01504: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4351 - val_loss: 1.4780\n",
      "Epoch 1505/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01505: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4355 - val_loss: 1.4772\n",
      "Epoch 1506/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01506: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4341 - val_loss: 1.4785\n",
      "Epoch 1507/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01507: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4318 - val_loss: 1.4798\n",
      "Epoch 1508/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01508: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4331 - val_loss: 1.4820\n",
      "Epoch 1509/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01509: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4326 - val_loss: 1.4794\n",
      "Epoch 1510/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01510: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4307 - val_loss: 1.4773\n",
      "Epoch 1511/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01511: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4312 - val_loss: 1.4766\n",
      "Epoch 1512/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 01512: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4337 - val_loss: 1.4792\n",
      "Epoch 1513/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01513: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4302 - val_loss: 1.4779\n",
      "Epoch 1514/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01514: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4309 - val_loss: 1.4794\n",
      "Epoch 1515/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01515: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4308 - val_loss: 1.4803\n",
      "Epoch 1516/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4329\n",
      "Epoch 01516: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4329 - val_loss: 1.4823\n",
      "Epoch 1517/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01517: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4357 - val_loss: 1.4776\n",
      "Epoch 1518/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01518: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4307 - val_loss: 1.4786\n",
      "Epoch 1519/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4312\n",
      "Epoch 01519: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4312 - val_loss: 1.4800\n",
      "Epoch 1520/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01520: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4342 - val_loss: 1.4825\n",
      "Epoch 1521/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4316\n",
      "Epoch 01521: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4316 - val_loss: 1.4817\n",
      "Epoch 1522/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01522: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4306 - val_loss: 1.4808\n",
      "Epoch 1523/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01523: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4343 - val_loss: 1.4834\n",
      "Epoch 1524/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01524: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4364 - val_loss: 1.4806\n",
      "Epoch 1525/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01525: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4344 - val_loss: 1.4781\n",
      "Epoch 1526/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4366\n",
      "Epoch 01526: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4366 - val_loss: 1.4758\n",
      "Epoch 1527/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01527: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4338 - val_loss: 1.4791\n",
      "Epoch 1528/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01528: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4331 - val_loss: 1.4863\n",
      "Epoch 1529/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01529: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4371 - val_loss: 1.4800\n",
      "Epoch 1530/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01530: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4351 - val_loss: 1.4765\n",
      "Epoch 1531/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01531: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4317 - val_loss: 1.4764\n",
      "Epoch 1532/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01532: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4319 - val_loss: 1.4788\n",
      "Epoch 1533/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01533: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4299 - val_loss: 1.4767\n",
      "Epoch 1534/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01534: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4320 - val_loss: 1.4808\n",
      "Epoch 1535/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01535: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4330 - val_loss: 1.4801\n",
      "Epoch 1536/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01536: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4333 - val_loss: 1.4768\n",
      "Epoch 1537/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01537: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4328 - val_loss: 1.4803\n",
      "Epoch 1538/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01538: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4336 - val_loss: 1.4783\n",
      "Epoch 1539/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4304\n",
      "Epoch 01539: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4304 - val_loss: 1.4775\n",
      "Epoch 1540/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4304\n",
      "Epoch 01540: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4304 - val_loss: 1.4764\n",
      "Epoch 1541/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01541: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4302 - val_loss: 1.4796\n",
      "Epoch 1542/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01542: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4297 - val_loss: 1.4801\n",
      "Epoch 1543/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01543: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4298 - val_loss: 1.4801\n",
      "Epoch 1544/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01544: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4294 - val_loss: 1.4786\n",
      "Epoch 1545/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01545: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4305 - val_loss: 1.4783\n",
      "Epoch 1546/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01546: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4308 - val_loss: 1.4823\n",
      "Epoch 1547/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4313\n",
      "Epoch 01547: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4313 - val_loss: 1.4763\n",
      "Epoch 1548/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01548: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4302 - val_loss: 1.4791\n",
      "Epoch 1549/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4316\n",
      "Epoch 01549: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4316 - val_loss: 1.4826\n",
      "Epoch 1550/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4404\n",
      "Epoch 01550: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4308 - val_loss: 1.4783\n",
      "Epoch 1551/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01551: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4311 - val_loss: 1.4817\n",
      "Epoch 1552/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01552: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4817\n",
      "Epoch 1553/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01553: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4320 - val_loss: 1.4772\n",
      "Epoch 1554/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01554: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4303 - val_loss: 1.4789\n",
      "Epoch 1555/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01555: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4309 - val_loss: 1.4835\n",
      "Epoch 1556/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01556: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4351 - val_loss: 1.4805\n",
      "Epoch 1557/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01557: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4323 - val_loss: 1.4785\n",
      "Epoch 1558/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01558: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4310 - val_loss: 1.4812\n",
      "Epoch 1559/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4301\n",
      "Epoch 01559: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4301 - val_loss: 1.4772\n",
      "Epoch 1560/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01560: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4311 - val_loss: 1.4775\n",
      "Epoch 1561/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01561: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4326 - val_loss: 1.4793\n",
      "Epoch 1562/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01562: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4323 - val_loss: 1.4838\n",
      "Epoch 1563/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01563: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4317 - val_loss: 1.4813\n",
      "Epoch 1564/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01564: loss did not improve from 1.42936\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4311 - val_loss: 1.4789\n",
      "Epoch 1565/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4184\n",
      "Epoch 01565: loss improved from 1.42936 to 1.42845, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4284 - val_loss: 1.4767\n",
      "Epoch 1566/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01566: loss improved from 1.42845 to 1.42756, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4276 - val_loss: 1.4802\n",
      "Epoch 1567/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4327\n",
      "Epoch 01567: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4327 - val_loss: 1.4806\n",
      "Epoch 1568/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01568: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4342 - val_loss: 1.4794\n",
      "Epoch 1569/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4316\n",
      "Epoch 01569: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4316 - val_loss: 1.4785\n",
      "Epoch 1570/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01570: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4310 - val_loss: 1.4796\n",
      "Epoch 1571/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01571: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4293 - val_loss: 1.4801\n",
      "Epoch 1572/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01572: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4299 - val_loss: 1.4783\n",
      "Epoch 1573/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01573: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4297 - val_loss: 1.4780\n",
      "Epoch 1574/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01574: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4288 - val_loss: 1.4815\n",
      "Epoch 1575/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01575: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4305 - val_loss: 1.4788\n",
      "Epoch 1576/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01576: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4297 - val_loss: 1.4796\n",
      "Epoch 1577/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01577: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4318 - val_loss: 1.4863\n",
      "Epoch 1578/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 01578: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4439 - val_loss: 1.4831\n",
      "Epoch 1579/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01579: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4382 - val_loss: 1.4801\n",
      "Epoch 1580/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01580: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4338 - val_loss: 1.4782\n",
      "Epoch 1581/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01581: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4309 - val_loss: 1.4785\n",
      "Epoch 1582/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01582: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4305 - val_loss: 1.4779\n",
      "Epoch 1583/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01583: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4307 - val_loss: 1.4776\n",
      "Epoch 1584/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01584: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4299 - val_loss: 1.4792\n",
      "Epoch 1585/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01585: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4314 - val_loss: 1.4836\n",
      "Epoch 1586/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4304\n",
      "Epoch 01586: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4304 - val_loss: 1.4787\n",
      "Epoch 1587/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01587: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4310 - val_loss: 1.4807\n",
      "Epoch 1588/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01588: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4320 - val_loss: 1.4779\n",
      "Epoch 1589/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 01589: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4290 - val_loss: 1.4779\n",
      "Epoch 1590/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01590: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4306 - val_loss: 1.4789\n",
      "Epoch 1591/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01591: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4295 - val_loss: 1.4788\n",
      "Epoch 1592/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01592: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4308 - val_loss: 1.4822\n",
      "Epoch 1593/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01593: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4319 - val_loss: 1.4801\n",
      "Epoch 1594/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4316\n",
      "Epoch 01594: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4316 - val_loss: 1.4790\n",
      "Epoch 1595/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01595: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4288 - val_loss: 1.4788\n",
      "Epoch 1596/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01596: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4288 - val_loss: 1.4772\n",
      "Epoch 1597/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01597: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4299 - val_loss: 1.4758\n",
      "Epoch 1598/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 01598: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4289 - val_loss: 1.4860\n",
      "Epoch 1599/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01599: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4326 - val_loss: 1.4834\n",
      "Epoch 1600/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01600: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4845\n",
      "Epoch 1601/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01601: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4368 - val_loss: 1.4822\n",
      "Epoch 1602/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4348\n",
      "Epoch 01602: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4348 - val_loss: 1.4819\n",
      "Epoch 1603/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01603: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4328 - val_loss: 1.4822\n",
      "Epoch 1604/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01604: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4310 - val_loss: 1.4828\n",
      "Epoch 1605/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4329\n",
      "Epoch 01605: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4329 - val_loss: 1.4789\n",
      "Epoch 1606/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01606: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4350 - val_loss: 1.4831\n",
      "Epoch 1607/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01607: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4320 - val_loss: 1.4816\n",
      "Epoch 1608/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01608: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4323 - val_loss: 1.4798\n",
      "Epoch 1609/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01609: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4330 - val_loss: 1.4788\n",
      "Epoch 1610/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4313\n",
      "Epoch 01610: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4313 - val_loss: 1.4774\n",
      "Epoch 1611/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01611: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4331 - val_loss: 1.4792\n",
      "Epoch 1612/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4324\n",
      "Epoch 01612: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4324 - val_loss: 1.4762\n",
      "Epoch 1613/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01613: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4319 - val_loss: 1.4795\n",
      "Epoch 1614/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01614: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4305 - val_loss: 1.4819\n",
      "Epoch 1615/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01615: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4336 - val_loss: 1.4791\n",
      "Epoch 1616/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01616: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4300 - val_loss: 1.4798\n",
      "Epoch 1617/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01617: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4300 - val_loss: 1.4795\n",
      "Epoch 1618/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01618: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4325 - val_loss: 1.4781\n",
      "Epoch 1619/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01619: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4335 - val_loss: 1.4795\n",
      "Epoch 1620/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01620: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4342 - val_loss: 1.4815\n",
      "Epoch 1621/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4304\n",
      "Epoch 01621: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4304 - val_loss: 1.4794\n",
      "Epoch 1622/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01622: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4281 - val_loss: 1.4763\n",
      "Epoch 1623/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01623: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4285 - val_loss: 1.4772\n",
      "Epoch 1624/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01624: loss did not improve from 1.42756\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4285 - val_loss: 1.4800\n",
      "Epoch 1625/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01625: loss improved from 1.42756 to 1.42723, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4272 - val_loss: 1.4784\n",
      "Epoch 1626/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01626: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4288 - val_loss: 1.4783\n",
      "Epoch 1627/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01627: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4309 - val_loss: 1.4803\n",
      "Epoch 1628/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01628: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4317 - val_loss: 1.4781\n",
      "Epoch 1629/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01629: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4317 - val_loss: 1.4803\n",
      "Epoch 1630/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01630: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4314 - val_loss: 1.4807\n",
      "Epoch 1631/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01631: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4336 - val_loss: 1.4914\n",
      "Epoch 1632/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01632: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4319 - val_loss: 1.4790\n",
      "Epoch 1633/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01633: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4318 - val_loss: 1.4762\n",
      "Epoch 1634/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4329\n",
      "Epoch 01634: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4329 - val_loss: 1.4792\n",
      "Epoch 1635/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01635: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4314 - val_loss: 1.4803\n",
      "Epoch 1636/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01636: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4311 - val_loss: 1.4775\n",
      "Epoch 1637/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 01637: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4287 - val_loss: 1.4781\n",
      "Epoch 1638/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01638: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4307 - val_loss: 1.4771\n",
      "Epoch 1639/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01639: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4305 - val_loss: 1.4808\n",
      "Epoch 1640/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01640: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4305 - val_loss: 1.4805\n",
      "Epoch 1641/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01641: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4297 - val_loss: 1.4756\n",
      "Epoch 1642/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01642: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4288 - val_loss: 1.4809\n",
      "Epoch 1643/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 01643: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4283 - val_loss: 1.4793\n",
      "Epoch 1644/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01644: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4309 - val_loss: 1.4808\n",
      "Epoch 1645/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01645: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4326 - val_loss: 1.4840\n",
      "Epoch 1646/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01646: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4377 - val_loss: 1.4812\n",
      "Epoch 1647/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01647: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4346 - val_loss: 1.4827\n",
      "Epoch 1648/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 01648: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4400 - val_loss: 1.4875\n",
      "Epoch 1649/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01649: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4365 - val_loss: 1.4802\n",
      "Epoch 1650/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01650: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4320 - val_loss: 1.4797\n",
      "Epoch 1651/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4313\n",
      "Epoch 01651: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4313 - val_loss: 1.4799\n",
      "Epoch 1652/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01652: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4299 - val_loss: 1.4799\n",
      "Epoch 1653/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4333\n",
      "Epoch 01653: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4333 - val_loss: 1.4805\n",
      "Epoch 1654/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01654: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4350 - val_loss: 1.4840\n",
      "Epoch 1655/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01655: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.4322 - val_loss: 1.4836\n",
      "Epoch 1656/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4267\n",
      "Epoch 01656: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4330 - val_loss: 1.4816\n",
      "Epoch 1657/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4324\n",
      "Epoch 01657: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4324 - val_loss: 1.4834\n",
      "Epoch 1658/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01658: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 1.4297 - val_loss: 1.4772\n",
      "Epoch 1659/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01659: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4304 - val_loss: 1.4792\n",
      "Epoch 1660/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01660: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4309 - val_loss: 1.4794\n",
      "Epoch 1661/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4292\n",
      "Epoch 01661: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4292 - val_loss: 1.4767\n",
      "Epoch 1662/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01662: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4285 - val_loss: 1.4805\n",
      "Epoch 1663/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01663: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4285 - val_loss: 1.4794\n",
      "Epoch 1664/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4251\n",
      "Epoch 01664: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4306 - val_loss: 1.4854\n",
      "Epoch 1665/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01665: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4314 - val_loss: 1.4804\n",
      "Epoch 1666/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4366\n",
      "Epoch 01666: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4315 - val_loss: 1.4803\n",
      "Epoch 1667/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01667: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4294 - val_loss: 1.4767\n",
      "Epoch 1668/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 01668: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4289 - val_loss: 1.4818\n",
      "Epoch 1669/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4316\n",
      "Epoch 01669: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4316 - val_loss: 1.4815\n",
      "Epoch 1670/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01670: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4311 - val_loss: 1.4796\n",
      "Epoch 1671/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01671: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4300 - val_loss: 1.4774\n",
      "Epoch 1672/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01672: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4280 - val_loss: 1.4847\n",
      "Epoch 1673/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01673: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4300 - val_loss: 1.4820\n",
      "Epoch 1674/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4214\n",
      "Epoch 01674: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4325 - val_loss: 1.4798\n",
      "Epoch 1675/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01675: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4298 - val_loss: 1.4774\n",
      "Epoch 1676/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 01676: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4296 - val_loss: 1.4806\n",
      "Epoch 1677/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01677: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4298 - val_loss: 1.4786\n",
      "Epoch 1678/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01678: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4300 - val_loss: 1.4841\n",
      "Epoch 1679/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01679: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4308 - val_loss: 1.4801\n",
      "Epoch 1680/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4329\n",
      "Epoch 01680: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4329 - val_loss: 1.4796\n",
      "Epoch 1681/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01681: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4298 - val_loss: 1.4798\n",
      "Epoch 1682/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01682: loss did not improve from 1.42723\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4284 - val_loss: 1.4766\n",
      "Epoch 1683/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01683: loss improved from 1.42723 to 1.42717, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4272 - val_loss: 1.4800\n",
      "Epoch 1684/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01684: loss did not improve from 1.42717\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4286 - val_loss: 1.4810\n",
      "Epoch 1685/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01685: loss did not improve from 1.42717\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4298 - val_loss: 1.4788\n",
      "Epoch 1686/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01686: loss did not improve from 1.42717\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4284 - val_loss: 1.4794\n",
      "Epoch 1687/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01687: loss did not improve from 1.42717\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4297 - val_loss: 1.4860\n",
      "Epoch 1688/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 01688: loss did not improve from 1.42717\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4347 - val_loss: 1.4784\n",
      "Epoch 1689/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01689: loss did not improve from 1.42717\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4312 - val_loss: 1.4788\n",
      "Epoch 1690/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01690: loss did not improve from 1.42717\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4311 - val_loss: 1.4827\n",
      "Epoch 1691/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01691: loss did not improve from 1.42717\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4309 - val_loss: 1.4805\n",
      "Epoch 1692/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01692: loss did not improve from 1.42717\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4293 - val_loss: 1.4775\n",
      "Epoch 1693/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4236\n",
      "Epoch 01693: loss did not improve from 1.42717\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4273 - val_loss: 1.4800\n",
      "Epoch 1694/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01694: loss did not improve from 1.42717\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4303 - val_loss: 1.4805\n",
      "Epoch 1695/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01695: loss improved from 1.42717 to 1.42693, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4269 - val_loss: 1.4818\n",
      "Epoch 1696/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 01696: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4290 - val_loss: 1.4774\n",
      "Epoch 1697/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4280\n",
      "Epoch 01697: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4280 - val_loss: 1.4784\n",
      "Epoch 1698/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01698: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4284 - val_loss: 1.4814\n",
      "Epoch 1699/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01699: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4291 - val_loss: 1.4812\n",
      "Epoch 1700/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 01700: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4290 - val_loss: 1.4839\n",
      "Epoch 1701/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01701: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4318 - val_loss: 1.4800\n",
      "Epoch 1702/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01702: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4300 - val_loss: 1.4778\n",
      "Epoch 1703/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01703: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4279 - val_loss: 1.4788\n",
      "Epoch 1704/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01704: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4303 - val_loss: 1.4820\n",
      "Epoch 1705/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01705: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4319 - val_loss: 1.4822\n",
      "Epoch 1706/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01706: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4307 - val_loss: 1.4808\n",
      "Epoch 1707/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01707: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4807\n",
      "Epoch 1708/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01708: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4274 - val_loss: 1.4813\n",
      "Epoch 1709/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01709: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4269 - val_loss: 1.4842\n",
      "Epoch 1710/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01710: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4311 - val_loss: 1.4805\n",
      "Epoch 1711/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01711: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4295 - val_loss: 1.4808\n",
      "Epoch 1712/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4322\n",
      "Epoch 01712: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4322 - val_loss: 1.4779\n",
      "Epoch 1713/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4249\n",
      "Epoch 01713: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4293 - val_loss: 1.4792\n",
      "Epoch 1714/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01714: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4284 - val_loss: 1.4819\n",
      "Epoch 1715/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01715: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4285 - val_loss: 1.4803\n",
      "Epoch 1716/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 01716: loss did not improve from 1.42693\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4289 - val_loss: 1.4775\n",
      "Epoch 1717/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01717: loss improved from 1.42693 to 1.42653, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4265 - val_loss: 1.4768\n",
      "Epoch 1718/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 01718: loss improved from 1.42653 to 1.42600, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4260 - val_loss: 1.4797\n",
      "Epoch 1719/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4354\n",
      "Epoch 01719: loss did not improve from 1.42600\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4264 - val_loss: 1.4797\n",
      "Epoch 1720/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 01720: loss did not improve from 1.42600\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4283 - val_loss: 1.4791\n",
      "Epoch 1721/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01721: loss did not improve from 1.42600\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4286 - val_loss: 1.4811\n",
      "Epoch 1722/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01722: loss did not improve from 1.42600\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4272 - val_loss: 1.4770\n",
      "Epoch 1723/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 01723: loss improved from 1.42600 to 1.42568, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4257 - val_loss: 1.4794\n",
      "Epoch 1724/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4270\n",
      "Epoch 01724: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4270 - val_loss: 1.4779\n",
      "Epoch 1725/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4282\n",
      "Epoch 01725: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4282 - val_loss: 1.4805\n",
      "Epoch 1726/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 01726: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4287 - val_loss: 1.4819\n",
      "Epoch 1727/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4282\n",
      "Epoch 01727: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4282 - val_loss: 1.4787\n",
      "Epoch 1728/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 01728: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4263 - val_loss: 1.4807\n",
      "Epoch 1729/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01729: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4814\n",
      "Epoch 1730/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01730: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4274 - val_loss: 1.4810\n",
      "Epoch 1731/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01731: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4285 - val_loss: 1.4773\n",
      "Epoch 1732/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01732: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4285 - val_loss: 1.4779\n",
      "Epoch 1733/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4280\n",
      "Epoch 01733: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4280 - val_loss: 1.4796\n",
      "Epoch 1734/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01734: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4318 - val_loss: 1.4812\n",
      "Epoch 1735/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01735: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4306 - val_loss: 1.4831\n",
      "Epoch 1736/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01736: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4801\n",
      "Epoch 1737/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4321\n",
      "Epoch 01737: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4321 - val_loss: 1.4794\n",
      "Epoch 1738/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01738: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4271 - val_loss: 1.4811\n",
      "Epoch 1739/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 01739: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4277 - val_loss: 1.4804\n",
      "Epoch 1740/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 01740: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4275 - val_loss: 1.4802\n",
      "Epoch 1741/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01741: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4265 - val_loss: 1.4810\n",
      "Epoch 1742/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01742: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4274 - val_loss: 1.4799\n",
      "Epoch 1743/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01743: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4272 - val_loss: 1.4809\n",
      "Epoch 1744/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4266\n",
      "Epoch 01744: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4266 - val_loss: 1.4827\n",
      "Epoch 1745/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 01745: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4277 - val_loss: 1.4852\n",
      "Epoch 1746/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01746: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4279 - val_loss: 1.4810\n",
      "Epoch 1747/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01747: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4278 - val_loss: 1.4815\n",
      "Epoch 1748/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4304\n",
      "Epoch 01748: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4304 - val_loss: 1.4815\n",
      "Epoch 1749/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01749: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4305 - val_loss: 1.4836\n",
      "Epoch 1750/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01750: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4291 - val_loss: 1.4817\n",
      "Epoch 1751/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01751: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4276 - val_loss: 1.4839\n",
      "Epoch 1752/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01752: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4286 - val_loss: 1.4807\n",
      "Epoch 1753/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01753: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4285 - val_loss: 1.4794\n",
      "Epoch 1754/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 01754: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4287 - val_loss: 1.4822\n",
      "Epoch 1755/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01755: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4273 - val_loss: 1.4801\n",
      "Epoch 1756/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01756: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4311 - val_loss: 1.4790\n",
      "Epoch 1757/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01757: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4309 - val_loss: 1.4841\n",
      "Epoch 1758/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01758: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4307 - val_loss: 1.4818\n",
      "Epoch 1759/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01759: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4286 - val_loss: 1.4825\n",
      "Epoch 1760/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01760: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4291 - val_loss: 1.4812\n",
      "Epoch 1761/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01761: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4279 - val_loss: 1.4834\n",
      "Epoch 1762/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01762: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4323 - val_loss: 1.4802\n",
      "Epoch 1763/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4333\n",
      "Epoch 01763: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4313 - val_loss: 1.4826\n",
      "Epoch 1764/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 01764: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4289 - val_loss: 1.4810\n",
      "Epoch 1765/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01765: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4272 - val_loss: 1.4802\n",
      "Epoch 1766/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01766: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4284 - val_loss: 1.4819\n",
      "Epoch 1767/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01767: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4274 - val_loss: 1.4819\n",
      "Epoch 1768/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01768: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4297 - val_loss: 1.4806\n",
      "Epoch 1769/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 01769: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4268 - val_loss: 1.4813\n",
      "Epoch 1770/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 01770: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4264 - val_loss: 1.4800\n",
      "Epoch 1771/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01771: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4302 - val_loss: 1.4828\n",
      "Epoch 1772/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01772: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4318 - val_loss: 1.4810\n",
      "Epoch 1773/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4322\n",
      "Epoch 01773: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4322 - val_loss: 1.4835\n",
      "Epoch 1774/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01774: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4320 - val_loss: 1.4813\n",
      "Epoch 1775/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01775: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4319 - val_loss: 1.4794\n",
      "Epoch 1776/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01776: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4295 - val_loss: 1.4817\n",
      "Epoch 1777/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01777: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4284 - val_loss: 1.4790\n",
      "Epoch 1778/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 01778: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4260 - val_loss: 1.4791\n",
      "Epoch 1779/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01779: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4276 - val_loss: 1.4820\n",
      "Epoch 1780/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01780: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4284 - val_loss: 1.4828\n",
      "Epoch 1781/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01781: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4285 - val_loss: 1.4852\n",
      "Epoch 1782/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4349\n",
      "Epoch 01782: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4318 - val_loss: 1.4814\n",
      "Epoch 1783/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01783: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4276 - val_loss: 1.4797\n",
      "Epoch 1784/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01784: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4298 - val_loss: 1.4813\n",
      "Epoch 1785/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4322\n",
      "Epoch 01785: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4322 - val_loss: 1.4812\n",
      "Epoch 1786/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4386\n",
      "Epoch 01786: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4386 - val_loss: 1.4847\n",
      "Epoch 1787/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01787: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4346 - val_loss: 1.4825\n",
      "Epoch 1788/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01788: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4350 - val_loss: 1.4814\n",
      "Epoch 1789/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01789: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4294 - val_loss: 1.4827\n",
      "Epoch 1790/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01790: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4279 - val_loss: 1.4795\n",
      "Epoch 1791/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01791: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4265 - val_loss: 1.4772\n",
      "Epoch 1792/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 01792: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4290 - val_loss: 1.4821\n",
      "Epoch 1793/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01793: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4294 - val_loss: 1.4828\n",
      "Epoch 1794/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01794: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4308 - val_loss: 1.4833\n",
      "Epoch 1795/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01795: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4309 - val_loss: 1.4826\n",
      "Epoch 1796/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01796: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4294 - val_loss: 1.4831\n",
      "Epoch 1797/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01797: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4285 - val_loss: 1.4818\n",
      "Epoch 1798/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01798: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4281 - val_loss: 1.4805\n",
      "Epoch 1799/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01799: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4281 - val_loss: 1.4795\n",
      "Epoch 1800/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01800: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4284 - val_loss: 1.4815\n",
      "Epoch 1801/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 01801: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4277 - val_loss: 1.4782\n",
      "Epoch 1802/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01802: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4269 - val_loss: 1.4814\n",
      "Epoch 1803/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4270\n",
      "Epoch 01803: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4270 - val_loss: 1.4839\n",
      "Epoch 1804/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01804: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4281 - val_loss: 1.4809\n",
      "Epoch 1805/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4352\n",
      "Epoch 01805: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4352 - val_loss: 1.4827\n",
      "Epoch 1806/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01806: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4363 - val_loss: 1.4816\n",
      "Epoch 1807/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01807: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4334 - val_loss: 1.4808\n",
      "Epoch 1808/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01808: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4341 - val_loss: 1.4831\n",
      "Epoch 1809/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4333\n",
      "Epoch 01809: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4333 - val_loss: 1.4870\n",
      "Epoch 1810/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01810: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4315 - val_loss: 1.4827\n",
      "Epoch 1811/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01811: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4274 - val_loss: 1.4802\n",
      "Epoch 1812/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01812: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4281 - val_loss: 1.4837\n",
      "Epoch 1813/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4237\n",
      "Epoch 01813: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4293 - val_loss: 1.4795\n",
      "Epoch 1814/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01814: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4294 - val_loss: 1.4783\n",
      "Epoch 1815/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 01815: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4264 - val_loss: 1.4840\n",
      "Epoch 1816/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01816: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4304 - val_loss: 1.4839\n",
      "Epoch 1817/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01817: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4319 - val_loss: 1.4803\n",
      "Epoch 1818/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01818: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4314 - val_loss: 1.4864\n",
      "Epoch 1819/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 01819: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4296 - val_loss: 1.4863\n",
      "Epoch 1820/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01820: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4278 - val_loss: 1.4821\n",
      "Epoch 1821/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01821: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4288 - val_loss: 1.4787\n",
      "Epoch 1822/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 01822: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4257 - val_loss: 1.4839\n",
      "Epoch 1823/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4292\n",
      "Epoch 01823: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4292 - val_loss: 1.4799\n",
      "Epoch 1824/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01824: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4265 - val_loss: 1.4800\n",
      "Epoch 1825/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4270\n",
      "Epoch 01825: loss did not improve from 1.42568\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4270 - val_loss: 1.4795\n",
      "Epoch 1826/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01826: loss improved from 1.42568 to 1.42562, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4256 - val_loss: 1.4799\n",
      "Epoch 1827/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4254\n",
      "Epoch 01827: loss improved from 1.42562 to 1.42537, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4254 - val_loss: 1.4804\n",
      "Epoch 1828/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 01828: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4268 - val_loss: 1.4852\n",
      "Epoch 1829/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 01829: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4283 - val_loss: 1.4799\n",
      "Epoch 1830/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01830: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4271 - val_loss: 1.4848\n",
      "Epoch 1831/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 01831: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4283 - val_loss: 1.4841\n",
      "Epoch 1832/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01832: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4286 - val_loss: 1.4845\n",
      "Epoch 1833/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 01833: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4290 - val_loss: 1.4834\n",
      "Epoch 1834/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01834: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4284 - val_loss: 1.4822\n",
      "Epoch 1835/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4262\n",
      "Epoch 01835: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4262 - val_loss: 1.4841\n",
      "Epoch 1836/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01836: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4291 - val_loss: 1.4816\n",
      "Epoch 1837/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01837: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4297 - val_loss: 1.4843\n",
      "Epoch 1838/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01838: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4286 - val_loss: 1.4855\n",
      "Epoch 1839/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01839: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4302 - val_loss: 1.4826\n",
      "Epoch 1840/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01840: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4297 - val_loss: 1.4785\n",
      "Epoch 1841/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4206\n",
      "Epoch 01841: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4291 - val_loss: 1.4791\n",
      "Epoch 1842/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 01842: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4260 - val_loss: 1.4849\n",
      "Epoch 1843/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 01843: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4277 - val_loss: 1.4815\n",
      "Epoch 1844/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01844: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4275 - val_loss: 1.4821\n",
      "Epoch 1845/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01845: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4256 - val_loss: 1.4800\n",
      "Epoch 1846/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 01846: loss did not improve from 1.42537\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4257 - val_loss: 1.4804\n",
      "Epoch 1847/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 01847: loss improved from 1.42537 to 1.42521, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4252 - val_loss: 1.4870\n",
      "Epoch 1848/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01848: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4345 - val_loss: 1.4873\n",
      "Epoch 1849/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01849: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4332 - val_loss: 1.4873\n",
      "Epoch 1850/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4322\n",
      "Epoch 01850: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4322 - val_loss: 1.4836\n",
      "Epoch 1851/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01851: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4345 - val_loss: 1.4860\n",
      "Epoch 1852/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01852: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4317 - val_loss: 1.4834\n",
      "Epoch 1853/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01853: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4293 - val_loss: 1.4831\n",
      "Epoch 1854/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4292\n",
      "Epoch 01854: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4292 - val_loss: 1.4790\n",
      "Epoch 1855/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01855: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4279 - val_loss: 1.4813\n",
      "Epoch 1856/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01856: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4307 - val_loss: 1.4849\n",
      "Epoch 1857/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01857: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4288 - val_loss: 1.4802\n",
      "Epoch 1858/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01858: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4269 - val_loss: 1.4792\n",
      "Epoch 1859/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 01859: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4261 - val_loss: 1.4803\n",
      "Epoch 1860/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 01860: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4261 - val_loss: 1.4796\n",
      "Epoch 1861/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01861: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4278 - val_loss: 1.4800\n",
      "Epoch 1862/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 01862: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4261 - val_loss: 1.4818\n",
      "Epoch 1863/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01863: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4273 - val_loss: 1.4869\n",
      "Epoch 1864/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01864: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4285 - val_loss: 1.4808\n",
      "Epoch 1865/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 01865: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4275 - val_loss: 1.4846\n",
      "Epoch 1866/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 01866: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4275 - val_loss: 1.4798\n",
      "Epoch 1867/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01867: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4291 - val_loss: 1.4815\n",
      "Epoch 1868/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4266\n",
      "Epoch 01868: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4266 - val_loss: 1.4865\n",
      "Epoch 1869/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01869: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4272 - val_loss: 1.4840\n",
      "Epoch 1870/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01870: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4276 - val_loss: 1.4862\n",
      "Epoch 1871/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01871: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4285 - val_loss: 1.4818\n",
      "Epoch 1872/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 01872: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4869\n",
      "Epoch 1873/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01873: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4281 - val_loss: 1.4816\n",
      "Epoch 1874/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01874: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4276 - val_loss: 1.4809\n",
      "Epoch 1875/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01875: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4274 - val_loss: 1.4804\n",
      "Epoch 1876/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 01876: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4260 - val_loss: 1.4826\n",
      "Epoch 1877/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01877: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4269 - val_loss: 1.4855\n",
      "Epoch 1878/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 01878: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4277 - val_loss: 1.4830\n",
      "Epoch 1879/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01879: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4265 - val_loss: 1.4809\n",
      "Epoch 1880/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01880: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4256 - val_loss: 1.4847\n",
      "Epoch 1881/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 01881: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4267 - val_loss: 1.4831\n",
      "Epoch 1882/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 01882: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4275 - val_loss: 1.4825\n",
      "Epoch 1883/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01883: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4276 - val_loss: 1.4811\n",
      "Epoch 1884/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01884: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4272 - val_loss: 1.4818\n",
      "Epoch 1885/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01885: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4269 - val_loss: 1.4815\n",
      "Epoch 1886/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01886: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4302 - val_loss: 1.4771\n",
      "Epoch 1887/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01887: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4284 - val_loss: 1.4852\n",
      "Epoch 1888/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01888: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4274 - val_loss: 1.4864\n",
      "Epoch 1889/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4266\n",
      "Epoch 01889: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4266 - val_loss: 1.4825\n",
      "Epoch 1890/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01890: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4294 - val_loss: 1.4810\n",
      "Epoch 1891/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 01891: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4261 - val_loss: 1.4825\n",
      "Epoch 1892/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 01892: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4852\n",
      "Epoch 1893/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4301\n",
      "Epoch 01893: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4301 - val_loss: 1.4822\n",
      "Epoch 1894/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01894: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4272 - val_loss: 1.4814\n",
      "Epoch 1895/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 01895: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4258 - val_loss: 1.4827\n",
      "Epoch 1896/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01896: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4273 - val_loss: 1.4873\n",
      "Epoch 1897/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01897: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4305 - val_loss: 1.4792\n",
      "Epoch 1898/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01898: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4274 - val_loss: 1.4822\n",
      "Epoch 1899/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4266\n",
      "Epoch 01899: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4266 - val_loss: 1.4878\n",
      "Epoch 1900/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01900: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4302 - val_loss: 1.4817\n",
      "Epoch 1901/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01901: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4272 - val_loss: 1.4800\n",
      "Epoch 1902/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 01902: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4287 - val_loss: 1.4806\n",
      "Epoch 1903/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01903: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4271 - val_loss: 1.4871\n",
      "Epoch 1904/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01904: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4828\n",
      "Epoch 1905/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01905: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4295 - val_loss: 1.4835\n",
      "Epoch 1906/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01906: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4821\n",
      "Epoch 1907/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01907: loss did not improve from 1.42521\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4278 - val_loss: 1.4801\n",
      "Epoch 1908/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 01908: loss improved from 1.42521 to 1.42518, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4252 - val_loss: 1.4818\n",
      "Epoch 1909/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4247\n",
      "Epoch 01909: loss improved from 1.42518 to 1.42509, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4251 - val_loss: 1.4844\n",
      "Epoch 1910/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 01910: loss did not improve from 1.42509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4255 - val_loss: 1.4800\n",
      "Epoch 1911/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 01911: loss improved from 1.42509 to 1.42486, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4249 - val_loss: 1.4813\n",
      "Epoch 1912/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 01912: loss improved from 1.42486 to 1.42413, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4241 - val_loss: 1.4825\n",
      "Epoch 1913/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 01913: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4268 - val_loss: 1.4821\n",
      "Epoch 1914/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4191\n",
      "Epoch 01914: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4823\n",
      "Epoch 1915/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 01915: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4263 - val_loss: 1.4812\n",
      "Epoch 1916/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4219\n",
      "Epoch 01916: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4245 - val_loss: 1.4800\n",
      "Epoch 1917/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01917: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4274 - val_loss: 1.4821\n",
      "Epoch 1918/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 01918: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4283 - val_loss: 1.4832\n",
      "Epoch 1919/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01919: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4293 - val_loss: 1.4896\n",
      "Epoch 1920/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01920: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4293 - val_loss: 1.4848\n",
      "Epoch 1921/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01921: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4831\n",
      "Epoch 1922/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 01922: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4259 - val_loss: 1.4798\n",
      "Epoch 1923/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 01923: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4245 - val_loss: 1.4848\n",
      "Epoch 1924/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 01924: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4268 - val_loss: 1.4840\n",
      "Epoch 1925/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4282\n",
      "Epoch 01925: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4282 - val_loss: 1.4862\n",
      "Epoch 1926/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 01926: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4277 - val_loss: 1.4808\n",
      "Epoch 1927/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 01927: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4267 - val_loss: 1.4808\n",
      "Epoch 1928/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 01928: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4263 - val_loss: 1.4819\n",
      "Epoch 1929/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4384\n",
      "Epoch 01929: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4263 - val_loss: 1.4815\n",
      "Epoch 1930/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 01930: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4268 - val_loss: 1.4799\n",
      "Epoch 1931/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 01931: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4788\n",
      "Epoch 1932/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 01932: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4246 - val_loss: 1.4843\n",
      "Epoch 1933/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 01933: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4253 - val_loss: 1.4846\n",
      "Epoch 1934/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01934: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4284 - val_loss: 1.4819\n",
      "Epoch 1935/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 01935: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4287 - val_loss: 1.4818\n",
      "Epoch 1936/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4312\n",
      "Epoch 01936: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4270 - val_loss: 1.4857\n",
      "Epoch 1937/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01937: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4277 - val_loss: 1.4821\n",
      "Epoch 1938/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01938: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4274 - val_loss: 1.4831\n",
      "Epoch 1939/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 01939: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4264 - val_loss: 1.4856\n",
      "Epoch 1940/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 01940: loss did not improve from 1.42413\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4248 - val_loss: 1.4817\n",
      "Epoch 1941/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4237\n",
      "Epoch 01941: loss improved from 1.42413 to 1.42365, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4237 - val_loss: 1.4810\n",
      "Epoch 1942/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01942: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4244 - val_loss: 1.4829\n",
      "Epoch 1943/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 01943: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4244 - val_loss: 1.4811\n",
      "Epoch 1944/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 01944: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4832\n",
      "Epoch 1945/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 01945: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4252 - val_loss: 1.4819\n",
      "Epoch 1946/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01946: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4241 - val_loss: 1.4809\n",
      "Epoch 1947/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 01947: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4246 - val_loss: 1.4824\n",
      "Epoch 1948/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 01948: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4245 - val_loss: 1.4801\n",
      "Epoch 1949/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4262\n",
      "Epoch 01949: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4262 - val_loss: 1.4887\n",
      "Epoch 1950/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01950: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4824\n",
      "Epoch 1951/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01951: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4800\n",
      "Epoch 1952/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01952: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4843\n",
      "Epoch 1953/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 01953: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4257 - val_loss: 1.4815\n",
      "Epoch 1954/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 01954: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4249 - val_loss: 1.4833\n",
      "Epoch 1955/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 01955: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4251 - val_loss: 1.4824\n",
      "Epoch 1956/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 01956: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4257 - val_loss: 1.4800\n",
      "Epoch 1957/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01957: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4835\n",
      "Epoch 1958/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 01958: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4283 - val_loss: 1.4825\n",
      "Epoch 1959/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01959: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4269 - val_loss: 1.4833\n",
      "Epoch 1960/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 01960: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4296 - val_loss: 1.4876\n",
      "Epoch 1961/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 01961: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4283 - val_loss: 1.4801\n",
      "Epoch 1962/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01962: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4271 - val_loss: 1.4843\n",
      "Epoch 1963/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4266\n",
      "Epoch 01963: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4266 - val_loss: 1.4808\n",
      "Epoch 1964/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 01964: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4264 - val_loss: 1.4801\n",
      "Epoch 1965/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4242\n",
      "Epoch 01965: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4242 - val_loss: 1.4807\n",
      "Epoch 1966/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01966: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4830\n",
      "Epoch 1967/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4161\n",
      "Epoch 01967: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4256 - val_loss: 1.4854\n",
      "Epoch 1968/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01968: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4269 - val_loss: 1.4820\n",
      "Epoch 1969/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 01969: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4260 - val_loss: 1.4836\n",
      "Epoch 1970/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 01970: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4290 - val_loss: 1.4828\n",
      "Epoch 1971/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4262\n",
      "Epoch 01971: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4262 - val_loss: 1.4796\n",
      "Epoch 1972/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01972: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4279 - val_loss: 1.4843\n",
      "Epoch 1973/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4327\n",
      "Epoch 01973: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4327 - val_loss: 1.4860\n",
      "Epoch 1974/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01974: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4308 - val_loss: 1.4907\n",
      "Epoch 1975/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01975: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4273 - val_loss: 1.4816\n",
      "Epoch 1976/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 01976: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4241 - val_loss: 1.4851\n",
      "Epoch 1977/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 01977: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4268 - val_loss: 1.4828\n",
      "Epoch 1978/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 01978: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4244 - val_loss: 1.4829\n",
      "Epoch 1979/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01979: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4272 - val_loss: 1.4811\n",
      "Epoch 1980/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 01980: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4264 - val_loss: 1.4822\n",
      "Epoch 1981/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4282\n",
      "Epoch 01981: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4282 - val_loss: 1.4864\n",
      "Epoch 1982/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01982: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4817\n",
      "Epoch 1983/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 01983: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4249 - val_loss: 1.4829\n",
      "Epoch 1984/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 01984: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4287 - val_loss: 1.4813\n",
      "Epoch 1985/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4292\n",
      "Epoch 01985: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4292 - val_loss: 1.4829\n",
      "Epoch 1986/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 01986: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4259 - val_loss: 1.4830\n",
      "Epoch 1987/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 01987: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4263 - val_loss: 1.4815\n",
      "Epoch 1988/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 01988: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4252 - val_loss: 1.4830\n",
      "Epoch 1989/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 01989: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4258 - val_loss: 1.4846\n",
      "Epoch 1990/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01990: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4294 - val_loss: 1.4841\n",
      "Epoch 1991/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4292\n",
      "Epoch 01991: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4292 - val_loss: 1.4805\n",
      "Epoch 1992/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01992: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4334 - val_loss: 1.4883\n",
      "Epoch 1993/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01993: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4318 - val_loss: 1.4856\n",
      "Epoch 1994/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01994: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4300 - val_loss: 1.4836\n",
      "Epoch 1995/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01995: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4269 - val_loss: 1.4821\n",
      "Epoch 1996/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 01996: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4820\n",
      "Epoch 1997/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 01997: loss did not improve from 1.42365\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4247 - val_loss: 1.4803\n",
      "Epoch 1998/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 01998: loss improved from 1.42365 to 1.42359, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4236 - val_loss: 1.4844\n",
      "Epoch 1999/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 01999: loss did not improve from 1.42359\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4822\n",
      "Epoch 2000/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02000: loss did not improve from 1.42359\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4832\n",
      "Epoch 2001/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 02001: loss did not improve from 1.42359\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4249 - val_loss: 1.4856\n",
      "Epoch 2002/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 02002: loss did not improve from 1.42359\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4249 - val_loss: 1.4851\n",
      "Epoch 2003/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02003: loss did not improve from 1.42359\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4261 - val_loss: 1.4830\n",
      "Epoch 2004/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02004: loss did not improve from 1.42359\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4801\n",
      "Epoch 2005/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02005: loss improved from 1.42359 to 1.42203, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4220 - val_loss: 1.4807\n",
      "Epoch 2006/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02006: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4240 - val_loss: 1.4865\n",
      "Epoch 2007/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 02007: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4860\n",
      "Epoch 2008/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02008: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4826\n",
      "Epoch 2009/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 02009: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4328 - val_loss: 1.4827\n",
      "Epoch 2010/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 02010: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4290 - val_loss: 1.4849\n",
      "Epoch 2011/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 02011: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4296 - val_loss: 1.4818\n",
      "Epoch 2012/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 02012: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4276 - val_loss: 1.4819\n",
      "Epoch 2013/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 02013: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4263 - val_loss: 1.4841\n",
      "Epoch 2014/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02014: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4838\n",
      "Epoch 2015/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 02015: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4864\n",
      "Epoch 2016/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4266\n",
      "Epoch 02016: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4266 - val_loss: 1.4839\n",
      "Epoch 2017/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 02017: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4267 - val_loss: 1.4816\n",
      "Epoch 2018/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02018: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4878\n",
      "Epoch 2019/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 02019: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4248 - val_loss: 1.4817\n",
      "Epoch 2020/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02020: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4238 - val_loss: 1.4817\n",
      "Epoch 2021/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 02021: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4268 - val_loss: 1.4856\n",
      "Epoch 2022/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 02022: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4268 - val_loss: 1.4838\n",
      "Epoch 2023/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4242\n",
      "Epoch 02023: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4242 - val_loss: 1.4860\n",
      "Epoch 2024/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4254\n",
      "Epoch 02024: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4254 - val_loss: 1.4830\n",
      "Epoch 2025/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02025: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4255 - val_loss: 1.4838\n",
      "Epoch 2026/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 02026: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4271 - val_loss: 1.4824\n",
      "Epoch 2027/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02027: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4831\n",
      "Epoch 2028/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02028: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4815\n",
      "Epoch 2029/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02029: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4236 - val_loss: 1.4834\n",
      "Epoch 2030/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02030: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4238 - val_loss: 1.4835\n",
      "Epoch 2031/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02031: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4840\n",
      "Epoch 2032/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02032: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4261 - val_loss: 1.4829\n",
      "Epoch 2033/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02033: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4261 - val_loss: 1.4815\n",
      "Epoch 2034/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02034: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4818\n",
      "Epoch 2035/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02035: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4840\n",
      "Epoch 2036/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 02036: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4275 - val_loss: 1.4896\n",
      "Epoch 2037/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 02037: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4296 - val_loss: 1.4861\n",
      "Epoch 2038/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 02038: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4289 - val_loss: 1.4802\n",
      "Epoch 2039/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02039: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4251 - val_loss: 1.4877\n",
      "Epoch 2040/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 02040: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4257 - val_loss: 1.4839\n",
      "Epoch 2041/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4242\n",
      "Epoch 02041: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4242 - val_loss: 1.4837\n",
      "Epoch 2042/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 02042: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4286 - val_loss: 1.4846\n",
      "Epoch 2043/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 02043: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4295 - val_loss: 1.4815\n",
      "Epoch 2044/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 02044: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4289 - val_loss: 1.4829\n",
      "Epoch 2045/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 02045: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4289 - val_loss: 1.4862\n",
      "Epoch 2046/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 02046: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4278 - val_loss: 1.4841\n",
      "Epoch 2047/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 02047: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4260 - val_loss: 1.4820\n",
      "Epoch 2048/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4242\n",
      "Epoch 02048: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4242 - val_loss: 1.4847\n",
      "Epoch 2049/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02049: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4256 - val_loss: 1.4842\n",
      "Epoch 2050/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 02050: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4249 - val_loss: 1.4845\n",
      "Epoch 2051/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02051: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4251 - val_loss: 1.4850\n",
      "Epoch 2052/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 02052: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4286 - val_loss: 1.4824\n",
      "Epoch 2053/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02053: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4825\n",
      "Epoch 2054/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02054: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4266 - val_loss: 1.4857\n",
      "Epoch 2055/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02055: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4261 - val_loss: 1.4853\n",
      "Epoch 2056/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 02056: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4260 - val_loss: 1.4821\n",
      "Epoch 2057/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 02057: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4346 - val_loss: 1.4840\n",
      "Epoch 2058/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 02058: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4307 - val_loss: 1.4849\n",
      "Epoch 2059/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 02059: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4279 - val_loss: 1.4834\n",
      "Epoch 2060/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 02060: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4299 - val_loss: 1.4821\n",
      "Epoch 2061/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 02061: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4300 - val_loss: 1.4842\n",
      "Epoch 2062/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 02062: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4293 - val_loss: 1.4857\n",
      "Epoch 2063/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 02063: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4350 - val_loss: 1.4863\n",
      "Epoch 2064/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 02064: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4318 - val_loss: 1.4858\n",
      "Epoch 2065/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 02065: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4287 - val_loss: 1.4831\n",
      "Epoch 2066/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4266\n",
      "Epoch 02066: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4266 - val_loss: 1.4803\n",
      "Epoch 2067/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 02067: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4866\n",
      "Epoch 2068/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 02068: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4290 - val_loss: 1.4802\n",
      "Epoch 2069/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4237\n",
      "Epoch 02069: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4237 - val_loss: 1.4811\n",
      "Epoch 2070/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4231\n",
      "Epoch 02070: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4231 - val_loss: 1.4829\n",
      "Epoch 2071/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02071: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4241 - val_loss: 1.4843\n",
      "Epoch 2072/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02072: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4238 - val_loss: 1.4832\n",
      "Epoch 2073/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4231\n",
      "Epoch 02073: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4231 - val_loss: 1.4830\n",
      "Epoch 2074/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02074: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4235 - val_loss: 1.4795\n",
      "Epoch 2075/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02075: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4241 - val_loss: 1.4886\n",
      "Epoch 2076/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 02076: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4276 - val_loss: 1.4845\n",
      "Epoch 2077/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 02077: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4277 - val_loss: 1.4844\n",
      "Epoch 2078/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02078: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4258 - val_loss: 1.4879\n",
      "Epoch 2079/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 02079: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4264 - val_loss: 1.4823\n",
      "Epoch 2080/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02080: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4234 - val_loss: 1.4814\n",
      "Epoch 2081/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02081: loss did not improve from 1.42203\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4821\n",
      "Epoch 2082/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02082: loss improved from 1.42203 to 1.42162, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4216 - val_loss: 1.4816\n",
      "Epoch 2083/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02083: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4859\n",
      "Epoch 2084/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02084: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4251 - val_loss: 1.4825\n",
      "Epoch 2085/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 02085: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4248 - val_loss: 1.4840\n",
      "Epoch 2086/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02086: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4238 - val_loss: 1.4876\n",
      "Epoch 2087/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02087: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4858\n",
      "Epoch 2088/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02088: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4827\n",
      "Epoch 2089/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02089: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4240 - val_loss: 1.4824\n",
      "Epoch 2090/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4231\n",
      "Epoch 02090: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4231 - val_loss: 1.4841\n",
      "Epoch 2091/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02091: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4229 - val_loss: 1.4862\n",
      "Epoch 2092/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02092: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4228 - val_loss: 1.4830\n",
      "Epoch 2093/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02093: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4221 - val_loss: 1.4839\n",
      "Epoch 2094/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02094: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4226 - val_loss: 1.4868\n",
      "Epoch 2095/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02095: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4244 - val_loss: 1.4837\n",
      "Epoch 2096/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02096: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4247 - val_loss: 1.4879\n",
      "Epoch 2097/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 02097: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4263 - val_loss: 1.4889\n",
      "Epoch 2098/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02098: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4246 - val_loss: 1.4841\n",
      "Epoch 2099/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02099: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4864\n",
      "Epoch 2100/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02100: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4235 - val_loss: 1.4855\n",
      "Epoch 2101/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02101: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4854\n",
      "Epoch 2102/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02102: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4836\n",
      "Epoch 2103/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02103: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4244 - val_loss: 1.4831\n",
      "Epoch 2104/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02104: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4251 - val_loss: 1.4833\n",
      "Epoch 2105/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4237\n",
      "Epoch 02105: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4237 - val_loss: 1.4837\n",
      "Epoch 2106/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02106: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4229 - val_loss: 1.4841\n",
      "Epoch 2107/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02107: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4220 - val_loss: 1.4865\n",
      "Epoch 2108/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02108: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4241 - val_loss: 1.4819\n",
      "Epoch 2109/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02109: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4228 - val_loss: 1.4831\n",
      "Epoch 2110/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 02110: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4249 - val_loss: 1.4817\n",
      "Epoch 2111/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02111: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4233 - val_loss: 1.4828\n",
      "Epoch 2112/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02112: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4836\n",
      "Epoch 2113/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02113: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4234 - val_loss: 1.4837\n",
      "Epoch 2114/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 02114: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4264 - val_loss: 1.4830\n",
      "Epoch 2115/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4282\n",
      "Epoch 02115: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4282 - val_loss: 1.4845\n",
      "Epoch 2116/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 02116: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4828\n",
      "Epoch 2117/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02117: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4259 - val_loss: 1.4825\n",
      "Epoch 2118/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4262\n",
      "Epoch 02118: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4262 - val_loss: 1.4826\n",
      "Epoch 2119/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02119: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4251 - val_loss: 1.4831\n",
      "Epoch 2120/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 02120: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4252 - val_loss: 1.4828\n",
      "Epoch 2121/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4237\n",
      "Epoch 02121: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4237 - val_loss: 1.4833\n",
      "Epoch 2122/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02122: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4836\n",
      "Epoch 2123/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02123: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4229 - val_loss: 1.4849\n",
      "Epoch 2124/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02124: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4858\n",
      "Epoch 2125/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02125: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4831\n",
      "Epoch 2126/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4271\n",
      "Epoch 02126: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4258 - val_loss: 1.4847\n",
      "Epoch 2127/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02127: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4259 - val_loss: 1.4873\n",
      "Epoch 2128/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 02128: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4263 - val_loss: 1.4885\n",
      "Epoch 2129/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4270\n",
      "Epoch 02129: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4270 - val_loss: 1.4823\n",
      "Epoch 2130/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02130: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4235 - val_loss: 1.4846\n",
      "Epoch 2131/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02131: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4240 - val_loss: 1.4817\n",
      "Epoch 2132/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02132: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4849\n",
      "Epoch 2133/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02133: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4246 - val_loss: 1.4839\n",
      "Epoch 2134/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02134: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4236 - val_loss: 1.4844\n",
      "Epoch 2135/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02135: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4222 - val_loss: 1.4857\n",
      "Epoch 2136/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02136: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4228 - val_loss: 1.4820\n",
      "Epoch 2137/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02137: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4233 - val_loss: 1.4860\n",
      "Epoch 2138/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02138: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4241 - val_loss: 1.4832\n",
      "Epoch 2139/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02139: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4244 - val_loss: 1.4820\n",
      "Epoch 2140/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4232\n",
      "Epoch 02140: loss did not improve from 1.42162\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4232 - val_loss: 1.4849\n",
      "Epoch 2141/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02141: loss improved from 1.42162 to 1.42097, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4210 - val_loss: 1.4846\n",
      "Epoch 2142/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4110\n",
      "Epoch 02142: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4212 - val_loss: 1.4839\n",
      "Epoch 2143/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02143: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4233 - val_loss: 1.4831\n",
      "Epoch 2144/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4230\n",
      "Epoch 02144: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4230 - val_loss: 1.4853\n",
      "Epoch 2145/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02145: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4225 - val_loss: 1.4859\n",
      "Epoch 2146/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02146: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4240 - val_loss: 1.4831\n",
      "Epoch 2147/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 02147: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4249 - val_loss: 1.4832\n",
      "Epoch 2148/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02148: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4852\n",
      "Epoch 2149/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02149: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4244 - val_loss: 1.4842\n",
      "Epoch 2150/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02150: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4236 - val_loss: 1.4862\n",
      "Epoch 2151/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02151: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4229 - val_loss: 1.4881\n",
      "Epoch 2152/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 02152: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4248 - val_loss: 1.4904\n",
      "Epoch 2153/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 02153: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4315 - val_loss: 1.4923\n",
      "Epoch 2154/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 02154: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4286 - val_loss: 1.4917\n",
      "Epoch 2155/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 02155: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4805\n",
      "Epoch 2156/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02156: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4226 - val_loss: 1.4854\n",
      "Epoch 2157/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02157: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4241 - val_loss: 1.4846\n",
      "Epoch 2158/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4232\n",
      "Epoch 02158: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4232 - val_loss: 1.4842\n",
      "Epoch 2159/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02159: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4213 - val_loss: 1.4859\n",
      "Epoch 2160/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02160: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4220 - val_loss: 1.4835\n",
      "Epoch 2161/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 02161: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4269 - val_loss: 1.4852\n",
      "Epoch 2162/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02162: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4879\n",
      "Epoch 2163/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02163: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4866\n",
      "Epoch 2164/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02164: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4816\n",
      "Epoch 2165/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02165: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4259 - val_loss: 1.4858\n",
      "Epoch 2166/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 02166: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4265 - val_loss: 1.4932\n",
      "Epoch 2167/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4270\n",
      "Epoch 02167: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4270 - val_loss: 1.4828\n",
      "Epoch 2168/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4237\n",
      "Epoch 02168: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4237 - val_loss: 1.4833\n",
      "Epoch 2169/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02169: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4244 - val_loss: 1.4820\n",
      "Epoch 2170/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4111\n",
      "Epoch 02170: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4221 - val_loss: 1.4844\n",
      "Epoch 2171/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02171: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4258 - val_loss: 1.4839\n",
      "Epoch 2172/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 02172: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4248 - val_loss: 1.4853\n",
      "Epoch 2173/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4237\n",
      "Epoch 02173: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4237 - val_loss: 1.4844\n",
      "Epoch 2174/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02174: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4225 - val_loss: 1.4862\n",
      "Epoch 2175/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4313\n",
      "Epoch 02175: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4313 - val_loss: 1.4869\n",
      "Epoch 2176/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 02176: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4289 - val_loss: 1.4817\n",
      "Epoch 2177/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02177: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4256 - val_loss: 1.4830\n",
      "Epoch 2178/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02178: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4259 - val_loss: 1.4906\n",
      "Epoch 2179/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02179: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4261 - val_loss: 1.4845\n",
      "Epoch 2180/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02180: loss did not improve from 1.42097\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4222 - val_loss: 1.4826\n",
      "Epoch 2181/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4206\n",
      "Epoch 02181: loss improved from 1.42097 to 1.42061, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4206 - val_loss: 1.4847\n",
      "Epoch 2182/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02182: loss improved from 1.42061 to 1.42055, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4205 - val_loss: 1.4826\n",
      "Epoch 2183/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02183: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4208 - val_loss: 1.4869\n",
      "Epoch 2184/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4227\n",
      "Epoch 02184: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4227 - val_loss: 1.4835\n",
      "Epoch 2185/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02185: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4234 - val_loss: 1.4887\n",
      "Epoch 2186/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02186: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4261 - val_loss: 1.4860\n",
      "Epoch 2187/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 02187: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4248 - val_loss: 1.4861\n",
      "Epoch 2188/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 02188: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4335 - val_loss: 1.4886\n",
      "Epoch 2189/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4322\n",
      "Epoch 02189: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4322 - val_loss: 1.4876\n",
      "Epoch 2190/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 02190: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4273 - val_loss: 1.4840\n",
      "Epoch 2191/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4232\n",
      "Epoch 02191: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4232 - val_loss: 1.4831\n",
      "Epoch 2192/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02192: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4222 - val_loss: 1.4847\n",
      "Epoch 2193/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02193: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4221 - val_loss: 1.4852\n",
      "Epoch 2194/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02194: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4216 - val_loss: 1.4843\n",
      "Epoch 2195/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4242\n",
      "Epoch 02195: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4222 - val_loss: 1.4906\n",
      "Epoch 2196/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02196: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4256 - val_loss: 1.4889\n",
      "Epoch 2197/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4254\n",
      "Epoch 02197: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4254 - val_loss: 1.4897\n",
      "Epoch 2198/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02198: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4258 - val_loss: 1.4859\n",
      "Epoch 2199/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 02199: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4260 - val_loss: 1.4883\n",
      "Epoch 2200/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02200: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4261 - val_loss: 1.4866\n",
      "Epoch 2201/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02201: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4246 - val_loss: 1.4877\n",
      "Epoch 2202/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02202: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4837\n",
      "Epoch 2203/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02203: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4255 - val_loss: 1.4852\n",
      "Epoch 2204/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02204: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4218 - val_loss: 1.4865\n",
      "Epoch 2205/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02205: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4217 - val_loss: 1.4827\n",
      "Epoch 2206/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02206: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4853\n",
      "Epoch 2207/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02207: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4833\n",
      "Epoch 2208/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4232\n",
      "Epoch 02208: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4232 - val_loss: 1.4914\n",
      "Epoch 2209/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02209: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4261 - val_loss: 1.4913\n",
      "Epoch 2210/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 02210: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4252 - val_loss: 1.4866\n",
      "Epoch 2211/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 02211: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4272 - val_loss: 1.4925\n",
      "Epoch 2212/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 02212: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4294 - val_loss: 1.4872\n",
      "Epoch 2213/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02213: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4238 - val_loss: 1.4895\n",
      "Epoch 2214/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02214: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4236 - val_loss: 1.4853\n",
      "Epoch 2215/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02215: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4217 - val_loss: 1.4860\n",
      "Epoch 2216/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4227\n",
      "Epoch 02216: loss did not improve from 1.42055\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4227 - val_loss: 1.4860\n",
      "Epoch 2217/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02217: loss improved from 1.42055 to 1.42046, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4205 - val_loss: 1.4870\n",
      "Epoch 2218/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02218: loss did not improve from 1.42046\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4221 - val_loss: 1.4862\n",
      "Epoch 2219/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02219: loss did not improve from 1.42046\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4235 - val_loss: 1.4905\n",
      "Epoch 2220/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02220: loss did not improve from 1.42046\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4226 - val_loss: 1.4843\n",
      "Epoch 2221/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4206\n",
      "Epoch 02221: loss did not improve from 1.42046\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4206 - val_loss: 1.4831\n",
      "Epoch 2222/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02222: loss improved from 1.42046 to 1.41992, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4199 - val_loss: 1.4838\n",
      "Epoch 2223/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02223: loss improved from 1.41992 to 1.41916, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4192 - val_loss: 1.4868\n",
      "Epoch 2224/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02224: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4223 - val_loss: 1.4852\n",
      "Epoch 2225/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02225: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4208 - val_loss: 1.4841\n",
      "Epoch 2226/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02226: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4196 - val_loss: 1.4824\n",
      "Epoch 2227/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02227: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4201 - val_loss: 1.4854\n",
      "Epoch 2228/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02228: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4211 - val_loss: 1.4897\n",
      "Epoch 2229/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02229: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4221 - val_loss: 1.4875\n",
      "Epoch 2230/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02230: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4240 - val_loss: 1.4842\n",
      "Epoch 2231/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02231: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4212 - val_loss: 1.4838\n",
      "Epoch 2232/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02232: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4198 - val_loss: 1.4868\n",
      "Epoch 2233/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4254\n",
      "Epoch 02233: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4254 - val_loss: 1.4863\n",
      "Epoch 2234/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 02234: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4252 - val_loss: 1.4890\n",
      "Epoch 2235/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02235: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4239 - val_loss: 1.4839\n",
      "Epoch 2236/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02236: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4226 - val_loss: 1.4853\n",
      "Epoch 2237/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02237: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4213 - val_loss: 1.4898\n",
      "Epoch 2238/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02238: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4222 - val_loss: 1.4883\n",
      "Epoch 2239/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02239: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4224 - val_loss: 1.4873\n",
      "Epoch 2240/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02240: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4863\n",
      "Epoch 2241/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 02241: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4265 - val_loss: 1.4873\n",
      "Epoch 2242/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02242: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4246 - val_loss: 1.4886\n",
      "Epoch 2243/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 02243: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4264 - val_loss: 1.4856\n",
      "Epoch 2244/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02244: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4259 - val_loss: 1.4841\n",
      "Epoch 2245/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 02245: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4287 - val_loss: 1.4832\n",
      "Epoch 2246/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 02246: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4248 - val_loss: 1.4897\n",
      "Epoch 2247/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4231\n",
      "Epoch 02247: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4231 - val_loss: 1.4838\n",
      "Epoch 2248/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02248: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4226 - val_loss: 1.4891\n",
      "Epoch 2249/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02249: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4224 - val_loss: 1.4809\n",
      "Epoch 2250/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02250: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4213 - val_loss: 1.4851\n",
      "Epoch 2251/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02251: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4216 - val_loss: 1.4905\n",
      "Epoch 2252/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 02252: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4265 - val_loss: 1.4896\n",
      "Epoch 2253/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02253: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4244 - val_loss: 1.4858\n",
      "Epoch 2254/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02254: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4228 - val_loss: 1.4882\n",
      "Epoch 2255/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02255: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4889\n",
      "Epoch 2256/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02256: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4261 - val_loss: 1.4850\n",
      "Epoch 2257/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02257: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4216 - val_loss: 1.4866\n",
      "Epoch 2258/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02258: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4838\n",
      "Epoch 2259/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02259: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4246 - val_loss: 1.4876\n",
      "Epoch 2260/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02260: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4255 - val_loss: 1.4832\n",
      "Epoch 2261/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02261: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4217 - val_loss: 1.4831\n",
      "Epoch 2262/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02262: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4839\n",
      "Epoch 2263/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02263: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4207 - val_loss: 1.4840\n",
      "Epoch 2264/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 02264: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4260 - val_loss: 1.4841\n",
      "Epoch 2265/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02265: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4246 - val_loss: 1.4852\n",
      "Epoch 2266/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02266: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4239 - val_loss: 1.4880\n",
      "Epoch 2267/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02267: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4234 - val_loss: 1.4868\n",
      "Epoch 2268/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02268: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4877\n",
      "Epoch 2269/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02269: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4240 - val_loss: 1.4861\n",
      "Epoch 2270/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02270: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4233 - val_loss: 1.4863\n",
      "Epoch 2271/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02271: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4892\n",
      "Epoch 2272/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02272: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4823\n",
      "Epoch 2273/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02273: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4201 - val_loss: 1.4875\n",
      "Epoch 2274/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02274: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4210 - val_loss: 1.4861\n",
      "Epoch 2275/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02275: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4199 - val_loss: 1.4840\n",
      "Epoch 2276/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02276: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4217 - val_loss: 1.4855\n",
      "Epoch 2277/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02277: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4871\n",
      "Epoch 2278/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02278: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4886\n",
      "Epoch 2279/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02279: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4245 - val_loss: 1.4883\n",
      "Epoch 2280/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02280: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4851\n",
      "Epoch 2281/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02281: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4228 - val_loss: 1.4839\n",
      "Epoch 2282/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02282: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4233 - val_loss: 1.4851\n",
      "Epoch 2283/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02283: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4251 - val_loss: 1.4847\n",
      "Epoch 2284/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02284: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4211 - val_loss: 1.4874\n",
      "Epoch 2285/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02285: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4224 - val_loss: 1.4851\n",
      "Epoch 2286/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02286: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4202 - val_loss: 1.4890\n",
      "Epoch 2287/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02287: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4235 - val_loss: 1.4848\n",
      "Epoch 2288/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4206\n",
      "Epoch 02288: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4206 - val_loss: 1.4875\n",
      "Epoch 2289/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02289: loss did not improve from 1.41916\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4839\n",
      "Epoch 2290/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02290: loss improved from 1.41916 to 1.41895, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4189 - val_loss: 1.4851\n",
      "Epoch 2291/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02291: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4875\n",
      "Epoch 2292/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02292: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4217 - val_loss: 1.4843\n",
      "Epoch 2293/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02293: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4222 - val_loss: 1.4837\n",
      "Epoch 2294/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02294: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4210 - val_loss: 1.4829\n",
      "Epoch 2295/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02295: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4859\n",
      "Epoch 2296/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02296: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4218 - val_loss: 1.4854\n",
      "Epoch 2297/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02297: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4225 - val_loss: 1.4824\n",
      "Epoch 2298/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02298: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4200 - val_loss: 1.4830\n",
      "Epoch 2299/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02299: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4213 - val_loss: 1.4853\n",
      "Epoch 2300/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4313\n",
      "Epoch 02300: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4313 - val_loss: 1.4848\n",
      "Epoch 2301/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 02301: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4291 - val_loss: 1.4866\n",
      "Epoch 2302/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 02302: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4275 - val_loss: 1.4839\n",
      "Epoch 2303/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02303: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4259 - val_loss: 1.4873\n",
      "Epoch 2304/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02304: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4890\n",
      "Epoch 2305/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02305: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4238 - val_loss: 1.4897\n",
      "Epoch 2306/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02306: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4240 - val_loss: 1.4844\n",
      "Epoch 2307/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02307: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4228 - val_loss: 1.4890\n",
      "Epoch 2308/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02308: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4216 - val_loss: 1.4855\n",
      "Epoch 2309/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02309: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4856\n",
      "Epoch 2310/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02310: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4843\n",
      "Epoch 2311/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02311: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4208 - val_loss: 1.4836\n",
      "Epoch 2312/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02312: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4199 - val_loss: 1.4872\n",
      "Epoch 2313/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02313: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4205 - val_loss: 1.4863\n",
      "Epoch 2314/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02314: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4207 - val_loss: 1.4879\n",
      "Epoch 2315/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02315: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4837\n",
      "Epoch 2316/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02316: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4201 - val_loss: 1.4892\n",
      "Epoch 2317/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02317: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4217 - val_loss: 1.4913\n",
      "Epoch 2318/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02318: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4205 - val_loss: 1.4851\n",
      "Epoch 2319/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02319: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4205 - val_loss: 1.4882\n",
      "Epoch 2320/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02320: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4235 - val_loss: 1.4850\n",
      "Epoch 2321/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02321: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4898\n",
      "Epoch 2322/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02322: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4238 - val_loss: 1.4879\n",
      "Epoch 2323/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02323: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4198 - val_loss: 1.4865\n",
      "Epoch 2324/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02324: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4858\n",
      "Epoch 2325/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02325: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4203 - val_loss: 1.4838\n",
      "Epoch 2326/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02326: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4195 - val_loss: 1.4913\n",
      "Epoch 2327/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4230\n",
      "Epoch 02327: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4230 - val_loss: 1.4863\n",
      "Epoch 2328/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02328: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4226 - val_loss: 1.4875\n",
      "Epoch 2329/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4232\n",
      "Epoch 02329: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4232 - val_loss: 1.4881\n",
      "Epoch 2330/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02330: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4880\n",
      "Epoch 2331/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02331: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4209 - val_loss: 1.4902\n",
      "Epoch 2332/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02332: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4223 - val_loss: 1.4924\n",
      "Epoch 2333/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02333: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4261 - val_loss: 1.4905\n",
      "Epoch 2334/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 02334: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4273 - val_loss: 1.4871\n",
      "Epoch 2335/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02335: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4251 - val_loss: 1.4858\n",
      "Epoch 2336/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4254\n",
      "Epoch 02336: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4254 - val_loss: 1.4879\n",
      "Epoch 2337/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 02337: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4249 - val_loss: 1.4936\n",
      "Epoch 2338/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 02338: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4269 - val_loss: 1.4898\n",
      "Epoch 2339/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02339: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4229 - val_loss: 1.4866\n",
      "Epoch 2340/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02340: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4212 - val_loss: 1.4868\n",
      "Epoch 2341/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02341: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4209 - val_loss: 1.4868\n",
      "Epoch 2342/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02342: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4203 - val_loss: 1.4861\n",
      "Epoch 2343/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02343: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4203 - val_loss: 1.4864\n",
      "Epoch 2344/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02344: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4849\n",
      "Epoch 2345/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02345: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4201 - val_loss: 1.4833\n",
      "Epoch 2346/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02346: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4213 - val_loss: 1.4858\n",
      "Epoch 2347/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02347: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4202 - val_loss: 1.4864\n",
      "Epoch 2348/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02348: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4202 - val_loss: 1.4844\n",
      "Epoch 2349/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02349: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4203 - val_loss: 1.4865\n",
      "Epoch 2350/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4206\n",
      "Epoch 02350: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4206 - val_loss: 1.4859\n",
      "Epoch 2351/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02351: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4887\n",
      "Epoch 2352/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02352: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4233 - val_loss: 1.4942\n",
      "Epoch 2353/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 02353: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4879\n",
      "Epoch 2354/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 02354: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4285 - val_loss: 1.4822\n",
      "Epoch 2355/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02355: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4250 - val_loss: 1.4880\n",
      "Epoch 2356/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02356: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4251 - val_loss: 1.4905\n",
      "Epoch 2357/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02357: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4234 - val_loss: 1.4904\n",
      "Epoch 2358/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02358: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4927\n",
      "Epoch 2359/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02359: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4872\n",
      "Epoch 2360/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02360: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4225 - val_loss: 1.4896\n",
      "Epoch 2361/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02361: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4258 - val_loss: 1.4879\n",
      "Epoch 2362/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 02362: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4249 - val_loss: 1.4860\n",
      "Epoch 2363/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02363: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4226 - val_loss: 1.4880\n",
      "Epoch 2364/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02364: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4882\n",
      "Epoch 2365/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02365: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4887\n",
      "Epoch 2366/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02366: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4244 - val_loss: 1.4973\n",
      "Epoch 2367/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 02367: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4302 - val_loss: 1.4865\n",
      "Epoch 2368/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 02368: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4267 - val_loss: 1.4841\n",
      "Epoch 2369/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02369: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4213 - val_loss: 1.4851\n",
      "Epoch 2370/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02370: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4210 - val_loss: 1.4858\n",
      "Epoch 2371/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4227\n",
      "Epoch 02371: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4227 - val_loss: 1.4874\n",
      "Epoch 2372/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02372: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4205 - val_loss: 1.4845\n",
      "Epoch 2373/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 02373: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4274 - val_loss: 1.4847\n",
      "Epoch 2374/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4280\n",
      "Epoch 02374: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4280 - val_loss: 1.4884\n",
      "Epoch 2375/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02375: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4240 - val_loss: 1.4881\n",
      "Epoch 2376/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02376: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4246 - val_loss: 1.4860\n",
      "Epoch 2377/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02377: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4216 - val_loss: 1.4849\n",
      "Epoch 2378/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02378: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4207 - val_loss: 1.4870\n",
      "Epoch 2379/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02379: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4857\n",
      "Epoch 2380/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02380: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4208 - val_loss: 1.4915\n",
      "Epoch 2381/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02381: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4259 - val_loss: 1.4889\n",
      "Epoch 2382/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 02382: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4869\n",
      "Epoch 2383/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02383: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4870\n",
      "Epoch 2384/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02384: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4870\n",
      "Epoch 2385/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02385: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4212 - val_loss: 1.4872\n",
      "Epoch 2386/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02386: loss did not improve from 1.41895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4194 - val_loss: 1.4872\n",
      "Epoch 2387/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4053\n",
      "Epoch 02387: loss improved from 1.41895 to 1.41889, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4189 - val_loss: 1.4855\n",
      "Epoch 2388/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02388: loss did not improve from 1.41889\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4189 - val_loss: 1.4837\n",
      "Epoch 2389/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02389: loss improved from 1.41889 to 1.41858, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4186 - val_loss: 1.4848\n",
      "Epoch 2390/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4174\n",
      "Epoch 02390: loss improved from 1.41858 to 1.41737, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4174 - val_loss: 1.4857\n",
      "Epoch 2391/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02391: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4197 - val_loss: 1.4872\n",
      "Epoch 2392/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02392: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4210 - val_loss: 1.4851\n",
      "Epoch 2393/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02393: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4235 - val_loss: 1.4855\n",
      "Epoch 2394/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4232\n",
      "Epoch 02394: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4232 - val_loss: 1.4886\n",
      "Epoch 2395/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4018\n",
      "Epoch 02395: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4235 - val_loss: 1.4896\n",
      "Epoch 2396/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02396: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4217 - val_loss: 1.4895\n",
      "Epoch 2397/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02397: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4863\n",
      "Epoch 2398/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02398: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4198 - val_loss: 1.4866\n",
      "Epoch 2399/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02399: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4200 - val_loss: 1.4862\n",
      "Epoch 2400/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02400: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - val_loss: 1.4891\n",
      "Epoch 2401/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02401: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4216 - val_loss: 1.4889\n",
      "Epoch 2402/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4232\n",
      "Epoch 02402: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4232 - val_loss: 1.4886\n",
      "Epoch 2403/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02403: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4211 - val_loss: 1.4898\n",
      "Epoch 2404/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02404: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4209 - val_loss: 1.4872\n",
      "Epoch 2405/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02405: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4192 - val_loss: 1.4878\n",
      "Epoch 2406/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4242\n",
      "Epoch 02406: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4186 - val_loss: 1.4872\n",
      "Epoch 2407/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4294\n",
      "Epoch 02407: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4211 - val_loss: 1.4873\n",
      "Epoch 2408/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02408: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4220 - val_loss: 1.4909\n",
      "Epoch 2409/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02409: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4221 - val_loss: 1.4880\n",
      "Epoch 2410/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02410: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4208 - val_loss: 1.4871\n",
      "Epoch 2411/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02411: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4214 - val_loss: 1.4881\n",
      "Epoch 2412/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02412: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4886\n",
      "Epoch 2413/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02413: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4225 - val_loss: 1.4878\n",
      "Epoch 2414/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02414: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4212 - val_loss: 1.4858\n",
      "Epoch 2415/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02415: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4198 - val_loss: 1.4858\n",
      "Epoch 2416/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02416: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4213 - val_loss: 1.4900\n",
      "Epoch 2417/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02417: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4203 - val_loss: 1.4881\n",
      "Epoch 2418/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02418: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4198 - val_loss: 1.4878\n",
      "Epoch 2419/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02419: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4212 - val_loss: 1.4896\n",
      "Epoch 2420/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02420: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4210 - val_loss: 1.4920\n",
      "Epoch 2421/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02421: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4847\n",
      "Epoch 2422/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02422: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4196 - val_loss: 1.4860\n",
      "Epoch 2423/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4206\n",
      "Epoch 02423: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4206 - val_loss: 1.4870\n",
      "Epoch 2424/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02424: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4209 - val_loss: 1.4887\n",
      "Epoch 2425/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02425: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4209 - val_loss: 1.4864\n",
      "Epoch 2426/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02426: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4208 - val_loss: 1.4880\n",
      "Epoch 2427/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02427: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4224 - val_loss: 1.4847\n",
      "Epoch 2428/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02428: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4221 - val_loss: 1.4853\n",
      "Epoch 2429/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02429: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4866\n",
      "Epoch 2430/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02430: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4876\n",
      "Epoch 2431/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02431: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - val_loss: 1.4890\n",
      "Epoch 2432/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02432: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4881\n",
      "Epoch 2433/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02433: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4208 - val_loss: 1.4851\n",
      "Epoch 2434/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02434: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4181 - val_loss: 1.4870\n",
      "Epoch 2435/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02435: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - val_loss: 1.4911\n",
      "Epoch 2436/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 02436: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4272 - val_loss: 1.4933\n",
      "Epoch 2437/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02437: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4875\n",
      "Epoch 2438/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02438: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4918\n",
      "Epoch 2439/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02439: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4212 - val_loss: 1.4874\n",
      "Epoch 2440/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02440: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4907\n",
      "Epoch 2441/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02441: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4212 - val_loss: 1.4881\n",
      "Epoch 2442/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02442: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4201 - val_loss: 1.4904\n",
      "Epoch 2443/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02443: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4892\n",
      "Epoch 2444/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02444: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4934\n",
      "Epoch 2445/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02445: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4199 - val_loss: 1.4868\n",
      "Epoch 2446/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02446: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4188 - val_loss: 1.4858\n",
      "Epoch 2447/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02447: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4191 - val_loss: 1.4864\n",
      "Epoch 2448/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02448: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4188 - val_loss: 1.4870\n",
      "Epoch 2449/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02449: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4196 - val_loss: 1.4848\n",
      "Epoch 2450/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02450: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4873\n",
      "Epoch 2451/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02451: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4188 - val_loss: 1.4883\n",
      "Epoch 2452/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02452: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4189 - val_loss: 1.4857\n",
      "Epoch 2453/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02453: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4186 - val_loss: 1.4863\n",
      "Epoch 2454/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02454: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4201 - val_loss: 1.4872\n",
      "Epoch 2455/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4187\n",
      "Epoch 02455: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4187 - val_loss: 1.4867\n",
      "Epoch 2456/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02456: loss did not improve from 1.41737\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4183 - val_loss: 1.4860\n",
      "Epoch 2457/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02457: loss improved from 1.41737 to 1.41733, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4173 - val_loss: 1.4848\n",
      "Epoch 2458/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02458: loss improved from 1.41733 to 1.41668, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4167 - val_loss: 1.4869\n",
      "Epoch 2459/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02459: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4183 - val_loss: 1.4873\n",
      "Epoch 2460/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02460: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4909\n",
      "Epoch 2461/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02461: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4212 - val_loss: 1.4880\n",
      "Epoch 2462/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02462: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4220 - val_loss: 1.4863\n",
      "Epoch 2463/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02463: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4865\n",
      "Epoch 2464/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02464: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4198 - val_loss: 1.4856\n",
      "Epoch 2465/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02465: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4198 - val_loss: 1.4841\n",
      "Epoch 2466/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02466: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4829\n",
      "Epoch 2467/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02467: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4183 - val_loss: 1.4879\n",
      "Epoch 2468/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02468: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4178 - val_loss: 1.4897\n",
      "Epoch 2469/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02469: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4871\n",
      "Epoch 2470/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02470: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4212 - val_loss: 1.4918\n",
      "Epoch 2471/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4341\n",
      "Epoch 02471: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4259 - val_loss: 1.4927\n",
      "Epoch 2472/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02472: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4246 - val_loss: 1.4905\n",
      "Epoch 2473/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02473: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4189 - val_loss: 1.4891\n",
      "Epoch 2474/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02474: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4194 - val_loss: 1.4889\n",
      "Epoch 2475/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02475: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4199 - val_loss: 1.4882\n",
      "Epoch 2476/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02476: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - val_loss: 1.4870\n",
      "Epoch 2477/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02477: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4188 - val_loss: 1.4900\n",
      "Epoch 2478/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02478: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4182 - val_loss: 1.4887\n",
      "Epoch 2479/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02479: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4181 - val_loss: 1.4889\n",
      "Epoch 2480/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02480: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4862\n",
      "Epoch 2481/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02481: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4179 - val_loss: 1.4861\n",
      "Epoch 2482/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02482: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4893\n",
      "Epoch 2483/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02483: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4184 - val_loss: 1.4903\n",
      "Epoch 2484/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02484: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4886\n",
      "Epoch 2485/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02485: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4203 - val_loss: 1.4854\n",
      "Epoch 2486/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02486: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4199 - val_loss: 1.4932\n",
      "Epoch 2487/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02487: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4247 - val_loss: 1.4912\n",
      "Epoch 2488/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02488: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4222 - val_loss: 1.4955\n",
      "Epoch 2489/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 02489: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4297 - val_loss: 1.4853\n",
      "Epoch 2490/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 02490: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4845\n",
      "Epoch 2491/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02491: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4199 - val_loss: 1.4862\n",
      "Epoch 2492/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02492: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4860\n",
      "Epoch 2493/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02493: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4189 - val_loss: 1.4847\n",
      "Epoch 2494/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02494: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4878\n",
      "Epoch 2495/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02495: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4186 - val_loss: 1.4874\n",
      "Epoch 2496/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02496: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4885\n",
      "Epoch 2497/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02497: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4903\n",
      "Epoch 2498/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02498: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4853\n",
      "Epoch 2499/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02499: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4181 - val_loss: 1.4863\n",
      "Epoch 2500/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02500: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4836\n",
      "Epoch 2501/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02501: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4173 - val_loss: 1.4857\n",
      "Epoch 2502/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4185\n",
      "Epoch 02502: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4185 - val_loss: 1.4877\n",
      "Epoch 2503/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02503: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4205 - val_loss: 1.4894\n",
      "Epoch 2504/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02504: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4192 - val_loss: 1.4852\n",
      "Epoch 2505/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02505: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4204 - val_loss: 1.4897\n",
      "Epoch 2506/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02506: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4204 - val_loss: 1.4923\n",
      "Epoch 2507/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02507: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4203 - val_loss: 1.4848\n",
      "Epoch 2508/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 02508: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4865\n",
      "Epoch 2509/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4312\n",
      "Epoch 02509: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4312 - val_loss: 1.4849\n",
      "Epoch 2510/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 02510: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4277 - val_loss: 1.4853\n",
      "Epoch 2511/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02511: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4241 - val_loss: 1.4873\n",
      "Epoch 2512/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02512: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4897\n",
      "Epoch 2513/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02513: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4892\n",
      "Epoch 2514/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02514: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4215 - val_loss: 1.4848\n",
      "Epoch 2515/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02515: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4209 - val_loss: 1.4893\n",
      "Epoch 2516/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4089\n",
      "Epoch 02516: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4210 - val_loss: 1.4881\n",
      "Epoch 2517/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02517: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4200 - val_loss: 1.4883\n",
      "Epoch 2518/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02518: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4188 - val_loss: 1.4903\n",
      "Epoch 2519/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02519: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4192 - val_loss: 1.4882\n",
      "Epoch 2520/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02520: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4189 - val_loss: 1.4900\n",
      "Epoch 2521/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4185\n",
      "Epoch 02521: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4185 - val_loss: 1.4893\n",
      "Epoch 2522/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02522: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4178 - val_loss: 1.4863\n",
      "Epoch 2523/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02523: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4180 - val_loss: 1.4858\n",
      "Epoch 2524/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02524: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4182 - val_loss: 1.4897\n",
      "Epoch 2525/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02525: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4887\n",
      "Epoch 2526/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02526: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4881\n",
      "Epoch 2527/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02527: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4212 - val_loss: 1.4911\n",
      "Epoch 2528/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02528: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4929\n",
      "Epoch 2529/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02529: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4212 - val_loss: 1.4879\n",
      "Epoch 2530/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02530: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4198 - val_loss: 1.4875\n",
      "Epoch 2531/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4177\n",
      "Epoch 02531: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4177 - val_loss: 1.4891\n",
      "Epoch 2532/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02532: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4175 - val_loss: 1.4848\n",
      "Epoch 2533/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02533: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4194 - val_loss: 1.4893\n",
      "Epoch 2534/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02534: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4201 - val_loss: 1.4874\n",
      "Epoch 2535/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02535: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4181 - val_loss: 1.4862\n",
      "Epoch 2536/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02536: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4208 - val_loss: 1.4901\n",
      "Epoch 2537/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02537: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4210 - val_loss: 1.4896\n",
      "Epoch 2538/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02538: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4223 - val_loss: 1.4881\n",
      "Epoch 2539/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02539: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4937\n",
      "Epoch 2540/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02540: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4241 - val_loss: 1.4856\n",
      "Epoch 2541/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02541: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4201 - val_loss: 1.4869\n",
      "Epoch 2542/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02542: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4198 - val_loss: 1.4844\n",
      "Epoch 2543/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02543: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4191 - val_loss: 1.4879\n",
      "Epoch 2544/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02544: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4173 - val_loss: 1.4885\n",
      "Epoch 2545/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02545: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4191 - val_loss: 1.4880\n",
      "Epoch 2546/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02546: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4201 - val_loss: 1.4901\n",
      "Epoch 2547/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02547: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4196 - val_loss: 1.4870\n",
      "Epoch 2548/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02548: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4199 - val_loss: 1.4881\n",
      "Epoch 2549/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02549: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4200 - val_loss: 1.4909\n",
      "Epoch 2550/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02550: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - val_loss: 1.4897\n",
      "Epoch 2551/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02551: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4207 - val_loss: 1.4881\n",
      "Epoch 2552/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02552: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4182 - val_loss: 1.4899\n",
      "Epoch 2553/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02553: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4181 - val_loss: 1.4949\n",
      "Epoch 2554/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02554: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4180 - val_loss: 1.4896\n",
      "Epoch 2555/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02555: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4198 - val_loss: 1.4856\n",
      "Epoch 2556/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02556: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4191 - val_loss: 1.4867\n",
      "Epoch 2557/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02557: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4224 - val_loss: 1.4850\n",
      "Epoch 2558/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02558: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4209 - val_loss: 1.4870\n",
      "Epoch 2559/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02559: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4179 - val_loss: 1.4895\n",
      "Epoch 2560/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02560: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - val_loss: 1.4875\n",
      "Epoch 2561/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4177\n",
      "Epoch 02561: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4177 - val_loss: 1.4892\n",
      "Epoch 2562/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02562: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4935\n",
      "Epoch 2563/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02563: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4862\n",
      "Epoch 2564/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02564: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4866\n",
      "Epoch 2565/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02565: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4222 - val_loss: 1.4858\n",
      "Epoch 2566/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4187\n",
      "Epoch 02566: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4187 - val_loss: 1.4886\n",
      "Epoch 2567/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02567: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4167 - val_loss: 1.4892\n",
      "Epoch 2568/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02568: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4192 - val_loss: 1.4877\n",
      "Epoch 2569/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02569: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4193 - val_loss: 1.4921\n",
      "Epoch 2570/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02570: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4928\n",
      "Epoch 2571/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02571: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4884\n",
      "Epoch 2572/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02572: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4894\n",
      "Epoch 2573/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02573: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4866\n",
      "Epoch 2574/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02574: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4192 - val_loss: 1.4935\n",
      "Epoch 2575/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4206\n",
      "Epoch 02575: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4206 - val_loss: 1.4902\n",
      "Epoch 2576/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02576: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4873\n",
      "Epoch 2577/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02577: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4167 - val_loss: 1.4876\n",
      "Epoch 2578/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02578: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4172 - val_loss: 1.4848\n",
      "Epoch 2579/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02579: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4169 - val_loss: 1.4860\n",
      "Epoch 2580/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4174\n",
      "Epoch 02580: loss did not improve from 1.41668\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4174 - val_loss: 1.4849\n",
      "Epoch 2581/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4163\n",
      "Epoch 02581: loss improved from 1.41668 to 1.41632, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4163 - val_loss: 1.4946\n",
      "Epoch 2582/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02582: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4202 - val_loss: 1.4934\n",
      "Epoch 2583/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02583: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4200 - val_loss: 1.4910\n",
      "Epoch 2584/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02584: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4207 - val_loss: 1.4892\n",
      "Epoch 2585/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02585: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4223 - val_loss: 1.4869\n",
      "Epoch 2586/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02586: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4881\n",
      "Epoch 2587/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02587: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4178 - val_loss: 1.4873\n",
      "Epoch 2588/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02588: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4181 - val_loss: 1.4880\n",
      "Epoch 2589/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02589: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4951\n",
      "Epoch 2590/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02590: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4892\n",
      "Epoch 2591/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02591: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4200 - val_loss: 1.4891\n",
      "Epoch 2592/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02592: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4233 - val_loss: 1.4911\n",
      "Epoch 2593/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02593: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4217 - val_loss: 1.4895\n",
      "Epoch 2594/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02594: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4194 - val_loss: 1.4863\n",
      "Epoch 2595/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02595: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4180 - val_loss: 1.4892\n",
      "Epoch 2596/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02596: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4166 - val_loss: 1.4887\n",
      "Epoch 2597/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02597: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4203 - val_loss: 1.4874\n",
      "Epoch 2598/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02598: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4183 - val_loss: 1.4882\n",
      "Epoch 2599/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02599: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4181 - val_loss: 1.4909\n",
      "Epoch 2600/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02600: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4210 - val_loss: 1.4860\n",
      "Epoch 2601/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4185\n",
      "Epoch 02601: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4185 - val_loss: 1.4880\n",
      "Epoch 2602/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4177\n",
      "Epoch 02602: loss did not improve from 1.41632\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4177 - val_loss: 1.4876\n",
      "Epoch 2603/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02603: loss improved from 1.41632 to 1.41586, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4159 - val_loss: 1.4888\n",
      "Epoch 2604/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4171\n",
      "Epoch 02604: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4171 - val_loss: 1.4892\n",
      "Epoch 2605/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02605: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4184 - val_loss: 1.4927\n",
      "Epoch 2606/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02606: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4974\n",
      "Epoch 2607/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4266\n",
      "Epoch 02607: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4266 - val_loss: 1.4961\n",
      "Epoch 2608/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02608: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4217 - val_loss: 1.4876\n",
      "Epoch 2609/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4232\n",
      "Epoch 02609: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4232 - val_loss: 1.4865\n",
      "Epoch 2610/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02610: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4900\n",
      "Epoch 2611/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02611: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4203 - val_loss: 1.4921\n",
      "Epoch 2612/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02612: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4934\n",
      "Epoch 2613/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02613: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4214 - val_loss: 1.4880\n",
      "Epoch 2614/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02614: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4192 - val_loss: 1.4842\n",
      "Epoch 2615/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02615: loss did not improve from 1.41586\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4170 - val_loss: 1.4875\n",
      "Epoch 2616/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4155\n",
      "Epoch 02616: loss improved from 1.41586 to 1.41554, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4155 - val_loss: 1.4982\n",
      "Epoch 2617/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02617: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4210 - val_loss: 1.4944\n",
      "Epoch 2618/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02618: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4879\n",
      "Epoch 2619/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02619: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4181 - val_loss: 1.4906\n",
      "Epoch 2620/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02620: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4900\n",
      "Epoch 2621/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02621: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4184 - val_loss: 1.4870\n",
      "Epoch 2622/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02622: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4919\n",
      "Epoch 2623/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4185\n",
      "Epoch 02623: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4185 - val_loss: 1.4886\n",
      "Epoch 2624/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02624: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4172 - val_loss: 1.4864\n",
      "Epoch 2625/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02625: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4164 - val_loss: 1.4935\n",
      "Epoch 2626/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02626: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4176 - val_loss: 1.4911\n",
      "Epoch 2627/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4187\n",
      "Epoch 02627: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4187 - val_loss: 1.4886\n",
      "Epoch 2628/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02628: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4176 - val_loss: 1.4911\n",
      "Epoch 2629/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02629: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4894\n",
      "Epoch 2630/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4156\n",
      "Epoch 02630: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4156 - val_loss: 1.4857\n",
      "Epoch 2631/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02631: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4169 - val_loss: 1.4880\n",
      "Epoch 2632/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4161\n",
      "Epoch 02632: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4161 - val_loss: 1.4888\n",
      "Epoch 2633/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02633: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4173 - val_loss: 1.4890\n",
      "Epoch 2634/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4161\n",
      "Epoch 02634: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4161 - val_loss: 1.4903\n",
      "Epoch 2635/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4174\n",
      "Epoch 02635: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4174 - val_loss: 1.4885\n",
      "Epoch 2636/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4158\n",
      "Epoch 02636: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4158 - val_loss: 1.4888\n",
      "Epoch 2637/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02637: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4877\n",
      "Epoch 2638/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02638: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4178 - val_loss: 1.4880\n",
      "Epoch 2639/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4163\n",
      "Epoch 02639: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4163 - val_loss: 1.4884\n",
      "Epoch 2640/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02640: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4170 - val_loss: 1.4876\n",
      "Epoch 2641/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02641: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4165 - val_loss: 1.4886\n",
      "Epoch 2642/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02642: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4871\n",
      "Epoch 2643/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02643: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4196 - val_loss: 1.4906\n",
      "Epoch 2644/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02644: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4210 - val_loss: 1.4922\n",
      "Epoch 2645/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02645: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4207 - val_loss: 1.4933\n",
      "Epoch 2646/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02646: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4218 - val_loss: 1.4913\n",
      "Epoch 2647/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02647: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4251 - val_loss: 1.4927\n",
      "Epoch 2648/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02648: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4228 - val_loss: 1.4927\n",
      "Epoch 2649/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02649: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4210 - val_loss: 1.4891\n",
      "Epoch 2650/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02650: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4179 - val_loss: 1.4878\n",
      "Epoch 2651/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4156\n",
      "Epoch 02651: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4156 - val_loss: 1.4907\n",
      "Epoch 2652/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02652: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4170 - val_loss: 1.4937\n",
      "Epoch 2653/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02653: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4183 - val_loss: 1.4971\n",
      "Epoch 2654/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02654: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4182 - val_loss: 1.4927\n",
      "Epoch 2655/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02655: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4168 - val_loss: 1.4896\n",
      "Epoch 2656/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02656: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4159 - val_loss: 1.4874\n",
      "Epoch 2657/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02657: loss did not improve from 1.41554\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4167 - val_loss: 1.4866\n",
      "Epoch 2658/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02658: loss improved from 1.41554 to 1.41525, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4152 - val_loss: 1.4889\n",
      "Epoch 2659/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02659: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4913\n",
      "Epoch 2660/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02660: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4919\n",
      "Epoch 2661/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02661: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4894\n",
      "Epoch 2662/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02662: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - val_loss: 1.4887\n",
      "Epoch 2663/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02663: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4186 - val_loss: 1.4912\n",
      "Epoch 2664/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02664: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4175 - val_loss: 1.4889\n",
      "Epoch 2665/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02665: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4164 - val_loss: 1.4907\n",
      "Epoch 2666/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02666: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4165 - val_loss: 1.4947\n",
      "Epoch 2667/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02667: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4188 - val_loss: 1.4895\n",
      "Epoch 2668/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02668: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4862\n",
      "Epoch 2669/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02669: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4172 - val_loss: 1.4906\n",
      "Epoch 2670/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02670: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4173 - val_loss: 1.4917\n",
      "Epoch 2671/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02671: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4198 - val_loss: 1.4918\n",
      "Epoch 2672/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02672: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4915\n",
      "Epoch 2673/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4120\n",
      "Epoch 02673: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - val_loss: 1.4925\n",
      "Epoch 2674/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02674: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4892\n",
      "Epoch 2675/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02675: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4876\n",
      "Epoch 2676/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02676: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4903\n",
      "Epoch 2677/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02677: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4166 - val_loss: 1.4922\n",
      "Epoch 2678/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02678: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4911\n",
      "Epoch 2679/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02679: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4183 - val_loss: 1.4878\n",
      "Epoch 2680/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02680: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4170 - val_loss: 1.4869\n",
      "Epoch 2681/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02681: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4192 - val_loss: 1.4936\n",
      "Epoch 2682/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02682: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4933\n",
      "Epoch 2683/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02683: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4172 - val_loss: 1.4907\n",
      "Epoch 2684/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4162\n",
      "Epoch 02684: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4162 - val_loss: 1.4859\n",
      "Epoch 2685/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4155\n",
      "Epoch 02685: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4155 - val_loss: 1.4886\n",
      "Epoch 2686/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02686: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4180 - val_loss: 1.4862\n",
      "Epoch 2687/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02687: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4176 - val_loss: 1.4885\n",
      "Epoch 2688/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02688: loss did not improve from 1.41525\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4193 - val_loss: 1.4869\n",
      "Epoch 2689/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4145\n",
      "Epoch 02689: loss improved from 1.41525 to 1.41450, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4145 - val_loss: 1.4915\n",
      "Epoch 2690/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02690: loss did not improve from 1.41450\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4166 - val_loss: 1.4878\n",
      "Epoch 2691/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02691: loss did not improve from 1.41450\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4152 - val_loss: 1.4858\n",
      "Epoch 2692/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4148\n",
      "Epoch 02692: loss did not improve from 1.41450\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4148 - val_loss: 1.4877\n",
      "Epoch 2693/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4140\n",
      "Epoch 02693: loss improved from 1.41450 to 1.41402, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4140 - val_loss: 1.4891\n",
      "Epoch 2694/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02694: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4151 - val_loss: 1.4914\n",
      "Epoch 2695/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4171\n",
      "Epoch 02695: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4171 - val_loss: 1.4920\n",
      "Epoch 2696/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02696: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4172 - val_loss: 1.4951\n",
      "Epoch 2697/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02697: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4178 - val_loss: 1.4930\n",
      "Epoch 2698/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4033\n",
      "Epoch 02698: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4205 - val_loss: 1.4849\n",
      "Epoch 2699/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02699: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4181 - val_loss: 1.4877\n",
      "Epoch 2700/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02700: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4184 - val_loss: 1.4885\n",
      "Epoch 2701/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02701: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4182 - val_loss: 1.4895\n",
      "Epoch 2702/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4187\n",
      "Epoch 02702: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4173 - val_loss: 1.4887\n",
      "Epoch 2703/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02703: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4178 - val_loss: 1.4905\n",
      "Epoch 2704/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02704: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4170 - val_loss: 1.4875\n",
      "Epoch 2705/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02705: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4198 - val_loss: 1.4888\n",
      "Epoch 2706/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02706: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4187 - val_loss: 1.4864\n",
      "Epoch 2707/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02707: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4873\n",
      "Epoch 2708/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02708: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4158 - val_loss: 1.4900\n",
      "Epoch 2709/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02709: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4187 - val_loss: 1.4903\n",
      "Epoch 2710/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02710: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4180 - val_loss: 1.4895\n",
      "Epoch 2711/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02711: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4168 - val_loss: 1.4864\n",
      "Epoch 2712/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02712: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4147 - val_loss: 1.4873\n",
      "Epoch 2713/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02713: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4920\n",
      "Epoch 2714/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02714: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4165 - val_loss: 1.4893\n",
      "Epoch 2715/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4156\n",
      "Epoch 02715: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4156 - val_loss: 1.4892\n",
      "Epoch 2716/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02716: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4170 - val_loss: 1.4922\n",
      "Epoch 2717/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02717: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4195 - val_loss: 1.4910\n",
      "Epoch 2718/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4161\n",
      "Epoch 02718: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4161 - val_loss: 1.4899\n",
      "Epoch 2719/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02719: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4175 - val_loss: 1.4903\n",
      "Epoch 2720/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02720: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4172 - val_loss: 1.4886\n",
      "Epoch 2721/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02721: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4169 - val_loss: 1.4882\n",
      "Epoch 2722/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02722: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4197 - val_loss: 1.4928\n",
      "Epoch 2723/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02723: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4214 - val_loss: 1.4925\n",
      "Epoch 2724/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02724: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4910\n",
      "Epoch 2725/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 02725: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4918\n",
      "Epoch 2726/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02726: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4890\n",
      "Epoch 2727/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02727: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4214 - val_loss: 1.4903\n",
      "Epoch 2728/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02728: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4888\n",
      "Epoch 2729/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02729: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4172 - val_loss: 1.4894\n",
      "Epoch 2730/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02730: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4207 - val_loss: 1.4940\n",
      "Epoch 2731/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4187\n",
      "Epoch 02731: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4187 - val_loss: 1.4899\n",
      "Epoch 2732/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02732: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4208 - val_loss: 1.4920\n",
      "Epoch 2733/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02733: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4182 - val_loss: 1.4892\n",
      "Epoch 2734/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02734: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4178 - val_loss: 1.4896\n",
      "Epoch 2735/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02735: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4167 - val_loss: 1.4943\n",
      "Epoch 2736/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02736: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4179 - val_loss: 1.4959\n",
      "Epoch 2737/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02737: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4199 - val_loss: 1.4940\n",
      "Epoch 2738/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02738: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4226 - val_loss: 1.4891\n",
      "Epoch 2739/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02739: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4189 - val_loss: 1.4899\n",
      "Epoch 2740/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02740: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4903\n",
      "Epoch 2741/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02741: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4194 - val_loss: 1.4920\n",
      "Epoch 2742/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02742: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4191 - val_loss: 1.4924\n",
      "Epoch 2743/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02743: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4173 - val_loss: 1.5045\n",
      "Epoch 2744/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02744: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4928\n",
      "Epoch 2745/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02745: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4169 - val_loss: 1.4945\n",
      "Epoch 2746/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02746: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4209 - val_loss: 1.4909\n",
      "Epoch 2747/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02747: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4182 - val_loss: 1.4889\n",
      "Epoch 2748/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02748: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4183 - val_loss: 1.4867\n",
      "Epoch 2749/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02749: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4168 - val_loss: 1.4880\n",
      "Epoch 2750/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02750: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4172 - val_loss: 1.4904\n",
      "Epoch 2751/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02751: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4192 - val_loss: 1.4916\n",
      "Epoch 2752/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02752: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4900\n",
      "Epoch 2753/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02753: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4893\n",
      "Epoch 2754/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02754: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4903\n",
      "Epoch 2755/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02755: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4892\n",
      "Epoch 2756/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02756: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4159 - val_loss: 1.4892\n",
      "Epoch 2757/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4158\n",
      "Epoch 02757: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4158 - val_loss: 1.4904\n",
      "Epoch 2758/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02758: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4186 - val_loss: 1.4902\n",
      "Epoch 2759/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02759: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4241 - val_loss: 1.4931\n",
      "Epoch 2760/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02760: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4913\n",
      "Epoch 2761/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02761: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4178 - val_loss: 1.4903\n",
      "Epoch 2762/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02762: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4896\n",
      "Epoch 2763/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02763: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4170 - val_loss: 1.4922\n",
      "Epoch 2764/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02764: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4189 - val_loss: 1.4940\n",
      "Epoch 2765/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02765: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4190 - val_loss: 1.4933\n",
      "Epoch 2766/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4219\n",
      "Epoch 02766: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4219 - val_loss: 1.4871\n",
      "Epoch 2767/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02767: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4935\n",
      "Epoch 2768/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02768: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4184 - val_loss: 1.4931\n",
      "Epoch 2769/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02769: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4175 - val_loss: 1.4932\n",
      "Epoch 2770/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02770: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4182 - val_loss: 1.4893\n",
      "Epoch 2771/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4071\n",
      "Epoch 02771: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4159 - val_loss: 1.4920\n",
      "Epoch 2772/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02772: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4172 - val_loss: 1.4931\n",
      "Epoch 2773/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02773: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4173 - val_loss: 1.4890\n",
      "Epoch 2774/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4153\n",
      "Epoch 02774: loss did not improve from 1.41402\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4153 - val_loss: 1.4870\n",
      "Epoch 2775/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4136\n",
      "Epoch 02775: loss improved from 1.41402 to 1.41361, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4136 - val_loss: 1.4917\n",
      "Epoch 2776/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02776: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4881\n",
      "Epoch 2777/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02777: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4157 - val_loss: 1.4883\n",
      "Epoch 2778/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4145\n",
      "Epoch 02778: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4889\n",
      "Epoch 2779/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4140\n",
      "Epoch 02779: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4140 - val_loss: 1.4915\n",
      "Epoch 2780/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4174\n",
      "Epoch 02780: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4174 - val_loss: 1.4915\n",
      "Epoch 2781/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02781: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4949\n",
      "Epoch 2782/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02782: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4180 - val_loss: 1.4919\n",
      "Epoch 2783/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4134\n",
      "Epoch 02783: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4876\n",
      "Epoch 2784/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4155\n",
      "Epoch 02784: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4155 - val_loss: 1.4924\n",
      "Epoch 2785/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4137\n",
      "Epoch 02785: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4137 - val_loss: 1.4878\n",
      "Epoch 2786/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4141\n",
      "Epoch 02786: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4141 - val_loss: 1.4902\n",
      "Epoch 2787/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02787: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4157 - val_loss: 1.4896\n",
      "Epoch 2788/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4098\n",
      "Epoch 02788: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4161 - val_loss: 1.4899\n",
      "Epoch 2789/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4150\n",
      "Epoch 02789: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4150 - val_loss: 1.4893\n",
      "Epoch 2790/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02790: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4896\n",
      "Epoch 2791/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02791: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4192 - val_loss: 1.4891\n",
      "Epoch 2792/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02792: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4146 - val_loss: 1.4880\n",
      "Epoch 2793/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02793: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4149 - val_loss: 1.4871\n",
      "Epoch 2794/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4231\n",
      "Epoch 02794: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4231 - val_loss: 1.4915\n",
      "Epoch 2795/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02795: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4919\n",
      "Epoch 2796/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02796: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4876\n",
      "Epoch 2797/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4145\n",
      "Epoch 02797: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4199 - val_loss: 1.4923\n",
      "Epoch 2798/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4187\n",
      "Epoch 02798: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4187 - val_loss: 1.4950\n",
      "Epoch 2799/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02799: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4184 - val_loss: 1.4949\n",
      "Epoch 2800/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02800: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4198 - val_loss: 1.4923\n",
      "Epoch 2801/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02801: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4205 - val_loss: 1.4890\n",
      "Epoch 2802/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02802: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4182 - val_loss: 1.4941\n",
      "Epoch 2803/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4171\n",
      "Epoch 02803: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4171 - val_loss: 1.4928\n",
      "Epoch 2804/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4079\n",
      "Epoch 02804: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4159 - val_loss: 1.4906\n",
      "Epoch 2805/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4153\n",
      "Epoch 02805: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4153 - val_loss: 1.4921\n",
      "Epoch 2806/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4153\n",
      "Epoch 02806: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4153 - val_loss: 1.4903\n",
      "Epoch 2807/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02807: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4164 - val_loss: 1.4912\n",
      "Epoch 2808/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4174\n",
      "Epoch 02808: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4174 - val_loss: 1.4889\n",
      "Epoch 2809/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4153\n",
      "Epoch 02809: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4153 - val_loss: 1.4888\n",
      "Epoch 2810/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02810: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4151 - val_loss: 1.4905\n",
      "Epoch 2811/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4162\n",
      "Epoch 02811: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4162 - val_loss: 1.4956\n",
      "Epoch 2812/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02812: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4228 - val_loss: 1.4924\n",
      "Epoch 2813/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02813: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4208 - val_loss: 1.4877\n",
      "Epoch 2814/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02814: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4218 - val_loss: 1.4945\n",
      "Epoch 2815/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02815: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4201 - val_loss: 1.4933\n",
      "Epoch 2816/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4158\n",
      "Epoch 02816: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4158 - val_loss: 1.4890\n",
      "Epoch 2817/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4079\n",
      "Epoch 02817: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4885\n",
      "Epoch 2818/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02818: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4164 - val_loss: 1.4905\n",
      "Epoch 2819/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02819: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4189 - val_loss: 1.4911\n",
      "Epoch 2820/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4171\n",
      "Epoch 02820: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4171 - val_loss: 1.4916\n",
      "Epoch 2821/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4139\n",
      "Epoch 02821: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4139 - val_loss: 1.4893\n",
      "Epoch 2822/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02822: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4147 - val_loss: 1.4875\n",
      "Epoch 2823/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02823: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4871\n",
      "Epoch 2824/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4140\n",
      "Epoch 02824: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4165 - val_loss: 1.4880\n",
      "Epoch 2825/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02825: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4167 - val_loss: 1.4881\n",
      "Epoch 2826/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4156\n",
      "Epoch 02826: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4156 - val_loss: 1.4883\n",
      "Epoch 2827/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02827: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4159 - val_loss: 1.4888\n",
      "Epoch 2828/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02828: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4147 - val_loss: 1.4863\n",
      "Epoch 2829/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4139\n",
      "Epoch 02829: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4139 - val_loss: 1.4919\n",
      "Epoch 2830/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4146\n",
      "Epoch 02830: loss did not improve from 1.41361\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4146 - val_loss: 1.4899\n",
      "Epoch 2831/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4132\n",
      "Epoch 02831: loss improved from 1.41361 to 1.41324, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4132 - val_loss: 1.4923\n",
      "Epoch 2832/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02832: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4151 - val_loss: 1.4973\n",
      "Epoch 2833/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4187\n",
      "Epoch 02833: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4187 - val_loss: 1.4937\n",
      "Epoch 2834/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02834: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4167 - val_loss: 1.4919\n",
      "Epoch 2835/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02835: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4147 - val_loss: 1.4895\n",
      "Epoch 2836/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02836: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4166 - val_loss: 1.4925\n",
      "Epoch 2837/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4162\n",
      "Epoch 02837: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4162 - val_loss: 1.4913\n",
      "Epoch 2838/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02838: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4165 - val_loss: 1.4885\n",
      "Epoch 2839/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4136\n",
      "Epoch 02839: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4136 - val_loss: 1.4901\n",
      "Epoch 2840/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4163\n",
      "Epoch 02840: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4163 - val_loss: 1.4901\n",
      "Epoch 2841/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02841: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4194 - val_loss: 1.4972\n",
      "Epoch 2842/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02842: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4194 - val_loss: 1.4967\n",
      "Epoch 2843/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 02843: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4286 - val_loss: 1.4918\n",
      "Epoch 2844/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 02844: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4275 - val_loss: 1.4940\n",
      "Epoch 2845/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02845: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4245 - val_loss: 1.4990\n",
      "Epoch 2846/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02846: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4211 - val_loss: 1.4890\n",
      "Epoch 2847/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02847: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4188 - val_loss: 1.4877\n",
      "Epoch 2848/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02848: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4175 - val_loss: 1.4896\n",
      "Epoch 2849/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02849: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4179 - val_loss: 1.4926\n",
      "Epoch 2850/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02850: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4201 - val_loss: 1.4922\n",
      "Epoch 2851/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02851: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4168 - val_loss: 1.4913\n",
      "Epoch 2852/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02852: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4175 - val_loss: 1.4905\n",
      "Epoch 2853/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02853: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4912\n",
      "Epoch 2854/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4158\n",
      "Epoch 02854: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4158 - val_loss: 1.4918\n",
      "Epoch 2855/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4160\n",
      "Epoch 02855: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4160 - val_loss: 1.4882\n",
      "Epoch 2856/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02856: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4175 - val_loss: 1.4907\n",
      "Epoch 2857/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4129\n",
      "Epoch 02857: loss did not improve from 1.41324\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4140 - val_loss: 1.4897\n",
      "Epoch 2858/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4125\n",
      "Epoch 02858: loss improved from 1.41324 to 1.41248, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4125 - val_loss: 1.4934\n",
      "Epoch 2859/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4023\n",
      "Epoch 02859: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 1.4189 - val_loss: 1.4861\n",
      "Epoch 2860/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 02860: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4286 - val_loss: 1.4893\n",
      "Epoch 2861/3000\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 1.4365\n",
      "Epoch 02861: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4252 - val_loss: 1.4887\n",
      "Epoch 2862/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02862: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4222 - val_loss: 1.4898\n",
      "Epoch 2863/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4230\n",
      "Epoch 02863: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4196 - val_loss: 1.4916\n",
      "Epoch 2864/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4111\n",
      "Epoch 02864: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - val_loss: 1.4921\n",
      "Epoch 2865/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02865: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4910\n",
      "Epoch 2866/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4160\n",
      "Epoch 02866: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4160 - val_loss: 1.4894\n",
      "Epoch 2867/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02867: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4165 - val_loss: 1.4904\n",
      "Epoch 2868/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02868: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4172 - val_loss: 1.4911\n",
      "Epoch 2869/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02869: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4180 - val_loss: 1.4930\n",
      "Epoch 2870/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02870: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4172 - val_loss: 1.4919\n",
      "Epoch 2871/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4187\n",
      "Epoch 02871: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4187 - val_loss: 1.4901\n",
      "Epoch 2872/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02872: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4178 - val_loss: 1.4912\n",
      "Epoch 2873/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4020\n",
      "Epoch 02873: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4154 - val_loss: 1.4920\n",
      "Epoch 2874/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4148\n",
      "Epoch 02874: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4148 - val_loss: 1.4924\n",
      "Epoch 2875/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02875: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4164 - val_loss: 1.4905\n",
      "Epoch 2876/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02876: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4151 - val_loss: 1.4908\n",
      "Epoch 2877/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4140\n",
      "Epoch 02877: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4140 - val_loss: 1.4885\n",
      "Epoch 2878/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4137\n",
      "Epoch 02878: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4137 - val_loss: 1.4914\n",
      "Epoch 2879/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4163\n",
      "Epoch 02879: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4163 - val_loss: 1.4912\n",
      "Epoch 2880/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4161\n",
      "Epoch 02880: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4161 - val_loss: 1.4896\n",
      "Epoch 2881/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4155\n",
      "Epoch 02881: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4155 - val_loss: 1.4893\n",
      "Epoch 2882/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4090\n",
      "Epoch 02882: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4149 - val_loss: 1.4907\n",
      "Epoch 2883/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4150\n",
      "Epoch 02883: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4150 - val_loss: 1.4882\n",
      "Epoch 2884/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4148\n",
      "Epoch 02884: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4148 - val_loss: 1.4908\n",
      "Epoch 2885/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4134\n",
      "Epoch 02885: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4134 - val_loss: 1.4928\n",
      "Epoch 2886/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4143\n",
      "Epoch 02886: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4143 - val_loss: 1.4903\n",
      "Epoch 2887/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4139\n",
      "Epoch 02887: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4139 - val_loss: 1.4925\n",
      "Epoch 2888/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4146\n",
      "Epoch 02888: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4146 - val_loss: 1.4877\n",
      "Epoch 2889/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4122\n",
      "Epoch 02889: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4150 - val_loss: 1.4900\n",
      "Epoch 2890/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4077\n",
      "Epoch 02890: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4145 - val_loss: 1.4933\n",
      "Epoch 2891/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02891: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4159 - val_loss: 1.4943\n",
      "Epoch 2892/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02892: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4172 - val_loss: 1.4946\n",
      "Epoch 2893/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4138\n",
      "Epoch 02893: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4138 - val_loss: 1.4911\n",
      "Epoch 2894/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4143\n",
      "Epoch 02894: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4143 - val_loss: 1.4924\n",
      "Epoch 2895/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02895: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4905\n",
      "Epoch 2896/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02896: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4159 - val_loss: 1.4950\n",
      "Epoch 2897/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02897: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4198 - val_loss: 1.4938\n",
      "Epoch 2898/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02898: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4204 - val_loss: 1.4900\n",
      "Epoch 2899/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02899: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4189 - val_loss: 1.4887\n",
      "Epoch 2900/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4150\n",
      "Epoch 02900: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4150 - val_loss: 1.4912\n",
      "Epoch 2901/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02901: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4175 - val_loss: 1.4885\n",
      "Epoch 2902/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02902: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4918\n",
      "Epoch 2903/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4137\n",
      "Epoch 02903: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4172 - val_loss: 1.4967\n",
      "Epoch 2904/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4376\n",
      "Epoch 02904: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4184 - val_loss: 1.4907\n",
      "Epoch 2905/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4078\n",
      "Epoch 02905: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4158 - val_loss: 1.4907\n",
      "Epoch 2906/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4232\n",
      "Epoch 02906: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4154 - val_loss: 1.4894\n",
      "Epoch 2907/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02907: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4196 - val_loss: 1.4898\n",
      "Epoch 2908/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02908: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4205 - val_loss: 1.4890\n",
      "Epoch 2909/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02909: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4200 - val_loss: 1.4931\n",
      "Epoch 2910/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02910: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4188 - val_loss: 1.4966\n",
      "Epoch 2911/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4302\n",
      "Epoch 02911: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4221 - val_loss: 1.4909\n",
      "Epoch 2912/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4110\n",
      "Epoch 02912: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4197 - val_loss: 1.4911\n",
      "Epoch 2913/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02913: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4157 - val_loss: 1.4952\n",
      "Epoch 2914/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02914: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4197 - val_loss: 1.4987\n",
      "Epoch 2915/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02915: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4180 - val_loss: 1.4960\n",
      "Epoch 2916/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02916: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4224 - val_loss: 1.4896\n",
      "Epoch 2917/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02917: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4167 - val_loss: 1.4884\n",
      "Epoch 2918/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02918: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4180 - val_loss: 1.4909\n",
      "Epoch 2919/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4177\n",
      "Epoch 02919: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4177 - val_loss: 1.4893\n",
      "Epoch 2920/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02920: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4166 - val_loss: 1.4922\n",
      "Epoch 2921/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02921: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4151 - val_loss: 1.4916\n",
      "Epoch 2922/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4177\n",
      "Epoch 02922: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4152 - val_loss: 1.4932\n",
      "Epoch 2923/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4148\n",
      "Epoch 02923: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4148 - val_loss: 1.4961\n",
      "Epoch 2924/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02924: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4205 - val_loss: 1.4935\n",
      "Epoch 2925/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02925: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4169 - val_loss: 1.4944\n",
      "Epoch 2926/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02926: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4170 - val_loss: 1.4913\n",
      "Epoch 2927/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02927: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4188 - val_loss: 1.4922\n",
      "Epoch 2928/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02928: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4179 - val_loss: 1.4891\n",
      "Epoch 2929/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4163\n",
      "Epoch 02929: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4163 - val_loss: 1.4890\n",
      "Epoch 2930/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02930: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4205 - val_loss: 1.4890\n",
      "Epoch 2931/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02931: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4180 - val_loss: 1.4920\n",
      "Epoch 2932/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02932: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4924\n",
      "Epoch 2933/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02933: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4949\n",
      "Epoch 2934/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4177\n",
      "Epoch 02934: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4177 - val_loss: 1.4896\n",
      "Epoch 2935/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4144\n",
      "Epoch 02935: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4144 - val_loss: 1.4940\n",
      "Epoch 2936/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02936: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4152 - val_loss: 1.4905\n",
      "Epoch 2937/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4136\n",
      "Epoch 02937: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4136 - val_loss: 1.4901\n",
      "Epoch 2938/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4139\n",
      "Epoch 02938: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4139 - val_loss: 1.4939\n",
      "Epoch 2939/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4137\n",
      "Epoch 02939: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4165 - val_loss: 1.4946\n",
      "Epoch 2940/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02940: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4172 - val_loss: 1.4895\n",
      "Epoch 2941/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4153\n",
      "Epoch 02941: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4153 - val_loss: 1.4876\n",
      "Epoch 2942/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4177\n",
      "Epoch 02942: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4177 - val_loss: 1.4896\n",
      "Epoch 2943/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02943: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4172 - val_loss: 1.4912\n",
      "Epoch 2944/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4163\n",
      "Epoch 02944: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4163 - val_loss: 1.4925\n",
      "Epoch 2945/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4155\n",
      "Epoch 02945: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4155 - val_loss: 1.4895\n",
      "Epoch 2946/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02946: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4164 - val_loss: 1.4878\n",
      "Epoch 2947/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4143\n",
      "Epoch 02947: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4143 - val_loss: 1.4918\n",
      "Epoch 2948/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02948: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4173 - val_loss: 1.4950\n",
      "Epoch 2949/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4177\n",
      "Epoch 02949: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4177 - val_loss: 1.4895\n",
      "Epoch 2950/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4139\n",
      "Epoch 02950: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4139 - val_loss: 1.4898\n",
      "Epoch 2951/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4008\n",
      "Epoch 02951: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4129 - val_loss: 1.4896\n",
      "Epoch 2952/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4128\n",
      "Epoch 02952: loss did not improve from 1.41248\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4128 - val_loss: 1.4888\n",
      "Epoch 2953/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4125\n",
      "Epoch 02953: loss improved from 1.41248 to 1.41247, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4125 - val_loss: 1.4910\n",
      "Epoch 2954/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4144\n",
      "Epoch 02954: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4144 - val_loss: 1.4896\n",
      "Epoch 2955/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4079\n",
      "Epoch 02955: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4155 - val_loss: 1.4939\n",
      "Epoch 2956/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4143\n",
      "Epoch 02956: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4143 - val_loss: 1.4902\n",
      "Epoch 2957/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4135\n",
      "Epoch 02957: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4135 - val_loss: 1.4897\n",
      "Epoch 2958/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02958: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4920\n",
      "Epoch 2959/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02959: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4151 - val_loss: 1.4926\n",
      "Epoch 2960/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4156\n",
      "Epoch 02960: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4156 - val_loss: 1.4892\n",
      "Epoch 2961/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4156\n",
      "Epoch 02961: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4156 - val_loss: 1.4908\n",
      "Epoch 2962/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02962: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4169 - val_loss: 1.4915\n",
      "Epoch 2963/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4150\n",
      "Epoch 02963: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4150 - val_loss: 1.4908\n",
      "Epoch 2964/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02964: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4900\n",
      "Epoch 2965/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4158\n",
      "Epoch 02965: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4158 - val_loss: 1.4874\n",
      "Epoch 2966/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4125\n",
      "Epoch 02966: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4125 - val_loss: 1.4897\n",
      "Epoch 2967/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4132\n",
      "Epoch 02967: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4132 - val_loss: 1.4936\n",
      "Epoch 2968/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4135\n",
      "Epoch 02968: loss did not improve from 1.41247\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4135 - val_loss: 1.4887\n",
      "Epoch 2969/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4118\n",
      "Epoch 02969: loss improved from 1.41247 to 1.41179, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4118 - val_loss: 1.4907\n",
      "Epoch 2970/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4118\n",
      "Epoch 02970: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4118 - val_loss: 1.4934\n",
      "Epoch 2971/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4144\n",
      "Epoch 02971: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4144 - val_loss: 1.4995\n",
      "Epoch 2972/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02972: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4152 - val_loss: 1.4913\n",
      "Epoch 2973/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4145\n",
      "Epoch 02973: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4145 - val_loss: 1.4887\n",
      "Epoch 2974/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02974: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4939\n",
      "Epoch 2975/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02975: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4147 - val_loss: 1.4937\n",
      "Epoch 2976/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02976: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4903\n",
      "Epoch 2977/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4138\n",
      "Epoch 02977: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4138 - val_loss: 1.4966\n",
      "Epoch 2978/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02978: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.5004\n",
      "Epoch 2979/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02979: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4170 - val_loss: 1.4941\n",
      "Epoch 2980/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4143\n",
      "Epoch 02980: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4143 - val_loss: 1.4911\n",
      "Epoch 2981/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4131\n",
      "Epoch 02981: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4131 - val_loss: 1.4944\n",
      "Epoch 2982/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4160\n",
      "Epoch 02982: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4160 - val_loss: 1.4934\n",
      "Epoch 2983/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4054\n",
      "Epoch 02983: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4142 - val_loss: 1.4906\n",
      "Epoch 2984/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4131\n",
      "Epoch 02984: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4131 - val_loss: 1.4893\n",
      "Epoch 2985/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4133\n",
      "Epoch 02985: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4133 - val_loss: 1.4934\n",
      "Epoch 2986/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02986: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4159 - val_loss: 1.4943\n",
      "Epoch 2987/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4137\n",
      "Epoch 02987: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4137 - val_loss: 1.4964\n",
      "Epoch 2988/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4163\n",
      "Epoch 02988: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4163 - val_loss: 1.4894\n",
      "Epoch 2989/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02989: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4178 - val_loss: 1.4932\n",
      "Epoch 2990/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02990: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4154 - val_loss: 1.4916\n",
      "Epoch 2991/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4160\n",
      "Epoch 02991: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4160 - val_loss: 1.4924\n",
      "Epoch 2992/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4134\n",
      "Epoch 02992: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4134 - val_loss: 1.4906\n",
      "Epoch 2993/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4161\n",
      "Epoch 02993: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4161 - val_loss: 1.4890\n",
      "Epoch 2994/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02994: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4149 - val_loss: 1.4934\n",
      "Epoch 2995/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4140\n",
      "Epoch 02995: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4140 - val_loss: 1.4930\n",
      "Epoch 2996/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4141\n",
      "Epoch 02996: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4141 - val_loss: 1.4921\n",
      "Epoch 2997/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02997: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4149 - val_loss: 1.4883\n",
      "Epoch 2998/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02998: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4157 - val_loss: 1.4916\n",
      "Epoch 2999/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02999: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4898\n",
      "Epoch 3000/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4133\n",
      "Epoch 03000: loss did not improve from 1.41179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4133 - val_loss: 1.4874\n"
     ]
    }
   ],
   "source": [
    "hist = mdl.fit(A_train_in,u_train[0,:,:,:,:],\n",
    "            epochs=nb_epoch,batch_size=batch_size,shuffle=True,\n",
    "            validation_data=(A_val_in,u_val[0,:,:,:,:]),\n",
    "            callbacks=cb,verbose=1)\n",
    "hist_train.extend(hist.history['loss'])\n",
    "hist_val.extend(hist.history['val_loss'])\n",
    "mdl.load_weights(tempfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmCUlEQVR4nO3deXxV1aH28d/KTOaQBAgJSEBBIIQpDBZFcMCpzmixVqtV6bW21VZvtVar3tZrW6vX67VqaR37otY6V8WpgoiiTEIIYZ5DGJKQQObprPePfRJyyMlAOBl2+nw/n5CTPZ21zg5P1l577b2NtRYREXG/oO4ugIiIBIYCXUSkl1Cgi4j0Egp0EZFeQoEuItJLhHTXGyclJdkhQ4Z019uLiLjSypUrC621yf7mdVugDxkyhBUrVnTX24uIuJIxZmdL89TlIiLSSyjQRUR6CQW6iEgv0W196CLSu9TW1pKXl0dVVVV3F6VXiIiIIC0tjdDQ0Havo0AXkYDIy8sjJiaGIUOGYIzp7uK4mrWWoqIi8vLySE9Pb/d66nIRkYCoqqoiMTFRYR4AxhgSExOP+WhHgS4iAaMwD5yOfJauC/RN+0t59KONFJZVd3dRRER6FNcF+ub9ZTz+6RYOltd0d1FEpAcpKSnhySefPOb1zj//fEpKSgJfoG7gukAXEfGnpUCvr69vdb3333+f+Pj4TipV13LtKBc9aElEmrrrrrvYunUr48aNIzQ0lOjoaFJSUli9ejW5ublccskl7N69m6qqKm699Vbmzp0LHLkNSVlZGeeddx6nnnoqX375Jampqbz99tv06dOnm2vWfq4LdJ1zEen5HvjnOnLzDwd0m6MGxnLfhaNbnP+73/2OnJwcVq9ezaJFi7jgggvIyclpHPb37LPP0rdvXyorK5k0aRKXX345iYmJPtvYvHkzL7/8Mn/5y1+48soref311/ne974X0Hp0JtcFuohIe0yePNlnDPfjjz/Om2++CcDu3bvZvHlzs0BPT09n3LhxAEycOJEdO3Z0VXEDwrWBblGfi0hP1VpLuqtERUU1vl60aBGffPIJS5cuJTIykhkzZvgd4x0eHt74Ojg4mMrKyi4pa6C0eVLUGDPIGLPQGLPeGLPOGHOrn2WuNsZke7++NMaM7ZzignpcRMSfmJgYSktL/c47dOgQCQkJREZGsmHDBr766qsuLl3XaE8LvQ643Vq7yhgTA6w0xnxsrc1tssx24HRrbbEx5jxgHjClE8orIuJXYmIi06ZNIyMjgz59+tC/f//Geeeeey5PP/00mZmZjBgxgqlTp3ZjSTtPm4Furd0L7PW+LjXGrAdSgdwmy3zZZJWvgLQAl9NPuTr7HUTEbV566SW/08PDw1mwYIHfeQ395ElJSeTk5DROv+OOOwJevs52TOPQjTFDgPHA160sdgPg/5MLAI1yERHxr90nRY0x0cDrwG3WWr/jkYwxM3EC/dQW5s8F5gIMHjz4mAsrIiIta1cL3RgTihPm8621b7SwTCbwV+Bia22Rv2WstfOstVnW2qzkZL/POG03dbmIiPhqzygXAzwDrLfWPtrCMoOBN4BrrLWbAlvEZu/WuZsXEXGp9nS5TAOuAdYaY1Z7p90NDAaw1j4N/BpIBJ703vKxzlqbFfDSiohIi9ozymUJbTSLrbU3AjcGqlDtoQuLRER8ue5uixrlIiKBEB0dDUB+fj6zZ8/2u8yMGTNYsWJFq9t57LHHqKioaPy5O2/H67pAFxEJpIEDB/Laa691eP2jA707b8fr2kDXKBcRaerOO+/0uR/6/fffzwMPPMCZZ57JhAkTGDNmDG+//Xaz9Xbs2EFGRgYAlZWVzJkzh8zMTL7zne/43Mvl5ptvJisri9GjR3PfffcBzg2/8vPzmTlzJjNnzgSc2/EWFhYC8Oijj5KRkUFGRgaPPfZY4/uNHDmSm266idGjRzNr1qyA3TPGdTfnUo+LiAssuAv2rQ3sNgeMgfN+1+LsOXPmcNttt/GjH/0IgFdffZUPPviAn/3sZ8TGxlJYWMjUqVO56KKLWnxe51NPPUVkZCTZ2dlkZ2czYcKExnkPPvggffv2pb6+njPPPJPs7Gx++tOf8uijj7Jw4UKSkpJ8trVy5Uqee+45vv76a6y1TJkyhdNPP52EhIROu02va1voIiJNjR8/ngMHDpCfn8+aNWtISEggJSWFu+++m8zMTM466yz27NnD/v37W9zG4sWLG4M1MzOTzMzMxnmvvvoqEyZMYPz48axbt47c3NyWNgPAkiVLuPTSS4mKiiI6OprLLruMzz//HOi82/S6roUuIi7QSku6M82ePZvXXnuNffv2MWfOHObPn09BQQErV64kNDSUIUOG+L1tblP+Wu/bt2/nj3/8I8uXLychIYHrrruuze3YVvqFO+s2va5robd0qCQiMmfOHF555RVee+01Zs+ezaFDh+jXrx+hoaEsXLiQnTt3trr+9OnTmT9/PgA5OTlkZ2cDcPjwYaKiooiLi2P//v0+N/pq6ba906dP56233qKiooLy8nLefPNNTjvttADWtjm10EWk1xg9ejSlpaWkpqaSkpLC1VdfzYUXXkhWVhbjxo3j5JNPbnX9m2++meuvv57MzEzGjRvH5MmTARg7dizjx49n9OjRDB06lGnTpjWuM3fuXM477zxSUlJYuHBh4/QJEyZw3XXXNW7jxhtvZPz48Z36FCTT2mFBZ8rKyrJtje/05+Pc/dz04gr++eNTGZMW1wklE5GOWL9+PSNHjuzuYvQq/j5TY8zKlq7Ed1+XS3cXQESkh3JdoIuIiH+uDXTdy0Wk5+muLtzeqCOfpesCXYNcRHqmiIgIioqKFOoBYK2lqKiIiIiIY1pPo1xEJCDS0tLIy8ujoKCgu4vSK0RERJCWdmyPZ3ZtoKsRINKzhIaGkp6e3t3F+LemLhcRkV7CdYEuIiL+uTbQ1eMiIuLLdYFudGmRiIhfrgt0ERHxz7WBrrGuIiK+3Bfo6nEREfHLfYEuIiJ+uTbQ1eEiIuLLdYGuHhcREf9cF+giIuKfawNdg1xERHy5LtD1kGgREf9cF+giIuKfiwNdfS4iIk25LtDV4SIi4p/rAl1ERPxzbaBrlIuIiC/XBboGuYiI+Oe6QBcREf9cG+jqcRER8eW6QNcTi0RE/HNdoIuIiH+uDXSNchER8dVmoBtjBhljFhpj1htj1hljbvWzjDHGPG6M2WKMyTbGTOic4mqUi4hIS0LasUwdcLu1dpUxJgZYaYz52Fqb22SZ84CTvF9TgKe830VEpIu02UK31u611q7yvi4F1gOpRy12MfCidXwFxBtjUgJeWt9ydebmRURc55j60I0xQ4DxwNdHzUoFdjf5OY/moY8xZq4xZoUxZkVBQcExFtW7jQ6tJSLS+7U70I0x0cDrwG3W2sNHz/azSrMmtLV2nrU2y1qblZycfGwlFRGRVrUr0I0xoThhPt9a+4afRfKAQU1+TgPyj794LVOHi4iIr/aMcjHAM8B6a+2jLSz2DnCtd7TLVOCQtXZvAMvZpECdslUREddrzyiXacA1wFpjzGrvtLuBwQDW2qeB94HzgS1ABXB9wEsqIiKtajPQrbVLaKNdbJ0hJ7cEqlDtoUEuIiK+XHelqO7lIiLin+sCXURE/HNtoFuNcxER8eG6QNe9XERE/HNdoIuIiH/uDXT1uIiI+HBdoKvHRUTEP9cFuoiI+OfaQFePi4iIL9cFutEwFxERv1wX6CIi4p9rA133chER8eW6QFePi4iIf64LdBER8c+1ga57uYiI+HJdoKvHRUTEP9cFuoiI+OfaQNcoFxERX64LdI1yERHxz3WBLiIi/rk20NXjIiLiy4WBrj4XERF/XBjoIiLij2sD3WqYi4iID9cFuka5iIj457pAFxER/1wb6OpwERHx5bpAV4+LiIh/rgt0ERHxz72Brj4XEREfrgt0PSRaRMQ/1wW6iIj459pA1xOLRER8uS7Q1eEiIuKf6wJdRET8c22g61YuIiK+XBfoGuQiIuKf6wK9gVroIiK+XBfoRqdFRUT8ajPQjTHPGmMOGGNyWpgfZ4z5pzFmjTFmnTHm+sAXU0RE2tKeFvrzwLmtzL8FyLXWjgVmAI8YY8KOv2itU4+LiIivNgPdWrsYONjaIkCMca7Jj/YuWxeY4jWnk6IiIv4Fog/9CWAkkA+sBW611nr8LWiMmWuMWWGMWVFQUBCAtxYRkQaBCPRzgNXAQGAc8IQxJtbfgtbaedbaLGttVnJy8nG9qZ4pKiLiKxCBfj3whnVsAbYDJwdguyIicgwCEei7gDMBjDH9gRHAtgBsV0REjkFIWwsYY17GGb2SZIzJA+4DQgGstU8DvwGeN8asxbl31p3W2sJOK7GXOlxERHy1GejW2qvamJ8PzApYidqgUS4iIv657kpRERHxz7WBrkEuIiK+XBfoupeLiIh/rgt0ERHxz8WBrj4XEZGmXBfoGuUiIuKf6wJdRET8c22ga5SLiIgv1wW6ulxERPxzXaCLiIh/rg109biIiPhyXaDrwiIREf9cF+giIuKfawNdo1xERHy5LtA1ykVExD/3BXpNGelmL0H1Nd1dFBGRHsV1gR61ayELw2+nT/mO7i6KiEiP4rpAN94+F6NOdBERH64LdDRsUUTEL9cFum04K2o93VsQEZEexnWBboy3yOpyERHx4bpAbxi3aFALXUSkKdcFuvX2oauBLiLiy3WBTkOXi27PJSLiw32BTsOwRXW5iIg05b5AbzwpqkAXEWnKhYHeMA5dXS4iIk25N9B1VlRExIcLA90pslELXUTEh+sCvfHCf/Whi4j4cF2g28YWuoiINOW6QG+McrXQRUR8uC7QdS8XERH/XBfoulJURMQ/1wW6bp8rIuKf6wL9yMlQtdBFRJpyXaAfGYeuFrqISFPuC3R0paiIiD/uC/SghlEu3VsMEZGeps1AN8Y8a4w5YIzJaWWZGcaY1caYdcaYzwJbxGbv5v1XXS4iIk21p4X+PHBuSzONMfHAk8BF1trRwBUBKVnLbwiAVZeLiIiPNgPdWrsYONjKIt8F3rDW7vIufyBAZfOv4aSoAl1ExEcg+tCHAwnGmEXGmJXGmGtbWtAYM9cYs8IYs6KgoKCDb6f7oYuI+BOIQA8BJgIXAOcA9xpjhvtb0Fo7z1qbZa3NSk5O7ti76dJ/ERG/QgKwjTyg0FpbDpQbYxYDY4FNAdh2M0HBTqBbT31nbF5ExLUC0UJ/GzjNGBNijIkEpgDrA7Bdv0KCnC6Xel36LyLio80WujHmZWAGkGSMyQPuA0IBrLVPW2vXG2M+ALIBD/BXa22LQxyPV3BQMAAej7pcRESaajPQrbVXtWOZh4GHA1KiNgQHNwS6ulxERJpy3ZWiwd4+dE+9ulxERJpyX6B7L/33qA9dRMSH6wI9pKHLpV596CIiTbku0Bv70NVCFxHx4bpAb6CToiIivtwX6A035/KohS4i0pT7Aj3IGWmpFrqIiC/3BXpwOADGU9PNBRER6VlcGOihAJj62m4uiIhIz+LCQA8DwNSrhS4i0pR7A92jFrqISFMuDHRvl4sCXUTEh/sC3RhqCSFIgS4i4sN9gQ7UEYKtUx+6iEhTrgz0ehOKravu7mKIiPQo7gz0oBA8aqGLiPhwZaB7gkJBwxZFJFBqq+D+OFj1t+4uyXFxZaBbBbqIO1kLRVs79z0ObocN78OhvCPTSve1vk7lQef7p79te9tL/gdWv9x8Xn0dbP3Uee+jvXqt8wdj11etb/84tfkIup6oj6ecb5svWP7m40y69KfdXRyR3slTDzXlUF0KcalHptfXQVBw443y2rWdrQuhTwIseRQ2vAs/+AgGT2m+bN4KZ2hyylioq3YabvOvgCn/AaMvcZapOOiEdU05ZL8Coy6BA7mw4T0YfSm8f8eR7d2dD9mvwru3wbAzYerNYD3OH5WY/pBxORRugScmOsuX7YNP7nfq7KmHkRfC/7vMmXfu7+GDO49sO3kEhEVD7EB4/QbY9EHz+qROhKwbIPdt5+dnz4HLn4Exs9v32R0jY233PCgiKyvLrlixokPreh7oS5CtZ41nKGP/65sAl0ykA16/EdKnw4Rru+49i7ZCTZkTfrWVUFcF4bHwrwecEIkd2HjdBgD7cyE0AvoOdX6uLnNuduepc7YTMwA8Hqg+DBFx8H8T4OA2Z9mpP3KmrXweSvc6006/09nG8r/ClS96t2vBBEN4jBPeXz/dcvnPvM8J5jUvQ22F77xTfgxLnwjUJ9XzXPsODD29Q6saY1Zaa7P8znNjoNc8PJKw8nzWewax7+pPmTmiX4BLJz2CtbB6Poy8CBb9DrL/Dr9ocrheWwUVRU7r0Vqn5RbZ1wm38gPwv2Phxk8hbaIz/3A+rH4J+p0MI853WpnFOyE2FYK9B6uVxVB1CBKGQHkhLJvnBFeQ82AVDu+FVS/AtFudw/Mz7oHQPs7hNEBkElz8hBOU/UZBzuswea7TKgyJgKAgp4W7+ytImwQh4VCy22lhnjTLqeeGd2HYTCdcv/qT72cy/hr45qh+3lEXH2kBtkdIhBP+Rzv9TiecK4ravy3pmAFj4D+WdGjVXhfozJsJ+as4YOOZXP0kc6cP5fZZwwkPCYaaCijbD33TOXC4in6xEb7r1pQ7tw9o2nLxx+OBzx+BSTc4IdETVJc6rZ/QPlCw0Qmmo9WUw/bFMPzc5ofEdTVO6yrhhPa/p7UtH1pvWwSfPewcCqeMdaalTnRaXXFpsOBO55D2qz/B6MucoAoOhzUvwS+2O4G5LxsGTQET5ITgoocg9y0YNBVm3g1/Pg1SxsHe1c72T5rlBPcZ98DfLnGm3bEFHh8PNaX+y9lvlBOYR5vzErzy3SM/98+A/TnO66bvmTIW9q5p+TMacQFsfK/l+dL9BoyBfWv9zzv15zD8HKeffcN7zut3f+YcqQBM/iGERzt5ADD7WdjxBax4Bm5aCDuWOA2B5BGQNByqSiAhHTZ96LxnxqWQ84bzO2s98MwsOP9hOPmCDlWl9wX689+GHZ/7TPrLqBe4yr5H9PpXAVh89rt8s+AZTvnBH5k8NMlZqLIYfj/EeZ0+HYZMh5AwZ0ee9YDzumgrxJ/ghNX8y2HMlZA4DNKy4MSz4JlznNbVrw8eabW1pHCLc4gbl+Y7ff86Z/un3AIH1jsBtfwvMPa7TqDED4aP7oFLn3Z+ic55CAZkHGkFjrwI1r8DZ/8GMi6Dp09zflFu+dppvTWc2Jk81+kDXPI/zi9WeYEz/Yx7Yc9KCIuCabc5h9wLH4TNH8HYq5xD4Anfh5O/DS9d0ZE9JL1Vn75Ov3bKONi5BE48G7Z87MxLGgEVhUda+APGOH8MR1zgfH9mFpzwLScwY1OhZJfzfyoq0fm/eXAbJI90jh7KDniPhG4DLKz/p9MIG/wtp2unbJ/T3ROX6hxxVB1yyhU70HnvnV86R0AtNdxqq6Cu0unXd5neF+iv3wRrXw1sgQBu3wiPjHBeDzmt2R+NVjUsf8a9Tqv08XGBLdus3zohL73DGffA8mecoGt6Mi0s2unPBjjtDsj/xhk5MSADZj0IL17kBGfaJOib7hyRBYXAsDNg4/vOH/YBmU43zL5sJxiHzXT+eFcWQ2WJs17RVudk4fjvQfwg/2WsLHaO0No6Qq0pd7YvXaL3Bfr+XHjqlMAWSNxj2JlOgNXXwJgrnEBsOrJh0o3OycmoZPjkAaivdkYWFGyAjQucdWNT4Y0bne6pjNnO4fXQmc68vGWQfroTrtsXQeJJ0CceYgaCp9bp8qqvg7zlztFbdanTZVS4yWmVRsQ7ZQsJdwK2IeyqyyA00unCau8IEZGj9L5AB/h6Hiz4z8AVyO0S0qF4u++0wafAwPHw1ZNNlhsCxTv8b+OiJ5zwsR5npETJLjjtdqePOyTC6d8ODnUOecdd7bQe4wc7XTUTvg+JJ8LSPzmtvoNbnT7phtZdwSaITna20zTkGuxdA6FRkHSi83NdDWCdUBSRRr0z0AE2feSclQ8KhswrYedSOPPXlOe8R9Hbv2JwkNNn/HTdhSRQyoigXURSzQN11zI/7CE+rM9ij01iqNnLjODmJ702e1J5pX4G1YTx29DnACizEeyzfTkxKB+ARfVj6WdKGBW0Ey54BN673XcjkYnOSdjSvc7h9fmPQMlOpx89JBym/NDpxzucD6tehOm/cEZCgHNYHB7rtAhXvUjtuGsw9TWE7Fzs9DXu/grGX3ukxVdT4fSHR8Qe3+cqIj1W7w30NlQX7cRDMH0SnZOSzy7ZzsKNB/h8c2GzZftQRSURGDxEUUU9QVQS0Wy5psvXEEo9R06MjhsUT1hwED+aOYzrnlsOwJd3nUFhWTURocEM7x9DVW09y7Yf5P21e7n7gpGUV9eRFB1OaHAQX2wpZPzgeCLDnCF0d/xjDW9+s4eHZ2dy2YQ0hv9qAelJUXz4s+mUVNQQHxnmt2ylVbUUl9cyODGyw59db5Kbf5i+UWEMiGt5f4q4xb9toLdmze4Sauo9BBk4eUAs1z67jJU7i7u0DIlRYRSVN7+FwZVZaZRW1bEg58jlytsfOp/0XzqXFP/fVeP5ycvOBVVXTxnM9dPSefjDDTxy5Tiiw0O4/KkvWbmzmFNPTCI8JIjHrxpPVPixXRS8p6SS8JAgkqLDeXHpDmaO6Ef/2AjCQpyjh8NVtcRGtDH0s4cYcpczpHDH7zo2TEykJ1Ggd0BJhRO08ZFhVNXWc8MLy/nWsCT2lFRyYnI0//VuLknR4USHB3PWyP78dcn2NrbYNX5/+RjufL35eNs/fXcCEaFB3PDCCv58zUT2HaoiZ88h5k4fykn9Y7DWUlpdx47Ccn7zbi7Ldzh/3JbcOZNTf78QgLDgIGrqPfz87OE8+vEmzssYwIKcfcy/cQqFZdVcPC612fv2BAp06U0U6F3I47FsLypnWHI0OXsO8dCC9fxw+jD+sdK5UZC1ltT4Pvx58bZuLmn7xESEUFpV165lF90xg36x4Tzy0SbW5R/imqlDODklhiGJUTzx6RbOHNmPoclRvJe9l7NG9uej3H388o21PHTZGL4zaTDWWowx7D9cRf+jLwg7Du0NdI/HsmFfKaMGtnwOoqbOwxur8rgyaxBBQRqpIl1Pge4SHo9lW2EZhWU1rN5dwg+mpRMabFi8uZDvP7uMqyYP5udnD2fSg5+QdUICq3eXMLhvJNsKyxmTGsfaPYe6uwrHpW9UGAe9XVApcRFMSe9LfkkVp56UxLaCMt5anc+Ht00nMTqMvOJKKmrq+GxTAZmp8YwbHM/AuAheXbGbb2cOZHthOQPiIli6taixe2rjb89lT3ElZzzyGc9fP4kZR90y4m9Ld3Dv2+t46aYpfGtYkt8y/vmzrTy0YAN/uDyTKyf5jt+u91jmLd7GNaecQPQxdnHlFVeQGt8Ho+GM0gYF+r8Rj8c2thwra+oJCoLwkGBW7SomLaEPBw5Xk1dcSXVdPeXV9XxrWCJ9woL5YkshP3/VGelz84xhfLGlkOw8d/+BaMtLN04hKjyEFTuLeXnZLrYcKGucd88FI/nte+sBp9spMiyE7YVlvLM6nxeW7uSnZ5xIXkklA2IjuOm0ocT2CeVXb67lleW7OSExks/+cyYAG/Yd5qR+MQS30prPzT/M+Y9/zn0XjiI3/zC3nT2c1Pg+AGwvLKfeYzmxX3TA6n2oopYvthYyY0Ry4wl4N3jwvVwyUuN6bNdeV1GgS6cpraqlsrae4vJaPNbyxZZCaustecUVhIUE8dwXOzh39AC+2FrY7q6b3uqH04dyRdYgznr0M/4+dyrPf7mD/YerWLWrpNmyC249jfjIUE556FMAcv/rHCLDQthTUskXWwr5xWvZ/PPHp5KRGuvTqj9YXsNtf1/N96YMZu7fVnL72cNZuauY56+fjMdjqfV4GHnvB3gszBrVn3nX+s0FAD5ct4+T+kVTU+/hk9z9/PGjTXx51xkM9P6xac1H6/YxfnACyTHNryPIK64gLaH9I7A8Hiejht7tDAr4dz8XokAXV6r3WN7NzmdQ30jGpcWTu/cwUeEhFHmHge49VIXHWn74t5WN6yRFh1NYVs1l41OJjghh1a5icvYc5udnD+fFpTspLOudz6KNDg+hrLrlP5hDEiPZUVTRbPo1U0/gJ2ecSHWdhw/X7aO6zsPA+AguGZfaOKrq6OXfWJXHzTOGccvME/npK6v5alsRj1wxlozUOPpGhVFT52H4PQsYGBfBl788k60FZTz2yWa+nZnCA++sI/9QFUnRYSz+xUyfI4R6j+WlZbu4YmIaEaHBVNXWU1PvYep//4uTB8Q0/uFrK9C3FpQRExFCv5jOGaZ671s5pCdF8cn6/fz2kgyGJgfu6Kk9FOgiXmXVdUSEBBESHERecQVBxlBZW8/+Q1XkH6piTGocIwbEsL2wnLp6D8YYNu8vZefBCnYdrOClr3cxNi2ONd7uqG/uPZvpf1hIrcdDWHAQl09MY13+YZZtP9jNNXWHsYPiGZUSw7DkaJZtP8hHufsZmRLLP/7jFM59bDF5xZXN1rlgTAqrd5cwa3R/rpg4iO2F5YxMiWFnUQXr9x3mDx9sBJyhvsYY5sxbSt+oMJ68eqLfMkz57084ITGK04cnM3VoIn/+bCsPXjqG5Jhw/r58F+EhwdR5LNNOTCQlrk/jSXaAmSOSee76yY0/5+w5REVNPZPTO+8OrQp0kW5mrSVnz2EKy6qZOjSRqtp6EqLCqKipI6+4kgFxEYQGBRERGkRecSX/WJnHzBHJfJS7n3NHD2DvoSqy80o49aQklmwuZGRKLD95+RuG94+mzmPZVlDO0OQoBsRG8OVW526HowfGsi7/sN/yPHX1BG6ev6orP4JuccGYFN5bu7fZ9KiwYMpr6ltd97nrJ3G99wLBBi/8YDLff3aZz7SV95zFsu0Hm32ePznjRM4ZPYBRKbFY4MWlO/jWsCSGJUcREtzxp38q0EX+TRWWVRMTEeI8K8CP1btLGBgfQXWth8iwYEKCgwgLDuL9tXuZeEIC2wrLGJUSx/tr9zK8fwwD4sL555q9/O+/NgOQkRrLfReO5rONBcz7fBs1dZ7GbWemxTWeWL9l5jD+tHAr/++GKSzbXsTjn27p/Mr3YB//bDon9Y/p0LoKdBHpcbYVlBHXJ5S+UWEYYxovbouNCOXd7Hxq6jycMiyRLQfKGJkSy8HyGob3j6GorJqtBeV4rGVw30gWbyrgy61F7Cgq54IxKXy64QBpCZG8viqv2XvOGJHMoo0Fzab/5zkj2HqgjNV5JdTUefx29QTSNVNP4DeXZHRo3eMKdGPMs8C3gQPW2hZLYIyZBHwFfMda+1pbhVKgi0h3KauuIyosuMVx/8XlNRSV15CW0IeIUOfopqSihnX5hxmUEEnf6DA+zt3HJeNS2bS/jNKqWjJS4xqXtdaSf6iKnUXlrM07xItLdzJ7YhpR4cEYDGeN6k96UsfuIX+8gT4dKANebCnQjTHBwMdAFfCsAl1EpHO0Fuht9sxbaxcDbZ2y/wnwOnDg2IsnIiKB0PFTrV7GmFTgUuDpdiw71xizwhizoqCgeT+WiIh03HEHOvAYcKe1tvUxQIC1dp61Nstam5WcnByAtxYRkQaBuJFDFvCK9+RCEnC+MabOWvtWALYtIiLtdNyBbq1Nb3htjHkeeFdhLiLS9doMdGPMy8AMIMkYkwfcB4QCWGvb7DcXEZGu0WagW2uvau/GrLXXHVdpRESkwwJxUlRERHqAbrv03xhTAOzs4OpJQGEAi9OdVJeeqbfUpbfUA1SXBidYa/0OE+y2QD8expgVLV0p5TaqS8/UW+rSW+oBqkt7qMtFRKSXUKCLiPQSbg30ed1dgABSXXqm3lKX3lIPUF3a5Mo+dBERac6tLXQRETmKAl1EpJdwXaAbY841xmw0xmwxxtzV3eVpizFmhzFmrTFmtTFmhXdaX2PMx8aYzd7vCU2W/6W3bhuNMed0X8mdp1UZYw4YY3KaTDvmshtjJno/gy3GmMdNS4+J6fq63G+M2ePdN6uNMef39LoYYwYZYxYaY9YbY9YZY271TnfdfmmlLm7cLxHGmGXGmDXeujzgnd61+8Va65ovIBjYCgwFwoA1wKjuLlcbZd4BJB017Q/AXd7XdwG/974e5a1TOJDurWtwN5Z9OjAByDmesgPLgFMAAywAzushdbkfuMPPsj22LkAKMMH7OgbY5C2v6/ZLK3Vx434xQLT3dSjwNTC1q/eL21rok4Et1tpt1toa4BXg4m4uU0dcDLzgff0CcEmT6a9Ya6uttduBLTh17hbW/9OqjqnsxpgUINZau9Q6v60vNlmny7RQl5b02LpYa/daa1d5X5cC64FUXLhfWqlLS3pyXay1tsz7Y6j3y9LF+8VtgZ4K7G7ycx6t/wL0BBb4yBiz0hgz1zutv7V2Lzi/1EA/73Q31O9Yy57qfX309J7ix8aYbG+XTMPhsCvqYowZAozHaQ26er8cVRdw4X4xxgQbY1bjPIrzY2ttl+8XtwW6v76knj7ucpq1dgJwHnCLcR663RI31q9BS2XvyXV6ChgGjAP2Ao94p/f4uhhjonGe43ubtfZwa4v6mdbT6+LK/WKtrbfWjgPScFrbGa0s3il1cVug5wGDmvycBuR3U1naxVqb7/1+AHgTpwtlv/fQCu/3hodru6F+x1r2PO/ro6d3O2vtfu9/Qg/wF450b/XouhhjQnECcL619g3vZFfuF391cet+aWCtLQEWAefSxfvFbYG+HDjJGJNujAkD5gDvdHOZWmSMiTLGxDS8BmYBOThl/r53se8Db3tfvwPMMcaEG2PSgZNwTpD0JMdUdu9hZqkxZqr3bP21TdbpVg3/0bwuxdk30IPr4n3fZ4D11tpHm8xy3X5pqS4u3S/Jxph47+s+wFnABrp6v3TlmeBAfAHn45wN3wr8qrvL00ZZh+KcyV4DrGsoL5AI/AvY7P3et8k6v/LWbSPdMBrkqPK/jHPIW4vTcrihI2XHee5sjnfeE3ivUO4BdfkbsBbI9v4HS+npdQFOxTkEzwZWe7/Od+N+aaUubtwvmcA33jLnAL/2Tu/S/aJL/0VEegm3dbmIiEgLFOgiIr2EAl1EpJdQoIuI9BIKdBGRXkKBLiLSSyjQRUR6if8PwMRVYxQOtgQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist_train,label='train')\n",
    "plt.plot(hist_val,label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are results calculated through two methods the same? True\n"
     ]
    }
   ],
   "source": [
    "# get results\n",
    "y = mdl.predict(A_train_in)\n",
    "\n",
    "mode_shape = []\n",
    "ae_modes = []\n",
    "for i in range(latent_dim):\n",
    "    mode_shape.append(decoders[i].predict(np.reshape(1,(1,1))))\n",
    "    ae_modes.append(decoders[i].predict(A_train_in[:,[i]]))\n",
    "y_add = np.sum(ae_modes,axis=0)\n",
    "print('are results calculated through two methods the same?',np.array_equal(y,y_add))\n",
    "\n",
    "mode_shape = np.array(mode_shape)\n",
    "ae_modes = np.array(ae_modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visulise the weights of the decoder\n",
    "fig1,ax1 = plt.subplots(2,latent_dim,sharey=True,sharex=True)\n",
    "fig1.suptitle('shape of the modes')\n",
    "ax1[0,0].set_ylabel('POD mode of data')\n",
    "ax1[1,0].set_ylabel('decoder weight')\n",
    "for i in range(latent_dim):\n",
    "    pltV = np.reshape(Q_POD_data[:,i],[2*Ny,Nz])\n",
    "    im1 = ax1[0,i].imshow(pltV[:Ny,:],'jet')\n",
    "    ax1[0,i].set_title(str(i+1))\n",
    "    ax1[0,i].set_xticks([])\n",
    "    ax1[0,i].set_yticks([])\n",
    "    div = make_axes_locatable(ax1[0,i])\n",
    "    cax = div.append_axes('right',size='5%',pad='2%')\n",
    "    plt.colorbar(im1,cax=cax)\n",
    "\n",
    "    im2 = ax1[1,i].imshow(mode_shape[i,0,:,:,0],'jet')\n",
    "    div = make_axes_locatable(ax1[1,i])\n",
    "    cax = div.append_axes('right',size='5%',pad='2%')\n",
    "    plt.colorbar(im2,cax=cax)\n",
    "\n",
    "# visualise the changing modes\n",
    "t = 100\n",
    "\n",
    "fig2,ax2 = plt.subplots(2,latent_dim,sharey=True,sharex=True)\n",
    "# fig2,ax2 = plt.subplots(2,[latent_dim],sharey='all')\n",
    "fig2.suptitle(f'AE modes or $a_i\\phi_i$ at t={t}')\n",
    "ax2[0,0].set_ylabel('POD mode of data')\n",
    "ax2[1,0].set_ylabel('decoder')\n",
    "for i in range(latent_dim):\n",
    "    pltV_t = pod_modes_t[i,0,:,:,t]\n",
    "    im1 = ax2[0,i].imshow(pltV_t,'jet')\n",
    "    ax2[0,i].set_title(str(i+1))\n",
    "    ax2[0,i].set_xticks([])\n",
    "    ax2[0,i].set_yticks([])\n",
    "    div = make_axes_locatable(ax2[0,i])\n",
    "    cax = div.append_axes('right',size='5%',pad='2%')\n",
    "    plt.colorbar(im1,cax=cax)\n",
    "\n",
    "    im2 = ax2[1,i].imshow(ae_modes[i,t,:,:,0],'jet')\n",
    "    div = make_axes_locatable(ax2[1,i])\n",
    "    cax = div.append_axes('right',size='5%',pad='2%')\n",
    "    plt.colorbar(im2,cax=cax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEOCAYAAAC0BAELAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyjElEQVR4nO3deXxV9ZnH8c+TEPZ9FQmbiiggAkaqRWkrota6b6Ol1boUt7Y6to7adirOTJ12qk7HarW4VK17Xaq1bmBVaovSgIiySZAtEkNYhCAQsjzzxzkXLiG5OSF3yU2+79frvu49v3uWJzfhPvyW8/uZuyMiIpJsOZkOQEREWiYlGBERSQklGBERSQklGBERSQklGBERSQklGBERSYmUJRgzG2hmb5rZYjNbaGbXhOU9zWyGmS0Ln3vEHXOTmRWZ2VIzOzGu/Agz+zB8704zs1TFLSIiyZHKGkwV8EN3PxQ4CrjazEYANwJvuPsw4I1wm/C984GRwEnAb80sNzzXPcBUYFj4OCmFcYuISBKkLMG4e4m7zwtflwOLgQHA6cDD4W4PA2eEr08HnnT3CndfARQB482sP9DV3Wd7cFfoI3HHiIhIM5WWPhgzGwKMBd4D+rl7CQRJCOgb7jYAWBN3WHFYNiB8XbtcRESasTapvoCZdQaeBa519y0Juk/qesMTlNd1rakETWl06tTpiEMOOaTxAYuItGJz585d7+59knGulCYYM8sjSC6PuftzYXGpmfV395Kw+WtdWF4MDIw7PB9YG5bn11G+F3efDkwHKCgo8MLCwqT9LCIirYGZrUrWuVI5isyAB4DF7n5H3FsvAheFry8CXogrP9/M2pnZUILO/DlhM1q5mR0VnvPCuGNERKSZSmUNZgLwbeBDM5sflv0Y+AXwtJldCqwGzgVw94Vm9jSwiGAE2tXuXh0edyXwENABeCV8iIhIM2Ytdbp+NZGJiDSemc1194JknCvlnfwiIulWWVlJcXExO3bsyHQozVb79u3Jz88nLy8vZddQghGRFqe4uJguXbowZMgQNPHH3tydDRs2UFxczNChQ1N2Hc1FJiItzo4dO+jVq5eSSz3MjF69eqW8hqcEIyItkpJLYun4fBpMMGbWz8weMLNXwu0R4QgwERFJgyFDhrB+/frI+991110cdNBBmFmjjku2KDWYh4DXgP3D7Y+Ba1MUj4iINNGECROYOXMmgwcPzmgcURJMb3d/GqgBcPcqoDrxISIirdfKlSs55JBDuOyyyxg1ahRTpkxh5syZTJgwgWHDhjFnzhwANm7cyBlnnMHo0aM56qijWLBgAQAbNmzghBNOYOzYsVx++eXE307y6KOPMn78eMaMGcPll19OdfXeX8djx45lyJAhaflZE4mSYL4ws16E83+Z2VHA5pRGJSKS5YqKirjmmmtYsGABS5Ys4fHHH+edd97htttu49ZbbwXg5ptvZuzYsSxYsIBbb72VCy+8EIBbbrmFY445hvfff5/TTjuN1atXA7B48WKeeuop/v73vzN//nxyc3N57LHHMvYzNiTKMOXrCKZxOdDM/g70Ac5JaVQiImlWVl7BvNWbGDeoB326tGvy+YYOHcphhx0GwMiRI5k0aRJmxmGHHcbKlSsBeOedd3j22WcBOO6449iwYQObN29m1qxZPPdcMH3jN77xDXr0CNZlfOONN5g7dy5HHnkkANu3b6dv3740Vw0mGHefZ2ZfAYYTzGy81N0rUx6ZiEgazVu9iZmLSgE4ceR+TT5fu3a7k1ROTs6u7ZycHKqqqgCoayaV2OiuukZ5uTsXXXQR//3f/93k+NKh3iYyMzsr9gBOI0gwBwOnhmUiIi3GuEE9OH5EP8YN6tHwzkkyceLEXU1cb731Fr1796Zr1657lL/yyits2rQJgEmTJvHMM8+wbl0wCf3GjRtZtSppkx8nXaI+mFPDx6UEsyJPCR/3A99KfWgiIunTp0s7Thy5X1Kax6KaNm0ahYWFjB49mhtvvJGHHw4W+7355puZNWsW48aN4/XXX2fQoEEAjBgxgv/6r//ihBNOYPTo0UyePJmSkpK9znvnnXeSn59PcXExo0eP5rLLLkvbzxSvwckuzewl4LuxVSjDNVzudvdmXYvRZJcirdfixYs59NBDMx1Gs1fX55TMyS6jjCIbEksuoVKCpjIREZF6RRlF9paZvQY8QTBU+XzgzZRGJSIiWS/KKLLvmdmZwMSwaLq7P5/asEREJNtFna7/HwSrTDowJ3XhiIhISxFlssvzCJLKOcB5wHtmphstRUQkoSg1mJ8AR7r7OgAz6wPMBJ5JdJCZPQicAqxz91Fh2VME99MAdAc+d/cxZjYEWAwsDd97192vCI85gmDCzQ7Ay8A13lLXeRYRaUGijCLLiSWX0IaIxz0EnBRf4O7/4u5j3H0M8CzwXNzby2PvxZJL6B5gKjAsfOxxThGRlq6x0/VPmTKF4cOHM2rUKC655BIqKzMz+UqURPGqmb1mZt8xs+8AfwFeaeggd58FbKzrPQvmQDiPYGRavcJ7brq6++yw1vIIcEaEmEVEWq0pU6awZMkSPvzwQ7Zv387999+fkTgaTDDufj0wHRgNHE4wiuzfmnjdY4FSd18WVzbUzN43s7fN7NiwbABQHLdPcVgmItJsZXq6/pNPPhkzw8wYP348xcXFe+2TDpGWTHb3Z4FpwH8Cb5tZzyZe9wL2rL2UAIPcfSzB7M2Pm1lXgsk19wqnvpOa2VQzKzSzwrKysiaGKCKy75rDdP2VlZX84Q9/4KSTMtOz0GAnv5ldDvwHsJ1g0TEj+JI/YF8uaGZtgLOAI2Jl7l4BVISv55rZcoLZAoqB/LjD84G19Z3b3acT1LYoKCjQQAARiW7rOlgzBwaOh85NnwK/OUzXf9VVVzFx4kSOPfbYevdJpSijyH4EjHT3ZC3sfDywxN131dnCkWkb3b3azA4g6Mz/xN03mll5uMjZe8CFwG+SFIeIyG5r5sDSl4PXh57S5NNlerr+W265hbKyMn73u9/tU/zJEKWJbDmwrbEnNrMngNnAcDMrNrNLw7fOZ+/O/YnAAjP7gGD48xXuHhsgcCXBDM5FYSwNDjAQEWm0geNh+MnBc5qkarr++++/n9dee40nnniCnJxIPSEpEaUGcxPwDzN7j7AZC8Ddf5DoIHe/oJ7y79RR9izBsOW69i8ERkWIU0Rk33Xum5SaS2NMmzaNiy++mNGjR9OxY8c9puu/4IILGDduHF/5ylfqnK6/pqaGvLw87r77bgYPHrzHea+44goGDx7M0UcfDcBZZ53Fz372s7T+bBBtuv45wDvAhwR9MAC4+8OpDa1pNF2/SOul6fqjSfV0/VFqMFXufl0yLiYiIq1HlMa5N8Phv/3NrGfskfLIREQkq0WpwXwzfL4prmyfhymLiEjrEGU9mKHpCEREJJncvc6hvhJIx5zBmRu/JiKSIu3bt2fDhg1p+RLNRu7Ohg0baN++fUqvE3XBMRGRrJGfn09xcTGaMqp+7du3Jz8/v+EdmyBhgglnPc539zUpjUJEJIny8vIYOlSt+5mWsIksnCL/T+kJRUREWpIofTDvmtmRKY9ERERalCh9MF8DrjCzlcAXhLMpu/voVAYmIiLZLUqC+XrKoxARkRYnyoqWq4CBwHHh621RjhMRkdatwURhZjcDN7D7Tv484NFUBiUiItkvSk3kTOA0gv4X3H0t0CWVQYmISPaLkmB2hsOVHcDMOqU2JBERaaqy8gpeW/gZZeUVDe+cIlESzNNm9jugu5l9F5gJ3JfasEREpCnmrd7EzEWlzFu9KWMxRJns8jYzmwxsAYYDP3P3GSmPTERE9tm4QT32eM6ESHORhQlFSUVEJEv06dKOE0ful9EY6m0iM7NyM9tS36OhE5vZg2a2zsw+iiubZmafmtn88HFy3Hs3mVmRmS01sxPjyo8wsw/D9+40zb8tIpIV6q3BuHsXADP7D+Az4A8Ed/FPIdoosoeAu4BHapX/r7vfFl9gZiOA84GRwP7ATDM72N2rgXuAqcC7wMvAScArEa4vIiIZFKWT/0R3/627l7v7Fne/Bzi7oYPcfRawMWIcpwNPunuFu68AioDxZtYf6Orus8ORbI8AZ0Q8p4iIZFCUBFNtZlPMLNfMcsxsClDdhGt+z8wWhE1osd6nAUD8kgDFYdmA8HXt8jqZ2VQzKzSzQq0DISKSWVESzDeB84BSYB1wbli2L+4BDgTGACXA7WF5Xf0qnqC8Tu4+3d0L3L2gT58++xiiiIgkQ5RhyisJmrCazN1LY6/N7D7gpXCzmGC+s5h8YG1Ynl9HuYiINHNR5iLLN7PnwxFhpWb2rJnt0zqbYZ9KzJlAbITZi8D5ZtbOzIYCw4A57l4ClJvZUeHosQuBF/bl2iIikl5R7oP5PfA4QdMYwLfCssmJDjKzJ4CvAr3NrBi4GfiqmY0haOZaCVwO4O4LzexpYBFQBVwdjiADuJJgRFoHgtFjGkEmIpIFLBiclWAHs/nuPqahsuamoKDACwsLMx2GiEhWMbO57l6QjHNF6eRfb2bfCkeR5ZrZt4ANybi4iIi0XFESzCUEo8g+Ixj5dU5YJiIiUq8oo8hWE6wHIyIiElmUUWQPm1n3uO0eZvZgSqMSEZGsF6WJbLS7fx7bcPdNwNiURSQiIi1ClASTEzelC2bWk4jT/IuISOsVJVHcDvzDzJ4Jt88Ffp66kEREpCWI0sn/iJnNBb5GMDfYWe6+KOWRiYhIVova1LUE2BTb38wGhaPLRERE6tRggjGz7xNM81JKME2/EUz1Mjq1oYmISDaLUoO5Bhju7rp7X0REIosyimwNsDnVgYiISMsSpQbzCfCWmf0FqIgVuvsdKYtKRESyXpQEszp8tA0fIiIiDYoyTPkWADPr5O5fpD4kERFpCaLMRXa0mS0CFofbh5vZb1MemYhIK1dWXsFrCz+jrLyi4Z2boSid/L8GTiRcA8bdPwAmpjAmEZFWr6y8ggffWcGfP1jLvNWbMh3OPomSYHD3NbWKquvcUUREkmLe6k0Ub9rGwB4dGTeoR8MHNENROvnXmNmXATeztsAPCJvLEgmn9D8FWOfuo8KyXwGnAjuB5cDF7v65mQ0Jz7k0PPxdd78iPOYI4CGgA/AycI03tM6ziEiWiyWVcYN60KdLuwxHs2+i1GCuAK4GBgCfAmPC7YY8BJxUq2wGMMrdRwMfAzfFvbfc3ceEjyviyu8BpgLDwkftc4qItDh9urTjxJH7ZW1ygWijyNYDUxp7YnefFdZM4stej9t8l2D55XqZWX+gq7vPDrcfAc4AXmlsPCIikl5RRpHlm9nzZrbOzErN7Fkzy0/CtS9hz0Qx1MzeN7O3zezYsGwAUBy3T3FYVl+sU82s0MwKy8rKkhCiiIjsqyhNZL8HXgT2J/hy/3NYts/M7CdAFfBYWFQCDHL3scB1wONm1pVgYs3a6u1/cffp7l7g7gV9+vRpSogiItJEURJMH3f/vbtXhY+HgH3+9jaziwg6/6fEOuvdvSI2maa7zyUYAHAwQY0lvraUD6zd12uLiEj6REkw683sW2aWGz6+RXhPTGOZ2UnADcBp7r4trryPmeWGrw8g6Mz/xN1LgHIzO8rMDLgQeGFfri0iIukVJcFcApwHfEbQlHUOcHFDB5nZE8BsYLiZFZvZpcBdQBdghpnNN7N7w90nAgvM7APgGeAKd98YvnclcD9QRFCzUQe/iEgWiHIfzEB3Py2+wMwmEEyAWS93v6CO4gfq2fdZ4Nl63isERkWIU0REmpEoNZjfRCwTERHZpd4ajJkdDXwZ6GNm18W91RXITXVgIiLZqKy8greWrgPgq8P7ZvWNkk2VqImsLdA53KdLXPkWGrhBUkSktZq3ehPPzfsUM6drhzxOHLlfpkPKmHoTjLu/DbxtZg+5+6o0xiQikrXGDerBlu2Vu163ZlE6+R8ys71ubnT341IQj4hIVuvTpR3nFgzMdBjNQpQE86O41+2BswnuwhcREalXlMku59Yq+ruZvZ2ieEREpIVoMMGYWc+4zRzgCKD19lqJSKtSVl7BvNWbsnpdlkyJ0kQ2l2CCSSNoGlsBXJrKoEREMi2WWLZsr2TOimBikdY8ImxfRGkiG5qOQEREmpN5qzcxc1Ep44f25PgR/Vr9iLB9EaUGg5mNAkYQdPID4O6PpCooEZFMawlLFmdalD6Ym4GvEiSYl4GvA+8ASjAi0mLFliyWfRdlLrJzgEnAZ+5+MXA4oHQuIiIJRUkw2929BqgKV5lcBxyQ2rBERCTbRemDKTSz7sB9BCPKtgJzUhmUiIhkvyijyK4KX95rZq8CXd19QWrDEhGRbBdpFFmMu69MURwiItLCROmDERERabSUJRgze9DM1pnZR3FlPc1shpktC597xL13k5kVmdlSMzsxrvwIM/swfO9OM7NUxSwiIsnTYIIJk0LtR16Ecz8EnFSr7EbgDXcfBrwRbmNmI4DzgZHhMb81s9iqmfcAU4Fh4aP2OUVEpBmKUoOZB5QBHwPLwtcrzGyemR1R30HuPgvYWKv4dODh8PXDwBlx5U+6e4W7rwCKgPFm1p9gUMFsd3eCmzvPQETSa+s6WPxS8FzXtkgdoiSYV4GT3b23u/ciuJP/aeAq4LeNvF4/dy8BCJ/7huUDgDVx+xWHZQPC17XL62RmU82s0MwKy8rKGhmaiNRrzRxY+nLwXNe2SB2ijCIrcPcrYhvu/rqZ3eru15lZsu7or6tfxROU18ndpwPTAQoKCurdT0TqsXUdLJsRvB42GTqH/wccOD7xs0gdoiSYjWZ2A/BkuP0vwKawj6SmkdcrNbP+7l4SNn/F6tfFQPwao/nA2rA8v45yEUmFZTNg9l3Qrhu07waHnhKUd+67+3Vd2yJ1iNJE9k2CL/Y/AS8Ag8KyXOC8Rl7vReCi8PVF4fli5eebWTszG0rQmT8nbEYrN7OjwtFjF8YdIyKp0K4r7HeYaifSZFHu5F8PfL+et4vqO87MniCYhbm3mRUDNwO/AJ42s0uB1cC54TUWmtnTwCKCRc2udvfq8FRXEoxI6wC8Ej5EJBWGTQ5qLgPH724eE9lHFgzOSrCD2cHAj4AhxCUkdz8upZE1UUFBgRcWFmY6DBFJMy1x3DRmNtfdC5Jxrih9MH8E7gXuB6ob2FdEJO3ik0psJUrQEseZFiXBVLn7PSmPREQkgUQ1k/ikEr8SpWRWlATzZzO7CngeqIgVunvtmyhFJBuULoIFT8Po86DfiExHE1mimknt5Y1Vc2keoiSY2Kiv6+PKHC06JpKdFjwNi8LBmJOnZTSUxkhUM1FSaZ6ijCIbmo5ARCRNRp+353OWUBLJPvUmGDM7zt3/amZn1fW+uz+XurBEJGX6jciqmotkr0Q1mK8AfwVOreM9B5RgRESkXvUmGHe/OXy+OH3hiIhIS5Goiey6RAe6+x3JD0dEWhPdFNmyJWoi6xI+DweOJJgvDIIms1mpDEpEWr6y8goefGcFxZu2AbopsiVK1ER2C4CZvQ6Mc/fycHsawd39IiL7bN7qTazZtI2BPTrqpsgWKsp9MIOAnXHbOwnmJROR5mrrumAxsGY8aWXtmyOl5YmSYP4AzDGz5wlGj51JsHSxiDRXsRUnodmu26L7Wlq+KDda/tzMXgWOCYsudvf3UxuWiDSJVpyUZiBKDQZgPlAS29/MBrn76lQFJSJN1ExWnNQosdatwQRjZt8nWCyslGC6fiNoKhud2tBEJBuVlVfw1tJ1u7bnrAjmxVVzWOsTpQZzDTDc3TekOhgR2UfNpFM/NvT4nys3kJeby1njBnD8iH4aJdZKRUkwa4DNqQ5ERJqgmXTqx4Yej9y/G6MGdOOrw/uqaawVi5JgPgHeMrO/sOd6MPt0J7+ZDQeeiis6APgZ0B34LlAWlv/Y3V8Oj7kJuJSgie4H7v7avlxbpMVqJp36Gnos8aIkmNXho234aBJ3XwqMATCzXOBTgsXMLgb+191vi9/fzEYA5wMjgf2BmWZ2sLtr+WaRmDR06kfpsNfQY4kXZZhy7I7+LsGmb03i9ScBy919lZnVt8/pwJPuXgGsMLMiYDwwO4lxiEgDtNa9NFaUUWSjCG627BlurwcudPeFSbj++cATcdvfM7MLgULgh+6+CRgAvBu3T3FYVlesU4GpAIMGDUpCeCKtS/wIsNr9J1rrXhorJ8I+04Hr3H2wuw8Gfgjc19QLm1lb4DR2z2t2D3AgQfNZCXB7bNc6Dve6zunu0929wN0L+vTp09QQRVq8svIK/li4hj8WrtnVBPbcvE95/v1i5q3etMe+seYv9a1IVFH6YDq5+5uxDXd/y8w6JeHaXwfmuXtpeN7S2Btmdh/wUrhZDAyMOy4fWJuE64u0CrHEMbhnR1Zt3LZHH0osoZg5XTvkMW5QD7ZsrwRUU5GmizSKzMz+naCZDOBbwIokXPsC4prHzKy/u5eEm2cCH4WvXwQeN7M7CDr5hwFzknB9kRapdkL5dOM2Zixex8H9OrNtZzA2JtaHUjuh9OnSjnMLBtZ7bpHGiJJgLgFuIVgi2QjWgmnSKpdm1hGYDFweV/w/ZjaGoPlrZew9d19oZk8Di4Aq4GqNIJNWJcFNlHWN7Ip1xvfu3I71Wyvo2DYXM2dwr44MqDU1vhKKpFKUUWSbgB+YWTegJrYuTFO4+zagV62ybyfY/+fAz5t6XZGsEkssOzbDqr8HZXFDkWN3za+ptWBXLIHEajB1NY2JpEOUUWRHAg8SrnBpZpuBS9x9bopjE2mdaieWwRNg+Ml73UQ5b/UmiutYsCv+XpRD+nfd41kknaI0kT0AXOXufwMws2OA36PJLkWSb+s6mP1b+HwVHHT87sQS1zQW38dyyuH7q2YizVaUBFMeSy4A7v6OmTW5mUxE4sTXWj5fDd0Hw7DJdU5cGetjOX5EP93wKM1alAQzx8x+RzDiy4F/IZibbByAu89LYXwirUNsssrBE2DUWQlnRdYNj5ItoiSYMeHzzbXKv0yQcI5LZkAirULtkWHxk1XWk1jiR4yp5iLZIMoosq+lIxCRVqX29PoRJqvUXGCSbRqcKsbMrjGzrha438zmmdkJ6QhOpMXZug4WvwQ9D6hzZFhdysoreG3hZwzu2VGLd0lWiTIX2SXuvgU4AehLcJPlL1IalUhLFBsh9tGzsPGT3TWXBsRqLqs2btNcYJJVovTBxCabPBn4vbt/YAnm1heReqyZs3uEWCMWBlOnvmSrKAlmrpm9DgwFbgrXhalJbVgiLVCEjnzYe/oXLeIl2SpKgrmUYCTZJ+6+zcx60cS5yERapQgd+UtKtvDrmcuorgn+D6fEItksSoJxYARwCvAfQCegfSqDEmlNYot8le+oZNbH6/m4tJzR+d3VJCZZL0qC+S1Bk9hxBAmmHHgWODKFcYlkt9JFsOBpGH0e9Buxx1tLSrbwwvy1TBzWm8UlW/hb0Xo2bq1ge1UNHfJyGZ3fnWuPH6bOfMl6URLMl9x9nJm9D8HsyuFqlCJSl63r4M1boWRBsD152q63lpRs4abnPqR0yw5WrP+Cleu/oGzrDkbnd+fYYb3p0j5vr6WKRbJVlARTaWa5hMsUm1kf1MkvrVH83fcAHz0Hn30I/UbBYWfv7rhfMwe8BvqPDmow7O64/0fRekq37KBf1/ZcdPRgFpdsYdXGbVwwfpBmPJYWJ0qCuRN4HuhrZj8HzgF+mtKoRJqj+LvvAeY9DFs+hbXzoPvA3R34tUaLxdZtKd60jbGDunP6mAGcPmZ/DunflaMP6p3+n0MkTRImGDPLIVge+d+ASQT3xJzh7ovTEJtI8xKfOADGXbS7BhN/X0vcaLFYcilaV85Bfbtw6uED1PwlrUbCBOPuNWZ2u7sfDSxJU0wizVPtYcZHXZFw99rJ5ZJjhiq5SKsSZaqY183s7GTevW9mK83sQzObb2aFYVlPM5thZsvC5x5x+99kZkVmttTMTkxWHNKKxeYEK10UPG9dl9TTzy5az9RHCpm/epOSi7RaUfpgriO496XKzHYQNJO5uze1R/Jr7r4+bvtG4A13/4WZ3Rhu32BmI4DzgZHA/sBMMzvY3aubeH1pjbaug2UzYO182LY+mLblizC5NHATZFRl5RX8z2tLWfLZFg7Zr6uSi7RaUfpgTnL3v6chltOBr4avHwbeAm4Iy5909wpghZkVAeOB2WmISVqaNXNgwZNQtRMGHR2M8tr4SaPmBqvLU++t5t5Zy7li4oF079yWru3bcMh+Xfm3E4cruUirFaUP5jbg6CRf1wma3hz4nbtPB/q5e0l43RIzi03WNAB4N+7Y4rBMpPEGjg+WJYbdSxLXuhGyMWJ34f/qtSWs31bJvbOW8/QVXwbYNZeYSGsVpYnsdTM7G3jO3T1J153g7mvDJDLDzBINIKir76fOOMxsKjAVYNCgQU2PUrJT7dUi43XuC2On7POpy8oreOK9VfxtWRn9u3VgzMDuzFi8jm4d25KTY1wx8UBNTikSakwfTLWZbScJfTDuvjZ8XmdmzxM0eZWaWf+w9tIfiPW6FgMD4w7PB9bWc97pwHSAgoKCZCVDyTa1V4tMglhN5Z8rNvDaolLKt1fRNm8zPTq15axxQYVad+CL7CnKksldknlBM+sE5Lh7efj6BII5zl4ELiJYzOwi4IXwkBeBx83sDoJO/mHAnGTGJC1M7ftVmqCsvII/f/Apsz5ez6ZtFXyxo5ou7drQp3NbRvTvpjvwRRKIUoPBzE4DJoabb7n7S024Zj/g+XDUcxvgcXd/1cz+CTxtZpcCq4FzAdx9oZk9DSwCqoCrNYJMEoowLX5dZhet5+HZqzj98P3ZurNqV/lThWtYX76TwwZ04/TDe9Olg+YLE4nCGupWMbNfEMyc/FhYdAEw191vTHFsTVJQUOCFhYWZDkOauVjT14drPueZeWvYUeUM69uZnp3aYeacOTaf8h2VrNqg+cKkdTCzue5ekIxzRanBnAyMcfea8OIPA+8T3KcikpVmF63njhlLKSuvoKrG2bitkm2VwX+2BvfswOSR/QH1q4g0RaQmMqA7sDF83S01oYhElGiUWARLSrbw4+c/ZOWGbQB0bd+GIwf34NNN2zigbxeum3ywaioiSRAlwfw38L6ZvUkwgmwicFNKoxKpy9Z1UPgQLPkLdA5nIY7Y11JWXsGf53/Kqo3b2L6zim07q+nbuS39u7fjuEP344Lxg1VTEUmyKKPInjCztwj6YQy4wd0/S3VgInvYug5m/xY+eBK2b4ScwyKPEptdtJ6fv7yYks3bwWHSof04a1z+rinzRSQ1GkwwZnYm8Fd3fzHc7m5mZ7j7n1IdnMiu5rDNxbD6XRhwBFTvhAnXNNg8Flvk68k5q/m4tJwOebmMHdSDS44ZqsQikgZRmshudvfnYxvu/rmZ3Qz8KWVRicTEbprM6wS5eXDIyQ3eiT+7aD2/fHURazbuoF1eDscf0o8JB/Vm7MDuXPAlNYWJpEuUBFPXlP5RBweI7Lut64J5wwZPgP3H1jsp5eyi9fxu1ifk5cDCknJyc6B40w4cyAUsx/j9xU2/6VJEGidKoigM76K/m2AOsO8Dc1MalbROsan0K8qhXRe27KhkxdzXeX7b4XzadxjL1nVkZ+UHbN5RSX73jlw7eTg5ucaTc1Yz+5P1VFY5NUC7XBjcqwPV1XBo/85cMF7z0olkQpQE833g34Gnwu3XgZ+mLCJpXVa8A3//P+gxGDatgm3r2VK+lWVftOOd9sdR9MWhvFsxkA0by6iJO2xp2RdM/9snHNS3M2MHdsedXTWYSYf05QfHH6ymMJEMizKK7At0U6U0VekimH03VFZA/hFw2NlBJ/1798KKWbCqDdVt2rOy3XCe2jyK9VXt+ceOQ9hv/3x6VjpjenRg2bqt7Kys3lWDmXrsAeTkGuMG9eAHxx+c6Z9QRGpRX4qkVqzZ64OnYM17VDqsWPgu983YyCe9v8rwHUdzTu5nbG4/gNzcXO7d8mXmVPWnXZscJhzYm+tOGK4RXyJZSglGUqd0Ebx5K2xZy7YtG9hW3YFP6MdLOwt4s2YoG1Z/zjyG8CT/SvvtOfTv3p6RB3bjmB1VXD7xAI4+qHemfwIRaYJ6E4yZ/dLdbzCzc939j+kMSlqAeY/Caz8BM4o7j+T3myfyeXV75uaNY0NedzrmtWFcz45U19TQrUMe3TrkMWZQD049fID6TkRaiEQ1mJPN7KcE08IowUh0pYuC5FLxORV5Xbiy9FQ+rBxIjsEpw/fj308dpSQi0gokSjCvAuuBTma2hXAlS5KwoqW0UKWLgulcPp1HFVCZ24V72lzMkppBtG/jnHfEIL6v0V0irUa9CcbdrweuN7MX3P30NMYk2WDRn2HWbdBvJLTtBCNOh/fupfqTt6iqqmF1m4HcWH0ZZXYgo/Zvx7+dOFx9KiKtTJRhyqebWT+CyS4B3nP3stSGJc3ZS396ii/Pv5aubKPis4V8QSfWLFzMzsqdUDmYYu/Do5UnsbB6fw7p2YbpFxao1iLSCkWZ7PJc4DbgLYLmsd+Y2fXu/kyKY5Nmoqy8gideeoVeS59gE92YUP0e7WwnW7w9r1R/iR3Wjjc2jaezfcG8moPZQDcG9+rAMb06c/nEA5RcRFqpKMOUfwoc6e7rAMysDzAT2KcEY2YDgUeA/YAaYLq7/5+ZTQO+C8RqRz9295fDY24CLgWqgR+4+2v7cm1pnCUlW3jwnU/YUPopZ5f+hgJbyg7asolOLPbBvNR7Km/sGAYOhw/sTvHGL+hVWc3QDm25brKaxERau0iTXcaSS2gDdU+AGVUV8EN3n2dmXYC5ZjYjfO9/3f22+J3NbARwPjAS2B+YaWYHu3t1E2KQBO5/ezl3v72cYTWr+U7VY4yyVWzM7c5CP5CFNpzqTv2ZcNIFTDvsEKZlOlgRabaiJJhXzew14Ilw+1+Al/f1gu5eApSEr8vNbDEwIMEhpwNPunsFsMLMioDxwOx9jUH29tR7q/m/vy6jTa6xeuN2JjOHaXkP0StnM3k5Tuf2ncj/zqN8bb+BmQ5VRLJElE7+683sLOAYgj6Y6fHrwzSFmQ0BxgLvAROA75nZhUAhQS1nE0HyeTfusGISJyRphFcWlHDXm8tYuX4rX1Q6vdnM5JyPuSr3T/TN3cIOOlLdYyA9J/8UlFxEpBEiTRXj7s8BzyXzwmbWGXgWuNbdt5jZPcB/Etxr85/A7cAlBEltr5DqOedUYCrAoEGaoj2RsvIK7nzjY554bzXX+KNclDeDd3IP4728L3F4zSJ2DjiWPFtM3jHXwohTMx2uiGShjMxFZmZ5BMnlsTB54e6lce/fB7wUbhYD8f91zgfW1nVed58OTAcoKCioMwkJfPTOS9TMnMZV1SX8qM1OOrCTNrlwQu77HHPCd+ja45hgYa8GliQWEUkk7QnGzAx4AFjs7nfElfcP+2cAzgQ+Cl+/CDweLnq2PzAMmJPGkLNeWXkF7z77awpW3stOjCHVn9PeqskJ64aeE1QJ2xx8El1Hf12JRUSSItFkl13dfUs97w1y99X7eM0JwLeBD81sflj2Y+ACMxtD8F23ErgcwN0XmtnTwCKCEWhXawRZBKWL4PV/Z/unH7BpR3tOqF5DXqyx0WC757IttyNdrYp2vYbAN26DocdkMmIRaWHMve6WJDOb5+7jwtdvuPukut5rrgoKCrywsDDTYaRX6SJ48+dQ8iFfbCsnr3IjbWp2TyBXCXyW04uc6hoWDbuaEy+8PsMBi0hzY2Zz3b0gGedK1EQW37neM8F7kia/fn0pd71ZRJVDLsFdp/vnbubb9jKTbA592EJX2w5Ae2AHuZR7W9bW9KRfuyoqj7mewcd9F9izU0tEJBUSJRiv53Vd25JCS0q2cNWjhXyyIUgeF/IXvp/3PDs9l262nTZWRR67s36FwxIfwoyaAp6umcSZxxzOT04ZmbH4RaR1SpRg+prZdQTfW7HXhNt9Uh6ZMLtoPTc++wGrNu0A4Ic8yiV5r9KWGnKNXRnFga3elg3ejXY5Vfy68hxezjuOggN78ejJh2rJYRHJiEQJ5j6gSx2vAe5PWUQCBM1hd/61iBpgPIv4WZvfc6h9iuUG7+cAtO0KXgPd8un2jdvpFnbS/yp8iIhkUqIE84C7F9f1hpnpzrskKyuv4JevLOalDz5lRzhG7ize5Md5j9KV7eQaWE4ssXSBwy+Ar1yvIcUi0mwlSjBvmNmJ7r4yvtDMLiaYYfnPqQyspbv/7eXcPmMJ26t2l/VmMw/k3M74vCJyCKaazo2vsbTvDhNvgC9flfZ4RUQaK1GC+Vdghpmd7O7LYNe0+d8EvpKO4FqSqx4t5OWPSmmXAxU1e743nVuZlBfeV2q7O+tzwzmrcwCO/gGc+J9pilZEpOkSLZn8splVAK+Y2RnAZQSrWk4MJ6GUBpSVV/DDp95nVtGGXWWH1yzix23+wAH2KZ3YXX2xWFbJiV8LIQf6DIeTdROkiGSfhFPFuPsbZvYdgtUs/wFMcvcdaYgr68V30sPuWkoV7L6jHmolFCC3A5x9nyaYFJGsl2iqmC8I7uUzoB0wCVgXziXm7q6xr3W4/+3l/NcrS4Cgk/6WvAfpSNBrb7nBB7kXawen3gHjvpW+QEVEUixRDeZjdx+btkiy1CsLSvjJ8x+wcXuQRH7KAyzOe4O24ftm7F1LIQeOuAROvT29wYqIpFHUO/mlljPv/hvvrwnmAn2Mn3FUXhHVBFO4WH1NYOqoF5FWJMqd/HWKn2q/tYiNBAO4kmd4LO+5XU1eZnEfZjj9fTDC2ODo7yuxiEirkyjB5AKd0cSWXPrQe3ywZDl/zrmO3+Rt31Veb00F4IjL1AQmIq1aogRT4u7/kbZImpHrnnqf595fy+38mjPy5gRLZIazSe6RbeOTSsc+cNGL0G9EmqMVEWmeok7X3+KdcufbfLR2K1fyDD/Pe45f5QXlVutT8JxY01dI/SoiInVKlGAmJXivxVhSsoV7fncHf6z+X9rWl1Riwx1yIbfHQXDN3LTGKCKSjRLdyb8xnYEk2+KSLdz/9nIu+8qBe5R/9X/eYOXGHVzIX/hp3mMMA+6gnqSSG9ZWBh0Dl/wlPYGLiLQQ9S6ZnO3G9M/zuZd1ZHNue8bteBCAhXyT9nm790mYVLoNgX/9IE3Riog0D+laMrlZMbOTgP8j+P6/391/kWj/NlaDGXSr3sHyvG/GnWfP/fZo/mrTEX5aktS4RURaq6xIMGaWC9wNTAaKgX+a2YvuvqjhY/cu26OmAjBtc9JiFRGRQFYkGGA8UOTunwCY2ZPA6UDCBFO79S+2tgrnParJJEVEUixbEswAYE3cdjHwpdo7mdlUYCpAu1wouG+rzy+tmbfX2W45LUVh7pPewPpMB9GAbIgRFGeyKc7kypY4hyfrRNmSYOq6J2ev0QnuPh2C+yLNrPD9z6qT0lGVSmZWmKwOtVTJhhhBcSab4kyubIozWefKaXiXZqEYGBi3nQ+szVAsIiISQbYkmH8Cw8xsqJm1Bc4HXsxwTCIikkBWNJG5e5WZfQ94jWDw14PuvrCBw6anPrKkyIY4syFGUJzJpjiTq9XF2WJvtBQRkczKliYyERHJMkowIiKSEi0uwZjZSWa21MyKzOzGDMcy0MzeNLPFZrbQzK4Jy6eZ2admNj98nBx3zE1h7EvN7MQ0xrrSzD4M4ykMy3qa2QwzWxY+98hknGY2PO4zm29mW8zs2kx/nmb2oJmtM7OP4soa/dmZ2RHh76DIzO40q2seiqTH+SszW2JmC8zseTPrHpYPMbPtcZ/pvRmOs9G/4wzF+VRcjCvNbH5YnsnPs77vodT/jbp7i3kQDABYDhwAtAU+AEZkMJ7+wLjwdRfgY2AEMA34UR37jwhjbgcMDX+W3DTFuhLoXavsf4Abw9c3Ar/MdJy1ftefAYMz/XkCE4FxwEdN+eyAOcDRBPd9vQJ8PQ1xngC0CV//Mi7OIfH71TpPJuJs9O84E3HWev924GfN4POs73so5X+jLa0Gs2tKGXffCcSmlMkIdy9x93nh63JgMcGsBPU5HXjS3SvcfQVQRPAzZcrpwMPh64eBM+LKMx3nJGC5u69KsE9a4nT3WUDt5S0a9dmZWX+gq7vP9uBf8iNxx6QsTnd/3d2rws13Ce4xq1em4kygWX2eMeH/7M8Dnkh0jjTFWd/3UMr/RltagqlrSplEX+hpY2ZDgLHAe2HR98JmiQfjqqaZjN+B181srgVT7gD0c/cSCP5Igb7NIM6Y89nzH29z+zwb+9kNCF/XLk+nSwj+Vxoz1MzeN7O3zezYsCyTcTbmd5zpz/NYoNTdl8WVZfzzrPU9lPK/0ZaWYCJNKZNuZtYZeBa41t23APcABwJjgBKCqjRkNv4J7j4O+DpwtZlNTLBvRj9nC262PQ34Y1jUHD/P+tQXU6Y/058AVcBjYVEJMMjdxwLXAY+bWVcyF2djf8eZ/t1fwJ7/Acr451nH91C9u9YTU6NjbWkJptlNKWNmeQS/1Mfc/TkAdy9192p3rwHuY3ezTcbid/e14fM64PkwptKwWhyryq/LdJyhrwPz3L0UmufnSeM/u2L2bJ5KW6xmdhFwCjAlbPogbB7ZEL6eS9AOf3Cm4tyH33EmP882wFnAU7GyTH+edX0PkYa/0ZaWYJrVlDJhO+wDwGJ3vyOuvH/cbmcCsVEoLwLnm1k7MxsKDCPoVEt1nJ3MrEvsNUHH70dhPBeFu10EvJDJOOPs8b/D5vZ5xl078mcXNlGUm9lR4d/NhXHHpIwFC/ndAJzm7tviyvtYsA4TZnZAGOcnGYyzUb/jTMUZOh5Y4u67mpMy+XnW9z1EOv5GkzlaoTk8gJMJRkksB36S4ViOIahCLgDmh4+TgT8AH4blLwL94475SRj7UpI8miRBnAcQjBr5AFgY+9yAXsAbwLLwuWcm4wyv2xHYAHSLK8vo50mQ7EqASoL/5V26L58dUEDwxbkcuItwpo0Ux1lE0N4e+/u8N9z37PBv4QNgHnBqhuNs9O84E3GG5Q8BV9TaN5OfZ33fQyn/G9VUMSIikhItrYlMRESaCSUYERFJCSUYERFJCSUYERFJCSUYERFJCSUYERFJCSUYkTQzszPM7D4ze8HMTsh0PCKpovtgRDIknLDxNne/NNOxiKSCajAimfNT4O5MByGSKm0yHYBIS2dmfwV6hpuHAN8mmHLjFQ/X6RBpiZRgRFLM3Y8DMLMrga8B+xNMiNjNzA5y93sTHS+SrdQHI5IGZnYhcA5wtrtXZjoekXRQDUYkxczsXGAKcLqSi7QmSjAiKWRmpwBXAae4+45MxyOSTmoiE0khM9sAbAS+CIt+4+4PZDAkkbRRghERkZTQfTAiIpISSjAiIpISSjAiIpISSjAiIpISSjAiIpISSjAiIpISSjAiIpISSjAiIpISSjAiIpIS/w81Pr+FqXGCYAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation coefficient 0.967711485775306\n"
     ]
    }
   ],
   "source": [
    "ke = 0.5 * np.einsum('z t x y u -> z t', ae_modes**2)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(A_data[:,0]**2, ke[0,:], alpha=0.4,label='mode 1',s=2)\n",
    "plt.scatter(A_data[:,1]**2, ke[1,:], alpha=0.4,label='mode 2',s=2)\n",
    "plt.xlabel('$z^2$')\n",
    "plt.ylabel('TKE of corresponding autoencoder mode')\n",
    "plt.xlim([0,2000])\n",
    "plt.ylim([0,2000])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('correlation coefficient', np.corrcoef(np.abs(A_data[:,0]**2), ke[0,:])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reproject autoencoder modes onto data pod modes\n",
    "lam_modes = mode_eval.equivalent_pca_energy(ae_modes,Q_POD_data)\n",
    "lam_modes_percent = lam_modes/lam_data\n",
    "\n",
    "x_axis = np.arange(1, Nz*Ny*Nu+1)\n",
    "\n",
    "plt.figure()\n",
    "for i in range(latent_dim):\n",
    "    plt.plot(x_axis,lam_modes[i,:],label='AE mode '+str(i+1),linestyle='--',marker='x')\n",
    "plt.plot(x_axis,lam_data,label='data')\n",
    "plt.xlim([0,10])\n",
    "plt.xticks(range(10))\n",
    "plt.ylim(bottom=0)\n",
    "plt.ylabel('equivalent PCA energy')\n",
    "plt.xlabel('POD mode')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "for i in range(latent_dim):\n",
    "    plt.plot(x_axis,lam_modes_percent[i,:],label='AE mode '+str(i+1),linestyle='--',marker='x')\n",
    "plt.xlim([0,10])\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(range(10))\n",
    "plt.legend()\n",
    "plt.xlabel('POD mode')\n",
    "plt.ylabel('% equivalent PCA energy')\n",
    "plt.title('precentage of POD modes captured')\n",
    "\n",
    "\n",
    "# Similarity between time coeffcient and latent variables\n",
    "mag_A = einsum('t x -> x',A_data**2)**0.5\n",
    "\n",
    "plt.figure()\n",
    "# plt.suptitle('cosine of the angle between the latent variables and POD time coefficient (as time series)')\n",
    "plt.xlabel('POD mode')\n",
    "plt.ylabel('cosine similarity')\n",
    "for i in range(latent_dim):\n",
    "    mag_z = np.sum(A_data[:,i]**2)**0.5\n",
    "    divisor = mag_A * mag_z\n",
    "    z_dot_A = A_data[:,[i]].T @ A_data\n",
    "    cos_angle = (z_dot_A / divisor).flatten()\n",
    "    plt.plot(x_axis, np.abs(cos_angle),label='AE mode '+str(i+1),linestyle='--',marker='x')\n",
    "plt.xlim([0,10])\n",
    "plt.xticks(range(10))\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:\n",
      "reconstruction by POD:    1.4777343273162842\n",
      "reconstructed by model:   1.4685799\n"
     ]
    }
   ],
   "source": [
    "c = []\n",
    "c.append(recons_data[:,:,:,0])\n",
    "c.append(recons_data[:,:,:,1])\n",
    "c = np.transpose(c,[1,2,3,0])\n",
    "\n",
    "print('mse:')\n",
    "print('reconstruction by POD:   ', mse(u_train[0,:,:,:,:],c).numpy())\n",
    "print('reconstructed by model:  ', mse(u_train[0,:,:,:,:],y).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('MD-CNN-AE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f85e8d5ca1c7d62daa514db97b690def00c9e189bf201b0a2c929de67d960fcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
