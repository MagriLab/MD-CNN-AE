{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras import backend as K\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# from MD_AE_tools.models.models import *\n",
    "from MD_AE_tools.models.models_no_bias import *\n",
    "import MD_AE_tools.mode_decomposition as md\n",
    "import MD_AE_tools.ae_mode_evaluation as mode_eval\n",
    "\n",
    "import myplot\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from numpy import einsum\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "import time\n",
    "import os\n",
    "import configparser\n",
    "import datetime\n",
    "import wandb\n",
    "\n",
    "# get system information\n",
    "config = configparser.ConfigParser()\n",
    "config.read('_system.ini')\n",
    "system_info = config['system_info']\n",
    "\n",
    "# use gpu\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[2], 'GPU')# use [] for cpu only, gpus[i] for the ith gpu\n",
    "        tf.config.set_logical_device_configuration(gpus[2],[tf.config.LogicalDeviceConfiguration(memory_limit=1024)]) # set hard memory limit\n",
    "        # tf.config.experimental.set_memory_growth(gpus[0], True) # allow memory growth\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "data_file = './data/PIV4_downsampled_by8.h5'\n",
    "Ntrain = 1632 # snapshots for training\n",
    "Nval = 550 # sanpshots for validation\n",
    "Ntest = 550\n",
    "\n",
    "# Boolean \n",
    "LATENT_STATE = True # save latent state\n",
    "SHUFFLE = True # shuffle before splitting into sets, test set is extracted before shuffling\n",
    "REMOVE_MEAN = True # train on fluctuating velocity\n",
    "\n",
    "## ae configuration\n",
    "lmb = 0.000 #1e-05 #regulariser\n",
    "drop_rate = 0.0\n",
    "features_layers = [32, 64, 128]\n",
    "latent_dim = 2\n",
    "act_fct = 'linear'\n",
    "resize_meth = 'bilinear'\n",
    "filter_window= (5,5)\n",
    "batch_norm = False\n",
    "\n",
    "## training\n",
    "nb_epoch = 3000\n",
    "batch_size = 200\n",
    "learning_rate = 0.001\n",
    "\n",
    "Nz = 24 # grid size\n",
    "Ny = 21\n",
    "Nu = 2\n",
    "Nt = 2732 # number of snapshots available\n",
    "D = 196.5 # mm diameter of bluff body\n",
    "U_inf = 15 # m/s freestream velocity\n",
    "f_piv = 720.0 # Hz PIV sampling frequency  \n",
    "dt = 1.0/f_piv \n",
    "Nx = [Ny, Nz]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================= READ DATA FROM FILE ================================\n",
    "hf = h5py.File('./data/ufluc_shuffle_1632.h5','r')\n",
    "u_all = np.array(hf.get('u_all'))\n",
    "u_train = np.array(hf.get('u_train'))\n",
    "u_val = np.array(hf.get('u_val'))\n",
    "u_test = np.array(hf.get('u_test'))\n",
    "u_mean_all = np.array(hf.get('u_mean_all'))\n",
    "u_mean_train = np.array(hf.get('u_mean_train'))\n",
    "u_mean_val = np.array(hf.get('u_mean_val'))\n",
    "u_mean_test = np.array(hf.get('u_mean_test'))\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating POD ...\n",
      "User has selected classic POD\n",
      "POD done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ym917/Codes/MD-CNN-AE/MD_AE_tools/mode_decomposition.py:220: RuntimeWarning: invalid value encountered in sqrt\n",
      "  normQ = (Q_POD.T @ Q_POD*self.w).real**0.5\n"
     ]
    }
   ],
   "source": [
    "# POD data\n",
    "x = einsum('t y z u -> y z t u',np.squeeze(u_train))\n",
    "X = np.vstack((x[:,:,:,0],x[:,:,:,1]))\n",
    "pod_data = md.POD(X,method='classic')\n",
    "Q_POD_data,lam_data = pod_data.get_modes\n",
    "Q_mean = pod_data.Q_mean\n",
    "recons_data = pod_data.reconstruct(latent_dim,shape=[2,Ny,Nz,u_train.shape[1]])\n",
    "recons_data = np.transpose(recons_data,[3,1,2,0])\n",
    "\n",
    "A_data = pod_data.get_time_coefficient\n",
    "pod_modes_t = []\n",
    "for i in range(latent_dim):\n",
    "    Q_add = pod_data.Phi[:,[i]] @ A_data[:,[i]].T\n",
    "    rebuildv = np.reshape(Q_add,[2,Ny,Nz,A_data.shape[0]])\n",
    "    pod_modes_t.append(rebuildv)\n",
    "pod_modes_t = np.array(pod_modes_t) # [latent_dim,velocity,Ny,Nz,time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating POD ...\n",
      "User has selected classic POD\n",
      "POD done.\n"
     ]
    }
   ],
   "source": [
    "# prepare validation data\n",
    "vy = u_val[0,:,:,:,0] + u_mean_val[:,:,0]\n",
    "vy = np.transpose(vy,[1,2,0])\n",
    "vz = u_val[0,:,:,:,1] + u_mean_val[:,:,1]\n",
    "vz = np.transpose(vz,[1,2,0])\n",
    "X = np.vstack((vz,vy))\n",
    "\n",
    "pod_val = md.POD(X,method='classic')\n",
    "Q_POD_val,lam_val = pod_val.get_modes\n",
    "A_val = pod_val.get_time_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(Nx,Nu,features_layers,latent_dim,filter_window,act_fct='tanh',batch_norm=batch_norm,drop_rate=drop_rate,lmb=lmb)\n",
    "layer_size = encoder.get_layer_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### single decoder\n",
    "\n",
    "decoders = []\n",
    "add_modes = []\n",
    "inn = Input(shape=(latent_dim))\n",
    "for i in range(latent_dim):\n",
    "    decoders.append(\n",
    "        Decoder(Nx=Nx,Nu=Nu,layer_size=layer_size,features_layers=features_layers,latent_dim=1,filter_window=filter_window,act_fct='tanh',batch_norm=batch_norm,drop_rate=drop_rate,lmb=lmb)\n",
    "        )\n",
    "out = decoders[0](inn)\n",
    "mdl = Model(inn,out)\n",
    "print(mdl.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 1)            0           ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 1)            0           ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " decoder_2 (Decoder)            (None, 21, 24, 2)    258752      ['lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " decoder_3 (Decoder)            (None, 21, 24, 2)    258752      ['lambda_3[0][0]']               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 21, 24, 2)    0           ['decoder_2[0][0]',              \n",
      "                                                                  'decoder_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 517,504\n",
      "Trainable params: 517,504\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# set up network\n",
    "# sum(decoders) = flow\n",
    "decoders = []\n",
    "add_modes = []\n",
    "inn = Input(shape=(latent_dim))\n",
    "for i in range(latent_dim):\n",
    "    decoders.append(\n",
    "        Decoder(Nx=Nx,Nu=Nu,layer_size=layer_size,features_layers=features_layers,latent_dim=1,filter_window=filter_window,act_fct='tanh',batch_norm=batch_norm,drop_rate=drop_rate,lmb=lmb)\n",
    "        )\n",
    "    x = Lambda(lambda x,i: x[:,i:i+1],arguments={'i':i})(inn)\n",
    "    x = decoders[i](x)\n",
    "    add_modes.append(x)\n",
    "    del x\n",
    "out = Add()(add_modes)\n",
    "mdl = Model(inn,out)\n",
    "print(mdl.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.compile(optimizer=Adam(learning_rate=learning_rate),loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training control\n",
    "pat = 200 # early stopping\n",
    "tempfn = './temp_decoder.h5'\n",
    "model_cb=ModelCheckpoint(tempfn, monitor='loss',save_best_only=True,verbose=1,save_weights_only=True)\n",
    "early_cb=EarlyStopping(monitor='loss', patience=pat,verbose=1)\n",
    "cb = [model_cb, early_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear history\n",
    "hist_train = []\n",
    "hist_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify input\n",
    "A_train_in = A_data[:,:2]\n",
    "A_val_in = A_val[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(A_train_in)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 2.0538\n",
      "Epoch 00001: loss improved from inf to 2.05378, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 2.0538 - val_loss: 1.6415\n",
      "Epoch 2/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.6335\n",
      "Epoch 00002: loss improved from 2.05378 to 1.63352, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.6335 - val_loss: 1.5523\n",
      "Epoch 3/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5731\n",
      "Epoch 00003: loss improved from 1.63352 to 1.57314, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.5731 - val_loss: 1.5291\n",
      "Epoch 4/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5430\n",
      "Epoch 00004: loss improved from 1.57314 to 1.54304, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.5430 - val_loss: 1.5119\n",
      "Epoch 5/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5255\n",
      "Epoch 00005: loss improved from 1.54304 to 1.52546, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.5255 - val_loss: 1.4935\n",
      "Epoch 6/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5106\n",
      "Epoch 00006: loss improved from 1.52546 to 1.51061, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.5106 - val_loss: 1.4866\n",
      "Epoch 7/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.5016\n",
      "Epoch 00007: loss improved from 1.51061 to 1.50158, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.5016 - val_loss: 1.4777\n",
      "Epoch 8/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4945\n",
      "Epoch 00008: loss improved from 1.50158 to 1.49452, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4945 - val_loss: 1.4772\n",
      "Epoch 9/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4967\n",
      "Epoch 00009: loss did not improve from 1.49452\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4967 - val_loss: 1.4768\n",
      "Epoch 10/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4931\n",
      "Epoch 00010: loss improved from 1.49452 to 1.49315, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4931 - val_loss: 1.4762\n",
      "Epoch 11/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4910\n",
      "Epoch 00011: loss improved from 1.49315 to 1.49097, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4910 - val_loss: 1.4807\n",
      "Epoch 12/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4923\n",
      "Epoch 00012: loss did not improve from 1.49097\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4923 - val_loss: 1.4774\n",
      "Epoch 13/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4909\n",
      "Epoch 00013: loss improved from 1.49097 to 1.49094, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4909 - val_loss: 1.4730\n",
      "Epoch 14/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4847\n",
      "Epoch 00014: loss improved from 1.49094 to 1.48467, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4847 - val_loss: 1.4712\n",
      "Epoch 15/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4841\n",
      "Epoch 00015: loss improved from 1.48467 to 1.48408, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4841 - val_loss: 1.4751\n",
      "Epoch 16/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4839\n",
      "Epoch 00016: loss improved from 1.48408 to 1.48388, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4839 - val_loss: 1.4748\n",
      "Epoch 17/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4847\n",
      "Epoch 00017: loss did not improve from 1.48388\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4847 - val_loss: 1.4750\n",
      "Epoch 18/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4855\n",
      "Epoch 00018: loss did not improve from 1.48388\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4855 - val_loss: 1.4757\n",
      "Epoch 19/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4838\n",
      "Epoch 00019: loss improved from 1.48388 to 1.48382, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4838 - val_loss: 1.4822\n",
      "Epoch 20/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4905\n",
      "Epoch 00020: loss did not improve from 1.48382\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4905 - val_loss: 1.4834\n",
      "Epoch 21/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4879\n",
      "Epoch 00021: loss did not improve from 1.48382\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4879 - val_loss: 1.4761\n",
      "Epoch 22/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4842\n",
      "Epoch 00022: loss did not improve from 1.48382\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4842 - val_loss: 1.4729\n",
      "Epoch 23/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4840\n",
      "Epoch 00023: loss did not improve from 1.48382\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4840 - val_loss: 1.4698\n",
      "Epoch 24/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4813\n",
      "Epoch 00024: loss improved from 1.48382 to 1.48129, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4813 - val_loss: 1.4666\n",
      "Epoch 25/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4787\n",
      "Epoch 00025: loss improved from 1.48129 to 1.47871, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4787 - val_loss: 1.4738\n",
      "Epoch 26/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4805\n",
      "Epoch 00026: loss did not improve from 1.47871\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4805 - val_loss: 1.4724\n",
      "Epoch 27/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4826\n",
      "Epoch 00027: loss did not improve from 1.47871\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4826 - val_loss: 1.4718\n",
      "Epoch 28/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4809\n",
      "Epoch 00028: loss did not improve from 1.47871\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4809 - val_loss: 1.4727\n",
      "Epoch 29/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4825\n",
      "Epoch 00029: loss did not improve from 1.47871\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4825 - val_loss: 1.4758\n",
      "Epoch 30/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4808\n",
      "Epoch 00030: loss did not improve from 1.47871\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4808 - val_loss: 1.4718\n",
      "Epoch 31/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4816\n",
      "Epoch 00031: loss did not improve from 1.47871\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4816 - val_loss: 1.4685\n",
      "Epoch 32/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4774\n",
      "Epoch 00032: loss improved from 1.47871 to 1.47736, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4774 - val_loss: 1.4716\n",
      "Epoch 33/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4781\n",
      "Epoch 00033: loss did not improve from 1.47736\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4781 - val_loss: 1.4731\n",
      "Epoch 34/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4758\n",
      "Epoch 00034: loss improved from 1.47736 to 1.47579, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4758 - val_loss: 1.4692\n",
      "Epoch 35/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4768\n",
      "Epoch 00035: loss did not improve from 1.47579\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4768 - val_loss: 1.4739\n",
      "Epoch 36/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4769\n",
      "Epoch 00036: loss did not improve from 1.47579\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4769 - val_loss: 1.4718\n",
      "Epoch 37/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4766\n",
      "Epoch 00037: loss did not improve from 1.47579\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4766 - val_loss: 1.4705\n",
      "Epoch 38/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4743\n",
      "Epoch 00038: loss improved from 1.47579 to 1.47431, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4743 - val_loss: 1.4717\n",
      "Epoch 39/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4762\n",
      "Epoch 00039: loss did not improve from 1.47431\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4762 - val_loss: 1.4721\n",
      "Epoch 40/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4766\n",
      "Epoch 00040: loss did not improve from 1.47431\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4766 - val_loss: 1.4724\n",
      "Epoch 41/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4736\n",
      "Epoch 00041: loss improved from 1.47431 to 1.47358, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4736 - val_loss: 1.4679\n",
      "Epoch 42/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4726\n",
      "Epoch 00042: loss improved from 1.47358 to 1.47263, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4726 - val_loss: 1.4659\n",
      "Epoch 43/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4750\n",
      "Epoch 00043: loss did not improve from 1.47263\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4750 - val_loss: 1.4772\n",
      "Epoch 44/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4746\n",
      "Epoch 00044: loss did not improve from 1.47263\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4746 - val_loss: 1.4736\n",
      "Epoch 45/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4764\n",
      "Epoch 00045: loss did not improve from 1.47263\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4764 - val_loss: 1.4757\n",
      "Epoch 46/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4755\n",
      "Epoch 00046: loss did not improve from 1.47263\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4755 - val_loss: 1.4673\n",
      "Epoch 47/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4715\n",
      "Epoch 00047: loss improved from 1.47263 to 1.47151, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4715 - val_loss: 1.4697\n",
      "Epoch 48/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4734\n",
      "Epoch 00048: loss did not improve from 1.47151\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4734 - val_loss: 1.4766\n",
      "Epoch 49/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4749\n",
      "Epoch 00049: loss did not improve from 1.47151\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4749 - val_loss: 1.4697\n",
      "Epoch 50/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4733\n",
      "Epoch 00050: loss did not improve from 1.47151\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4733 - val_loss: 1.4678\n",
      "Epoch 51/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4744\n",
      "Epoch 00051: loss did not improve from 1.47151\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4744 - val_loss: 1.4731\n",
      "Epoch 52/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4743\n",
      "Epoch 00052: loss did not improve from 1.47151\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4743 - val_loss: 1.4722\n",
      "Epoch 53/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4719\n",
      "Epoch 00053: loss did not improve from 1.47151\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4719 - val_loss: 1.4664\n",
      "Epoch 54/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4710\n",
      "Epoch 00054: loss improved from 1.47151 to 1.47105, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4710 - val_loss: 1.4660\n",
      "Epoch 55/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4706\n",
      "Epoch 00055: loss improved from 1.47105 to 1.47064, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4706 - val_loss: 1.4670\n",
      "Epoch 56/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4702\n",
      "Epoch 00056: loss improved from 1.47064 to 1.47022, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4702 - val_loss: 1.4723\n",
      "Epoch 57/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4729\n",
      "Epoch 00057: loss did not improve from 1.47022\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4729 - val_loss: 1.4688\n",
      "Epoch 58/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4717\n",
      "Epoch 00058: loss did not improve from 1.47022\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4717 - val_loss: 1.4721\n",
      "Epoch 59/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4708\n",
      "Epoch 00059: loss did not improve from 1.47022\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4708 - val_loss: 1.4690\n",
      "Epoch 60/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4720\n",
      "Epoch 00060: loss did not improve from 1.47022\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4720 - val_loss: 1.4680\n",
      "Epoch 61/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4715\n",
      "Epoch 00061: loss did not improve from 1.47022\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4715 - val_loss: 1.4695\n",
      "Epoch 62/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4730\n",
      "Epoch 00062: loss did not improve from 1.47022\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4730 - val_loss: 1.4697\n",
      "Epoch 63/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4717\n",
      "Epoch 00063: loss did not improve from 1.47022\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4717 - val_loss: 1.4709\n",
      "Epoch 64/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4681\n",
      "Epoch 00064: loss improved from 1.47022 to 1.46813, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4681 - val_loss: 1.4693\n",
      "Epoch 65/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4710\n",
      "Epoch 00065: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4710 - val_loss: 1.4686\n",
      "Epoch 66/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4726\n",
      "Epoch 00066: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4726 - val_loss: 1.4727\n",
      "Epoch 67/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4715\n",
      "Epoch 00067: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4715 - val_loss: 1.4775\n",
      "Epoch 68/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4743\n",
      "Epoch 00068: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4743 - val_loss: 1.4670\n",
      "Epoch 69/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4788\n",
      "Epoch 00069: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4788 - val_loss: 1.4734\n",
      "Epoch 70/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4698\n",
      "Epoch 00070: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4698 - val_loss: 1.4708\n",
      "Epoch 71/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4693\n",
      "Epoch 00071: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4693 - val_loss: 1.4692\n",
      "Epoch 72/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4686\n",
      "Epoch 00072: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4686 - val_loss: 1.4667\n",
      "Epoch 73/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4709\n",
      "Epoch 00073: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4709 - val_loss: 1.4699\n",
      "Epoch 74/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4741\n",
      "Epoch 00074: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4741 - val_loss: 1.4668\n",
      "Epoch 75/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4698\n",
      "Epoch 00075: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4698 - val_loss: 1.4739\n",
      "Epoch 76/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4726\n",
      "Epoch 00076: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4726 - val_loss: 1.4672\n",
      "Epoch 77/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4693\n",
      "Epoch 00077: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4693 - val_loss: 1.4724\n",
      "Epoch 78/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4699\n",
      "Epoch 00078: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4699 - val_loss: 1.4734\n",
      "Epoch 79/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4696\n",
      "Epoch 00079: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4696 - val_loss: 1.4679\n",
      "Epoch 80/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4716\n",
      "Epoch 00080: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4716 - val_loss: 1.4723\n",
      "Epoch 81/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4693\n",
      "Epoch 00081: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4693 - val_loss: 1.4729\n",
      "Epoch 82/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4684\n",
      "Epoch 00082: loss did not improve from 1.46813\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4684 - val_loss: 1.4697\n",
      "Epoch 83/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4680\n",
      "Epoch 00083: loss improved from 1.46813 to 1.46801, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4680 - val_loss: 1.4815\n",
      "Epoch 84/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4757\n",
      "Epoch 00084: loss did not improve from 1.46801\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4757 - val_loss: 1.4702\n",
      "Epoch 85/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4719\n",
      "Epoch 00085: loss did not improve from 1.46801\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4719 - val_loss: 1.4779\n",
      "Epoch 86/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4712\n",
      "Epoch 00086: loss did not improve from 1.46801\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4712 - val_loss: 1.4693\n",
      "Epoch 87/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4686\n",
      "Epoch 00087: loss did not improve from 1.46801\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4686 - val_loss: 1.4750\n",
      "Epoch 88/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4703\n",
      "Epoch 00088: loss did not improve from 1.46801\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4703 - val_loss: 1.4700\n",
      "Epoch 89/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4683\n",
      "Epoch 00089: loss did not improve from 1.46801\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4683 - val_loss: 1.4760\n",
      "Epoch 90/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4727\n",
      "Epoch 00090: loss did not improve from 1.46801\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4727 - val_loss: 1.4728\n",
      "Epoch 91/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4727\n",
      "Epoch 00091: loss did not improve from 1.46801\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4727 - val_loss: 1.4742\n",
      "Epoch 92/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4708\n",
      "Epoch 00092: loss did not improve from 1.46801\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4708 - val_loss: 1.4736\n",
      "Epoch 93/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4687\n",
      "Epoch 00093: loss did not improve from 1.46801\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4687 - val_loss: 1.4692\n",
      "Epoch 94/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4689\n",
      "Epoch 00094: loss did not improve from 1.46801\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4689 - val_loss: 1.4734\n",
      "Epoch 95/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4674\n",
      "Epoch 00095: loss improved from 1.46801 to 1.46744, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4674 - val_loss: 1.4693\n",
      "Epoch 96/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4685\n",
      "Epoch 00096: loss did not improve from 1.46744\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4685 - val_loss: 1.4712\n",
      "Epoch 97/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4675\n",
      "Epoch 00097: loss did not improve from 1.46744\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4675 - val_loss: 1.4709\n",
      "Epoch 98/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4678\n",
      "Epoch 00098: loss did not improve from 1.46744\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4678 - val_loss: 1.4698\n",
      "Epoch 99/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4668\n",
      "Epoch 00099: loss improved from 1.46744 to 1.46677, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4668 - val_loss: 1.4704\n",
      "Epoch 100/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4670\n",
      "Epoch 00100: loss did not improve from 1.46677\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4670 - val_loss: 1.4671\n",
      "Epoch 101/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4673\n",
      "Epoch 00101: loss did not improve from 1.46677\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4673 - val_loss: 1.4810\n",
      "Epoch 102/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4775\n",
      "Epoch 00102: loss did not improve from 1.46677\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4775 - val_loss: 1.4708\n",
      "Epoch 103/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4675\n",
      "Epoch 00103: loss did not improve from 1.46677\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4675 - val_loss: 1.4706\n",
      "Epoch 104/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4673\n",
      "Epoch 00104: loss did not improve from 1.46677\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4673 - val_loss: 1.4697\n",
      "Epoch 105/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4672\n",
      "Epoch 00105: loss did not improve from 1.46677\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4672 - val_loss: 1.4682\n",
      "Epoch 106/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4670\n",
      "Epoch 00106: loss did not improve from 1.46677\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4670 - val_loss: 1.4699\n",
      "Epoch 107/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4680\n",
      "Epoch 00107: loss did not improve from 1.46677\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4680 - val_loss: 1.4721\n",
      "Epoch 108/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4664\n",
      "Epoch 00108: loss improved from 1.46677 to 1.46638, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4664 - val_loss: 1.4671\n",
      "Epoch 109/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4663\n",
      "Epoch 00109: loss improved from 1.46638 to 1.46632, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4663 - val_loss: 1.4676\n",
      "Epoch 110/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4662\n",
      "Epoch 00110: loss improved from 1.46632 to 1.46623, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4662 - val_loss: 1.4732\n",
      "Epoch 111/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4692\n",
      "Epoch 00111: loss did not improve from 1.46623\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4692 - val_loss: 1.4682\n",
      "Epoch 112/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4656\n",
      "Epoch 00112: loss improved from 1.46623 to 1.46559, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4656 - val_loss: 1.4650\n",
      "Epoch 113/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4635\n",
      "Epoch 00113: loss improved from 1.46559 to 1.46349, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4635 - val_loss: 1.4663\n",
      "Epoch 114/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4627\n",
      "Epoch 00114: loss improved from 1.46349 to 1.46270, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4627 - val_loss: 1.4681\n",
      "Epoch 115/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4658\n",
      "Epoch 00115: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4658 - val_loss: 1.4763\n",
      "Epoch 116/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4689\n",
      "Epoch 00116: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4689 - val_loss: 1.4738\n",
      "Epoch 117/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4663\n",
      "Epoch 00117: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4663 - val_loss: 1.4732\n",
      "Epoch 118/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4667\n",
      "Epoch 00118: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4667 - val_loss: 1.4690\n",
      "Epoch 119/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4647\n",
      "Epoch 00119: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4647 - val_loss: 1.4683\n",
      "Epoch 120/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4654\n",
      "Epoch 00120: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4654 - val_loss: 1.4682\n",
      "Epoch 121/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4642\n",
      "Epoch 00121: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4642 - val_loss: 1.4674\n",
      "Epoch 122/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4648\n",
      "Epoch 00122: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4648 - val_loss: 1.4672\n",
      "Epoch 123/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4655\n",
      "Epoch 00123: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4655 - val_loss: 1.4716\n",
      "Epoch 124/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4680\n",
      "Epoch 00124: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4680 - val_loss: 1.4717\n",
      "Epoch 125/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4651\n",
      "Epoch 00125: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4651 - val_loss: 1.4683\n",
      "Epoch 126/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4644\n",
      "Epoch 00126: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4644 - val_loss: 1.4667\n",
      "Epoch 127/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4632\n",
      "Epoch 00127: loss did not improve from 1.46270\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4632 - val_loss: 1.4659\n",
      "Epoch 128/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4616\n",
      "Epoch 00128: loss improved from 1.46270 to 1.46157, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4616 - val_loss: 1.4686\n",
      "Epoch 129/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4633\n",
      "Epoch 00129: loss did not improve from 1.46157\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4633 - val_loss: 1.4676\n",
      "Epoch 130/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4614\n",
      "Epoch 00130: loss improved from 1.46157 to 1.46143, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4614 - val_loss: 1.4694\n",
      "Epoch 131/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4628\n",
      "Epoch 00131: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4628 - val_loss: 1.4694\n",
      "Epoch 132/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4628\n",
      "Epoch 00132: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4628 - val_loss: 1.4675\n",
      "Epoch 133/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4636\n",
      "Epoch 00133: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4636 - val_loss: 1.4771\n",
      "Epoch 134/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4644\n",
      "Epoch 00134: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4644 - val_loss: 1.4652\n",
      "Epoch 135/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4622\n",
      "Epoch 00135: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4622 - val_loss: 1.4717\n",
      "Epoch 136/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4653\n",
      "Epoch 00136: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4653 - val_loss: 1.4741\n",
      "Epoch 137/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4673\n",
      "Epoch 00137: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4673 - val_loss: 1.4727\n",
      "Epoch 138/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4679\n",
      "Epoch 00138: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4679 - val_loss: 1.4768\n",
      "Epoch 139/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4672\n",
      "Epoch 00139: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4672 - val_loss: 1.4692\n",
      "Epoch 140/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4655\n",
      "Epoch 00140: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4655 - val_loss: 1.4677\n",
      "Epoch 141/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4647\n",
      "Epoch 00141: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4647 - val_loss: 1.4694\n",
      "Epoch 142/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4635\n",
      "Epoch 00142: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4635 - val_loss: 1.4681\n",
      "Epoch 143/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4621\n",
      "Epoch 00143: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4621 - val_loss: 1.4740\n",
      "Epoch 144/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4670\n",
      "Epoch 00144: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4670 - val_loss: 1.4707\n",
      "Epoch 145/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4650\n",
      "Epoch 00145: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4650 - val_loss: 1.4716\n",
      "Epoch 146/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4661\n",
      "Epoch 00146: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4661 - val_loss: 1.4720\n",
      "Epoch 147/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4630\n",
      "Epoch 00147: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4630 - val_loss: 1.4669\n",
      "Epoch 148/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4655\n",
      "Epoch 00148: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4655 - val_loss: 1.4708\n",
      "Epoch 149/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4670\n",
      "Epoch 00149: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4670 - val_loss: 1.4727\n",
      "Epoch 150/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4638\n",
      "Epoch 00150: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4638 - val_loss: 1.4757\n",
      "Epoch 151/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4636\n",
      "Epoch 00151: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4636 - val_loss: 1.4687\n",
      "Epoch 152/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4633\n",
      "Epoch 00152: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4633 - val_loss: 1.4681\n",
      "Epoch 153/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4624\n",
      "Epoch 00153: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4624 - val_loss: 1.4679\n",
      "Epoch 154/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4647\n",
      "Epoch 00154: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4647 - val_loss: 1.4697\n",
      "Epoch 155/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4638\n",
      "Epoch 00155: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4638 - val_loss: 1.4682\n",
      "Epoch 156/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4646\n",
      "Epoch 00156: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4646 - val_loss: 1.4662\n",
      "Epoch 157/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4642\n",
      "Epoch 00157: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4642 - val_loss: 1.4704\n",
      "Epoch 158/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4656\n",
      "Epoch 00158: loss did not improve from 1.46143\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4656 - val_loss: 1.4680\n",
      "Epoch 159/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4608\n",
      "Epoch 00159: loss improved from 1.46143 to 1.46077, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4608 - val_loss: 1.4667\n",
      "Epoch 160/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4634\n",
      "Epoch 00160: loss did not improve from 1.46077\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4634 - val_loss: 1.4788\n",
      "Epoch 161/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4741\n",
      "Epoch 00161: loss did not improve from 1.46077\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4741 - val_loss: 1.4753\n",
      "Epoch 162/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4710\n",
      "Epoch 00162: loss did not improve from 1.46077\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4710 - val_loss: 1.4714\n",
      "Epoch 163/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4636\n",
      "Epoch 00163: loss did not improve from 1.46077\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4636 - val_loss: 1.4675\n",
      "Epoch 164/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4628\n",
      "Epoch 00164: loss did not improve from 1.46077\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4628 - val_loss: 1.4665\n",
      "Epoch 165/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4606\n",
      "Epoch 00165: loss improved from 1.46077 to 1.46062, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4606 - val_loss: 1.4712\n",
      "Epoch 166/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4605\n",
      "Epoch 00166: loss improved from 1.46062 to 1.46046, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4605 - val_loss: 1.4666\n",
      "Epoch 167/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4608\n",
      "Epoch 00167: loss did not improve from 1.46046\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4608 - val_loss: 1.4676\n",
      "Epoch 168/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4602\n",
      "Epoch 00168: loss improved from 1.46046 to 1.46019, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4602 - val_loss: 1.4703\n",
      "Epoch 169/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4625\n",
      "Epoch 00169: loss did not improve from 1.46019\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4625 - val_loss: 1.4725\n",
      "Epoch 170/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4663\n",
      "Epoch 00170: loss did not improve from 1.46019\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4663 - val_loss: 1.4735\n",
      "Epoch 171/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4617\n",
      "Epoch 00171: loss did not improve from 1.46019\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4617 - val_loss: 1.4655\n",
      "Epoch 172/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4588\n",
      "Epoch 00172: loss improved from 1.46019 to 1.45882, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4588 - val_loss: 1.4677\n",
      "Epoch 173/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4605\n",
      "Epoch 00173: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4605 - val_loss: 1.4688\n",
      "Epoch 174/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4624\n",
      "Epoch 00174: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4624 - val_loss: 1.4687\n",
      "Epoch 175/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4629\n",
      "Epoch 00175: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4629 - val_loss: 1.4724\n",
      "Epoch 176/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4609\n",
      "Epoch 00176: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4609 - val_loss: 1.4708\n",
      "Epoch 177/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4623\n",
      "Epoch 00177: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4623 - val_loss: 1.4651\n",
      "Epoch 178/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4609\n",
      "Epoch 00178: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4609 - val_loss: 1.4681\n",
      "Epoch 179/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4599\n",
      "Epoch 00179: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4599 - val_loss: 1.4710\n",
      "Epoch 180/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4615\n",
      "Epoch 00180: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4615 - val_loss: 1.4673\n",
      "Epoch 181/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4600\n",
      "Epoch 00181: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4600 - val_loss: 1.4690\n",
      "Epoch 182/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4617\n",
      "Epoch 00182: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4617 - val_loss: 1.4678\n",
      "Epoch 183/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4605\n",
      "Epoch 00183: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4605 - val_loss: 1.4716\n",
      "Epoch 184/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4613\n",
      "Epoch 00184: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4613 - val_loss: 1.4704\n",
      "Epoch 185/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4623\n",
      "Epoch 00185: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4623 - val_loss: 1.4683\n",
      "Epoch 186/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4611\n",
      "Epoch 00186: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4611 - val_loss: 1.4679\n",
      "Epoch 187/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4600\n",
      "Epoch 00187: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4600 - val_loss: 1.4664\n",
      "Epoch 188/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4623\n",
      "Epoch 00188: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4623 - val_loss: 1.4717\n",
      "Epoch 189/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4632\n",
      "Epoch 00189: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4632 - val_loss: 1.4689\n",
      "Epoch 190/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4642\n",
      "Epoch 00190: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4642 - val_loss: 1.4745\n",
      "Epoch 191/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4664\n",
      "Epoch 00191: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4664 - val_loss: 1.4700\n",
      "Epoch 192/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4632\n",
      "Epoch 00192: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4632 - val_loss: 1.4773\n",
      "Epoch 193/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4664\n",
      "Epoch 00193: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4664 - val_loss: 1.4785\n",
      "Epoch 194/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4613\n",
      "Epoch 00194: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4613 - val_loss: 1.4688\n",
      "Epoch 195/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4620\n",
      "Epoch 00195: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4620 - val_loss: 1.4714\n",
      "Epoch 196/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4629\n",
      "Epoch 00196: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4629 - val_loss: 1.4671\n",
      "Epoch 197/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4617\n",
      "Epoch 00197: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4617 - val_loss: 1.4684\n",
      "Epoch 198/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4589\n",
      "Epoch 00198: loss did not improve from 1.45882\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4589 - val_loss: 1.4726\n",
      "Epoch 199/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4586\n",
      "Epoch 00199: loss improved from 1.45882 to 1.45860, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4586 - val_loss: 1.4656\n",
      "Epoch 200/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4597\n",
      "Epoch 00200: loss did not improve from 1.45860\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4597 - val_loss: 1.4663\n",
      "Epoch 201/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4594\n",
      "Epoch 00201: loss did not improve from 1.45860\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4594 - val_loss: 1.4701\n",
      "Epoch 202/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4596\n",
      "Epoch 00202: loss did not improve from 1.45860\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4596 - val_loss: 1.4675\n",
      "Epoch 203/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4607\n",
      "Epoch 00203: loss did not improve from 1.45860\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4607 - val_loss: 1.4686\n",
      "Epoch 204/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4613\n",
      "Epoch 00204: loss did not improve from 1.45860\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4613 - val_loss: 1.4668\n",
      "Epoch 205/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4590\n",
      "Epoch 00205: loss did not improve from 1.45860\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4590 - val_loss: 1.4676\n",
      "Epoch 206/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4603\n",
      "Epoch 00206: loss did not improve from 1.45860\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4603 - val_loss: 1.4680\n",
      "Epoch 207/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4591\n",
      "Epoch 00207: loss did not improve from 1.45860\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4591 - val_loss: 1.4679\n",
      "Epoch 208/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4571\n",
      "Epoch 00208: loss improved from 1.45860 to 1.45711, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4571 - val_loss: 1.4687\n",
      "Epoch 209/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4613\n",
      "Epoch 00209: loss did not improve from 1.45711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4613 - val_loss: 1.4716\n",
      "Epoch 210/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4607\n",
      "Epoch 00210: loss did not improve from 1.45711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4607 - val_loss: 1.4742\n",
      "Epoch 211/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4609\n",
      "Epoch 00211: loss did not improve from 1.45711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4609 - val_loss: 1.4664\n",
      "Epoch 212/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4587\n",
      "Epoch 00212: loss did not improve from 1.45711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4587 - val_loss: 1.4673\n",
      "Epoch 213/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4568\n",
      "Epoch 00213: loss improved from 1.45711 to 1.45678, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4568 - val_loss: 1.4654\n",
      "Epoch 214/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4591\n",
      "Epoch 00214: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4591 - val_loss: 1.4678\n",
      "Epoch 215/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4622\n",
      "Epoch 00215: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4622 - val_loss: 1.4728\n",
      "Epoch 216/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4640\n",
      "Epoch 00216: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4640 - val_loss: 1.4709\n",
      "Epoch 217/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4639\n",
      "Epoch 00217: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4639 - val_loss: 1.4703\n",
      "Epoch 218/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4596\n",
      "Epoch 00218: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4596 - val_loss: 1.4706\n",
      "Epoch 219/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4597\n",
      "Epoch 00219: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4597 - val_loss: 1.4721\n",
      "Epoch 220/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4659\n",
      "Epoch 00220: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4659 - val_loss: 1.4726\n",
      "Epoch 221/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4622\n",
      "Epoch 00221: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4622 - val_loss: 1.4751\n",
      "Epoch 222/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4647\n",
      "Epoch 00222: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4647 - val_loss: 1.4716\n",
      "Epoch 223/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4628\n",
      "Epoch 00223: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4628 - val_loss: 1.4673\n",
      "Epoch 224/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4588\n",
      "Epoch 00224: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4588 - val_loss: 1.4701\n",
      "Epoch 225/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4579\n",
      "Epoch 00225: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4579 - val_loss: 1.4689\n",
      "Epoch 226/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4603\n",
      "Epoch 00226: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4603 - val_loss: 1.4733\n",
      "Epoch 227/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4627\n",
      "Epoch 00227: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4627 - val_loss: 1.4703\n",
      "Epoch 228/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4597\n",
      "Epoch 00228: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4597 - val_loss: 1.4661\n",
      "Epoch 229/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4572\n",
      "Epoch 00229: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4572 - val_loss: 1.4739\n",
      "Epoch 230/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4596\n",
      "Epoch 00230: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4596 - val_loss: 1.4655\n",
      "Epoch 231/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4596\n",
      "Epoch 00231: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4596 - val_loss: 1.4730\n",
      "Epoch 232/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4613\n",
      "Epoch 00232: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4613 - val_loss: 1.4682\n",
      "Epoch 233/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4609\n",
      "Epoch 00233: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4609 - val_loss: 1.4726\n",
      "Epoch 234/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4599\n",
      "Epoch 00234: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4599 - val_loss: 1.4670\n",
      "Epoch 235/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4585\n",
      "Epoch 00235: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4585 - val_loss: 1.4679\n",
      "Epoch 236/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4569\n",
      "Epoch 00236: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4569 - val_loss: 1.4687\n",
      "Epoch 237/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4585\n",
      "Epoch 00237: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4585 - val_loss: 1.4669\n",
      "Epoch 238/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4572\n",
      "Epoch 00238: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4572 - val_loss: 1.4695\n",
      "Epoch 239/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4600\n",
      "Epoch 00239: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4600 - val_loss: 1.4698\n",
      "Epoch 240/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4615\n",
      "Epoch 00240: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4615 - val_loss: 1.4712\n",
      "Epoch 241/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4579\n",
      "Epoch 00241: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4579 - val_loss: 1.4691\n",
      "Epoch 242/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4594\n",
      "Epoch 00242: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4594 - val_loss: 1.4745\n",
      "Epoch 243/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4602\n",
      "Epoch 00243: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4602 - val_loss: 1.4688\n",
      "Epoch 244/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4584\n",
      "Epoch 00244: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4584 - val_loss: 1.4697\n",
      "Epoch 245/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4601\n",
      "Epoch 00245: loss did not improve from 1.45678\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4601 - val_loss: 1.4655\n",
      "Epoch 246/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4562\n",
      "Epoch 00246: loss improved from 1.45678 to 1.45617, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4562 - val_loss: 1.4660\n",
      "Epoch 247/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4553\n",
      "Epoch 00247: loss improved from 1.45617 to 1.45534, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4553 - val_loss: 1.4715\n",
      "Epoch 248/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4587\n",
      "Epoch 00248: loss did not improve from 1.45534\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4587 - val_loss: 1.4648\n",
      "Epoch 249/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4568\n",
      "Epoch 00249: loss did not improve from 1.45534\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4568 - val_loss: 1.4681\n",
      "Epoch 250/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00250: loss improved from 1.45534 to 1.45509, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4551 - val_loss: 1.4662\n",
      "Epoch 251/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4564\n",
      "Epoch 00251: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4564 - val_loss: 1.4697\n",
      "Epoch 252/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4567\n",
      "Epoch 00252: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4567 - val_loss: 1.4684\n",
      "Epoch 253/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4570\n",
      "Epoch 00253: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4570 - val_loss: 1.4681\n",
      "Epoch 254/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4703\n",
      "Epoch 00254: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4703 - val_loss: 1.4773\n",
      "Epoch 255/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4691\n",
      "Epoch 00255: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4691 - val_loss: 1.4683\n",
      "Epoch 256/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4610\n",
      "Epoch 00256: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4610 - val_loss: 1.4686\n",
      "Epoch 257/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4600\n",
      "Epoch 00257: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4600 - val_loss: 1.4674\n",
      "Epoch 258/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4570\n",
      "Epoch 00258: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4570 - val_loss: 1.4688\n",
      "Epoch 259/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4570\n",
      "Epoch 00259: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4570 - val_loss: 1.4674\n",
      "Epoch 260/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4567\n",
      "Epoch 00260: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4567 - val_loss: 1.4722\n",
      "Epoch 261/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4563\n",
      "Epoch 00261: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4563 - val_loss: 1.4686\n",
      "Epoch 262/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4572\n",
      "Epoch 00262: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4572 - val_loss: 1.4668\n",
      "Epoch 263/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4557\n",
      "Epoch 00263: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4557 - val_loss: 1.4674\n",
      "Epoch 264/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4565\n",
      "Epoch 00264: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4565 - val_loss: 1.4766\n",
      "Epoch 265/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4637\n",
      "Epoch 00265: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4637 - val_loss: 1.4704\n",
      "Epoch 266/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4592\n",
      "Epoch 00266: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4592 - val_loss: 1.4728\n",
      "Epoch 267/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4578\n",
      "Epoch 00267: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4578 - val_loss: 1.4672\n",
      "Epoch 268/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00268: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4551 - val_loss: 1.4644\n",
      "Epoch 269/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4553\n",
      "Epoch 00269: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4553 - val_loss: 1.4661\n",
      "Epoch 270/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00270: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4555 - val_loss: 1.4796\n",
      "Epoch 271/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4603\n",
      "Epoch 00271: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4603 - val_loss: 1.4754\n",
      "Epoch 272/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4623\n",
      "Epoch 00272: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4623 - val_loss: 1.4659\n",
      "Epoch 273/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4582\n",
      "Epoch 00273: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4582 - val_loss: 1.4709\n",
      "Epoch 274/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4590\n",
      "Epoch 00274: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4590 - val_loss: 1.4655\n",
      "Epoch 275/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4560\n",
      "Epoch 00275: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4560 - val_loss: 1.4690\n",
      "Epoch 276/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4580\n",
      "Epoch 00276: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4580 - val_loss: 1.4673\n",
      "Epoch 277/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4600\n",
      "Epoch 00277: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4600 - val_loss: 1.4670\n",
      "Epoch 278/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4587\n",
      "Epoch 00278: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4587 - val_loss: 1.4724\n",
      "Epoch 279/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4597\n",
      "Epoch 00279: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4597 - val_loss: 1.4750\n",
      "Epoch 280/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4593\n",
      "Epoch 00280: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4593 - val_loss: 1.4676\n",
      "Epoch 281/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4585\n",
      "Epoch 00281: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4585 - val_loss: 1.4652\n",
      "Epoch 282/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4569\n",
      "Epoch 00282: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4569 - val_loss: 1.4695\n",
      "Epoch 283/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00283: loss did not improve from 1.45509\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4570 - val_loss: 1.4664\n",
      "Epoch 284/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00284: loss improved from 1.45509 to 1.45508, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4551 - val_loss: 1.4690\n",
      "Epoch 285/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4552\n",
      "Epoch 00285: loss did not improve from 1.45508\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4552 - val_loss: 1.4698\n",
      "Epoch 286/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00286: loss did not improve from 1.45508\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4551 - val_loss: 1.4671\n",
      "Epoch 287/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00287: loss improved from 1.45508 to 1.45489, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4549 - val_loss: 1.4660\n",
      "Epoch 288/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4562\n",
      "Epoch 00288: loss did not improve from 1.45489\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4562 - val_loss: 1.4681\n",
      "Epoch 289/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4574\n",
      "Epoch 00289: loss did not improve from 1.45489\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4574 - val_loss: 1.4702\n",
      "Epoch 290/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4542\n",
      "Epoch 00290: loss improved from 1.45489 to 1.45424, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4542 - val_loss: 1.4670\n",
      "Epoch 291/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4545\n",
      "Epoch 00291: loss did not improve from 1.45424\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4545 - val_loss: 1.4645\n",
      "Epoch 292/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4537\n",
      "Epoch 00292: loss improved from 1.45424 to 1.45365, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4537 - val_loss: 1.4706\n",
      "Epoch 293/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4548\n",
      "Epoch 00293: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4548 - val_loss: 1.4680\n",
      "Epoch 294/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4548\n",
      "Epoch 00294: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4548 - val_loss: 1.4690\n",
      "Epoch 295/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4561\n",
      "Epoch 00295: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4561 - val_loss: 1.4686\n",
      "Epoch 296/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4559\n",
      "Epoch 00296: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4559 - val_loss: 1.4699\n",
      "Epoch 297/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4593\n",
      "Epoch 00297: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4593 - val_loss: 1.4731\n",
      "Epoch 298/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4554\n",
      "Epoch 00298: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4554 - val_loss: 1.4665\n",
      "Epoch 299/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4539\n",
      "Epoch 00299: loss did not improve from 1.45365\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4539 - val_loss: 1.4665\n",
      "Epoch 300/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4536\n",
      "Epoch 00300: loss improved from 1.45365 to 1.45364, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4536 - val_loss: 1.4694\n",
      "Epoch 301/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4557\n",
      "Epoch 00301: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4557 - val_loss: 1.4681\n",
      "Epoch 302/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4548\n",
      "Epoch 00302: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4548 - val_loss: 1.4695\n",
      "Epoch 303/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4565\n",
      "Epoch 00303: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4565 - val_loss: 1.4693\n",
      "Epoch 304/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4561\n",
      "Epoch 00304: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4561 - val_loss: 1.4702\n",
      "Epoch 305/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4563\n",
      "Epoch 00305: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4563 - val_loss: 1.4649\n",
      "Epoch 306/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4585\n",
      "Epoch 00306: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4585 - val_loss: 1.4690\n",
      "Epoch 307/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4550\n",
      "Epoch 00307: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4550 - val_loss: 1.4679\n",
      "Epoch 308/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4629\n",
      "Epoch 00308: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4629 - val_loss: 1.4701\n",
      "Epoch 309/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4566\n",
      "Epoch 00309: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4566 - val_loss: 1.4676\n",
      "Epoch 310/3000\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.4563\n",
      "Epoch 00310: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4558 - val_loss: 1.4667\n",
      "Epoch 311/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4554\n",
      "Epoch 00311: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4554 - val_loss: 1.4703\n",
      "Epoch 312/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4600\n",
      "Epoch 00312: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4600 - val_loss: 1.4674\n",
      "Epoch 313/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4565\n",
      "Epoch 00313: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4565 - val_loss: 1.4729\n",
      "Epoch 314/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4563\n",
      "Epoch 00314: loss did not improve from 1.45364\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4563 - val_loss: 1.4666\n",
      "Epoch 315/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4528\n",
      "Epoch 00315: loss improved from 1.45364 to 1.45280, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4528 - val_loss: 1.4734\n",
      "Epoch 316/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00316: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4555 - val_loss: 1.4779\n",
      "Epoch 317/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4593\n",
      "Epoch 00317: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4593 - val_loss: 1.4712\n",
      "Epoch 318/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4563\n",
      "Epoch 00318: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4563 - val_loss: 1.4706\n",
      "Epoch 319/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4577\n",
      "Epoch 00319: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4577 - val_loss: 1.4734\n",
      "Epoch 320/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4573\n",
      "Epoch 00320: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4573 - val_loss: 1.4676\n",
      "Epoch 321/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00321: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4540 - val_loss: 1.4692\n",
      "Epoch 322/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4556\n",
      "Epoch 00322: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4556 - val_loss: 1.4745\n",
      "Epoch 323/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4597\n",
      "Epoch 00323: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4597 - val_loss: 1.4680\n",
      "Epoch 324/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4567\n",
      "Epoch 00324: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4567 - val_loss: 1.4757\n",
      "Epoch 325/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4580\n",
      "Epoch 00325: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4580 - val_loss: 1.4702\n",
      "Epoch 326/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4568\n",
      "Epoch 00326: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4568 - val_loss: 1.4744\n",
      "Epoch 327/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4595\n",
      "Epoch 00327: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4595 - val_loss: 1.4799\n",
      "Epoch 328/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4599\n",
      "Epoch 00328: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4599 - val_loss: 1.4699\n",
      "Epoch 329/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4558\n",
      "Epoch 00329: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4558 - val_loss: 1.4670\n",
      "Epoch 330/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00330: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4551 - val_loss: 1.4702\n",
      "Epoch 331/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4556\n",
      "Epoch 00331: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4556 - val_loss: 1.4677\n",
      "Epoch 332/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4611\n",
      "Epoch 00332: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4611 - val_loss: 1.4685\n",
      "Epoch 333/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4588\n",
      "Epoch 00333: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4588 - val_loss: 1.4716\n",
      "Epoch 334/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4573\n",
      "Epoch 00334: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4573 - val_loss: 1.4741\n",
      "Epoch 335/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4575\n",
      "Epoch 00335: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4575 - val_loss: 1.4721\n",
      "Epoch 336/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4546\n",
      "Epoch 00336: loss did not improve from 1.45280\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4546 - val_loss: 1.4675\n",
      "Epoch 337/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4528\n",
      "Epoch 00337: loss improved from 1.45280 to 1.45277, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4528 - val_loss: 1.4670\n",
      "Epoch 338/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4522\n",
      "Epoch 00338: loss improved from 1.45277 to 1.45217, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4522 - val_loss: 1.4692\n",
      "Epoch 339/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4538\n",
      "Epoch 00339: loss did not improve from 1.45217\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4538 - val_loss: 1.4708\n",
      "Epoch 340/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4542\n",
      "Epoch 00340: loss did not improve from 1.45217\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4542 - val_loss: 1.4674\n",
      "Epoch 341/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00341: loss improved from 1.45217 to 1.45153, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4515 - val_loss: 1.4701\n",
      "Epoch 342/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4533\n",
      "Epoch 00342: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4533 - val_loss: 1.4662\n",
      "Epoch 343/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00343: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4540 - val_loss: 1.4703\n",
      "Epoch 344/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4534\n",
      "Epoch 00344: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4534 - val_loss: 1.4647\n",
      "Epoch 345/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4539\n",
      "Epoch 00345: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4539 - val_loss: 1.4717\n",
      "Epoch 346/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4535\n",
      "Epoch 00346: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4535 - val_loss: 1.4675\n",
      "Epoch 347/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00347: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4529 - val_loss: 1.4667\n",
      "Epoch 348/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4556\n",
      "Epoch 00348: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4556 - val_loss: 1.4755\n",
      "Epoch 349/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4558\n",
      "Epoch 00349: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4558 - val_loss: 1.4692\n",
      "Epoch 350/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4551\n",
      "Epoch 00350: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4551 - val_loss: 1.4671\n",
      "Epoch 351/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4534\n",
      "Epoch 00351: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4534 - val_loss: 1.4694\n",
      "Epoch 352/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4543\n",
      "Epoch 00352: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4543 - val_loss: 1.4715\n",
      "Epoch 353/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4543\n",
      "Epoch 00353: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4543 - val_loss: 1.4690\n",
      "Epoch 354/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4527\n",
      "Epoch 00354: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4527 - val_loss: 1.4693\n",
      "Epoch 355/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4553\n",
      "Epoch 00355: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4553 - val_loss: 1.4728\n",
      "Epoch 356/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00356: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4549 - val_loss: 1.4684\n",
      "Epoch 357/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4547\n",
      "Epoch 00357: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4547 - val_loss: 1.4708\n",
      "Epoch 358/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4543\n",
      "Epoch 00358: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4543 - val_loss: 1.4703\n",
      "Epoch 359/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00359: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4540 - val_loss: 1.4702\n",
      "Epoch 360/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00360: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4540 - val_loss: 1.4703\n",
      "Epoch 361/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4530\n",
      "Epoch 00361: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4530 - val_loss: 1.4675\n",
      "Epoch 362/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4553\n",
      "Epoch 00362: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4553 - val_loss: 1.4707\n",
      "Epoch 363/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4533\n",
      "Epoch 00363: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4533 - val_loss: 1.4706\n",
      "Epoch 364/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4546\n",
      "Epoch 00364: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4546 - val_loss: 1.4697\n",
      "Epoch 365/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4535\n",
      "Epoch 00365: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4535 - val_loss: 1.4691\n",
      "Epoch 366/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4530\n",
      "Epoch 00366: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4530 - val_loss: 1.4673\n",
      "Epoch 367/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4559\n",
      "Epoch 00367: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4559 - val_loss: 1.4671\n",
      "Epoch 368/3000\n",
      "8/9 [=========================>....] - ETA: 0s - loss: 1.4568\n",
      "Epoch 00368: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4544 - val_loss: 1.4674\n",
      "Epoch 369/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4541\n",
      "Epoch 00369: loss did not improve from 1.45153\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4541 - val_loss: 1.4667\n",
      "Epoch 370/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4511\n",
      "Epoch 00370: loss improved from 1.45153 to 1.45107, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4511 - val_loss: 1.4731\n",
      "Epoch 371/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4543\n",
      "Epoch 00371: loss did not improve from 1.45107\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4543 - val_loss: 1.4699\n",
      "Epoch 372/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4541\n",
      "Epoch 00372: loss did not improve from 1.45107\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4541 - val_loss: 1.4681\n",
      "Epoch 373/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4527\n",
      "Epoch 00373: loss did not improve from 1.45107\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4527 - val_loss: 1.4675\n",
      "Epoch 374/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00374: loss did not improve from 1.45107\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4515 - val_loss: 1.4667\n",
      "Epoch 375/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00375: loss did not improve from 1.45107\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4515 - val_loss: 1.4738\n",
      "Epoch 376/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00376: loss did not improve from 1.45107\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4549 - val_loss: 1.4704\n",
      "Epoch 377/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4552\n",
      "Epoch 00377: loss did not improve from 1.45107\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4552 - val_loss: 1.4684\n",
      "Epoch 378/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4524\n",
      "Epoch 00378: loss did not improve from 1.45107\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4524 - val_loss: 1.4677\n",
      "Epoch 379/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4514\n",
      "Epoch 00379: loss did not improve from 1.45107\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4514 - val_loss: 1.4701\n",
      "Epoch 380/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4509\n",
      "Epoch 00380: loss improved from 1.45107 to 1.45089, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4509 - val_loss: 1.4667\n",
      "Epoch 381/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4507\n",
      "Epoch 00381: loss improved from 1.45089 to 1.45073, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4507 - val_loss: 1.4691\n",
      "Epoch 382/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4573\n",
      "Epoch 00382: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4573 - val_loss: 1.4694\n",
      "Epoch 383/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4552\n",
      "Epoch 00383: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4552 - val_loss: 1.4727\n",
      "Epoch 384/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00384: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4549 - val_loss: 1.4691\n",
      "Epoch 385/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4539\n",
      "Epoch 00385: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4539 - val_loss: 1.4667\n",
      "Epoch 386/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4544\n",
      "Epoch 00386: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4544 - val_loss: 1.4753\n",
      "Epoch 387/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4539\n",
      "Epoch 00387: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4539 - val_loss: 1.4693\n",
      "Epoch 388/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00388: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4529 - val_loss: 1.4723\n",
      "Epoch 389/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00389: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4555 - val_loss: 1.4683\n",
      "Epoch 390/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4516\n",
      "Epoch 00390: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4516 - val_loss: 1.4687\n",
      "Epoch 391/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4530\n",
      "Epoch 00391: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4530 - val_loss: 1.4713\n",
      "Epoch 392/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4545\n",
      "Epoch 00392: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4545 - val_loss: 1.4733\n",
      "Epoch 393/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4531\n",
      "Epoch 00393: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4531 - val_loss: 1.4750\n",
      "Epoch 394/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4538\n",
      "Epoch 00394: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4538 - val_loss: 1.4744\n",
      "Epoch 395/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00395: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4540 - val_loss: 1.4695\n",
      "Epoch 396/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4539\n",
      "Epoch 00396: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4539 - val_loss: 1.4716\n",
      "Epoch 397/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4530\n",
      "Epoch 00397: loss did not improve from 1.45073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4530 - val_loss: 1.4682\n",
      "Epoch 398/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4503\n",
      "Epoch 00398: loss improved from 1.45073 to 1.45033, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4503 - val_loss: 1.4653\n",
      "Epoch 399/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4536\n",
      "Epoch 00399: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4536 - val_loss: 1.4703\n",
      "Epoch 400/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4537\n",
      "Epoch 00400: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4537 - val_loss: 1.4656\n",
      "Epoch 401/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4513\n",
      "Epoch 00401: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4513 - val_loss: 1.4690\n",
      "Epoch 402/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4522\n",
      "Epoch 00402: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4522 - val_loss: 1.4693\n",
      "Epoch 403/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4524\n",
      "Epoch 00403: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4524 - val_loss: 1.4699\n",
      "Epoch 404/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4531\n",
      "Epoch 00404: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4531 - val_loss: 1.4695\n",
      "Epoch 405/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4516\n",
      "Epoch 00405: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4516 - val_loss: 1.4679\n",
      "Epoch 406/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4505\n",
      "Epoch 00406: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4505 - val_loss: 1.4689\n",
      "Epoch 407/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00407: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4525 - val_loss: 1.4776\n",
      "Epoch 408/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4576\n",
      "Epoch 00408: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4576 - val_loss: 1.4751\n",
      "Epoch 409/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4580\n",
      "Epoch 00409: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4580 - val_loss: 1.4754\n",
      "Epoch 410/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4536\n",
      "Epoch 00410: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4536 - val_loss: 1.4715\n",
      "Epoch 411/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00411: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4529 - val_loss: 1.4683\n",
      "Epoch 412/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4560\n",
      "Epoch 00412: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4560 - val_loss: 1.4692\n",
      "Epoch 413/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4524\n",
      "Epoch 00413: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4524 - val_loss: 1.4648\n",
      "Epoch 414/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4553\n",
      "Epoch 00414: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4553 - val_loss: 1.4729\n",
      "Epoch 415/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4555\n",
      "Epoch 00415: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4555 - val_loss: 1.4758\n",
      "Epoch 416/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4558\n",
      "Epoch 00416: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4558 - val_loss: 1.4733\n",
      "Epoch 417/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4535\n",
      "Epoch 00417: loss did not improve from 1.45033\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4535 - val_loss: 1.4687\n",
      "Epoch 418/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00418: loss improved from 1.45033 to 1.45017, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4502 - val_loss: 1.4686\n",
      "Epoch 419/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4501\n",
      "Epoch 00419: loss improved from 1.45017 to 1.45006, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4501 - val_loss: 1.4729\n",
      "Epoch 420/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00420: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4525 - val_loss: 1.4742\n",
      "Epoch 421/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00421: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4525 - val_loss: 1.4831\n",
      "Epoch 422/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4603\n",
      "Epoch 00422: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4603 - val_loss: 1.4754\n",
      "Epoch 423/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4556\n",
      "Epoch 00423: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4556 - val_loss: 1.4687\n",
      "Epoch 424/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4538\n",
      "Epoch 00424: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4538 - val_loss: 1.4703\n",
      "Epoch 425/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4535\n",
      "Epoch 00425: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4535 - val_loss: 1.4691\n",
      "Epoch 426/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4541\n",
      "Epoch 00426: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4541 - val_loss: 1.4761\n",
      "Epoch 427/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4560\n",
      "Epoch 00427: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4560 - val_loss: 1.4736\n",
      "Epoch 428/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4521\n",
      "Epoch 00428: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4521 - val_loss: 1.4778\n",
      "Epoch 429/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4553\n",
      "Epoch 00429: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4553 - val_loss: 1.4724\n",
      "Epoch 430/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4523\n",
      "Epoch 00430: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4523 - val_loss: 1.4707\n",
      "Epoch 431/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4512\n",
      "Epoch 00431: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4512 - val_loss: 1.4689\n",
      "Epoch 432/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4580\n",
      "Epoch 00432: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4580 - val_loss: 1.4712\n",
      "Epoch 433/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4527\n",
      "Epoch 00433: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4527 - val_loss: 1.4781\n",
      "Epoch 434/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4567\n",
      "Epoch 00434: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4567 - val_loss: 1.4669\n",
      "Epoch 435/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4536\n",
      "Epoch 00435: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4536 - val_loss: 1.4725\n",
      "Epoch 436/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4511\n",
      "Epoch 00436: loss did not improve from 1.45006\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4511 - val_loss: 1.4673\n",
      "Epoch 437/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00437: loss improved from 1.45006 to 1.44921, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4492 - val_loss: 1.4677\n",
      "Epoch 438/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00438: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4492 - val_loss: 1.4697\n",
      "Epoch 439/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4503\n",
      "Epoch 00439: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4503 - val_loss: 1.4691\n",
      "Epoch 440/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4523\n",
      "Epoch 00440: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4523 - val_loss: 1.4721\n",
      "Epoch 441/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00441: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4519 - val_loss: 1.4765\n",
      "Epoch 442/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4504\n",
      "Epoch 00442: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4504 - val_loss: 1.4703\n",
      "Epoch 443/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00443: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4492 - val_loss: 1.4685\n",
      "Epoch 444/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4512\n",
      "Epoch 00444: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4512 - val_loss: 1.4673\n",
      "Epoch 445/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4527\n",
      "Epoch 00445: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4527 - val_loss: 1.4696\n",
      "Epoch 446/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4510\n",
      "Epoch 00446: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4510 - val_loss: 1.4665\n",
      "Epoch 447/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00447: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4519 - val_loss: 1.4696\n",
      "Epoch 448/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00448: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4502 - val_loss: 1.4725\n",
      "Epoch 449/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4539\n",
      "Epoch 00449: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4539 - val_loss: 1.4740\n",
      "Epoch 450/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4556\n",
      "Epoch 00450: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4556 - val_loss: 1.4791\n",
      "Epoch 451/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4559\n",
      "Epoch 00451: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4559 - val_loss: 1.4736\n",
      "Epoch 452/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4516\n",
      "Epoch 00452: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4516 - val_loss: 1.4688\n",
      "Epoch 453/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4508\n",
      "Epoch 00453: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4508 - val_loss: 1.4726\n",
      "Epoch 454/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00454: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4519 - val_loss: 1.4701\n",
      "Epoch 455/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4506\n",
      "Epoch 00455: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4506 - val_loss: 1.4687\n",
      "Epoch 456/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4533\n",
      "Epoch 00456: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4511 - val_loss: 1.4673\n",
      "Epoch 457/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4500\n",
      "Epoch 00457: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4500 - val_loss: 1.4668\n",
      "Epoch 458/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4549\n",
      "Epoch 00458: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4549 - val_loss: 1.4683\n",
      "Epoch 459/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4369\n",
      "Epoch 00459: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4522 - val_loss: 1.4748\n",
      "Epoch 460/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00460: loss did not improve from 1.44921\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4525 - val_loss: 1.4694\n",
      "Epoch 461/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4479\n",
      "Epoch 00461: loss improved from 1.44921 to 1.44793, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4479 - val_loss: 1.4664\n",
      "Epoch 462/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00462: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4502 - val_loss: 1.4744\n",
      "Epoch 463/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4507\n",
      "Epoch 00463: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4507 - val_loss: 1.4694\n",
      "Epoch 464/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4497\n",
      "Epoch 00464: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4497 - val_loss: 1.4674\n",
      "Epoch 465/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00465: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4525 - val_loss: 1.4735\n",
      "Epoch 466/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4499\n",
      "Epoch 00466: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4499 - val_loss: 1.4670\n",
      "Epoch 467/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4496\n",
      "Epoch 00467: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4496 - val_loss: 1.4693\n",
      "Epoch 468/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4498\n",
      "Epoch 00468: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4498 - val_loss: 1.4679\n",
      "Epoch 469/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4489\n",
      "Epoch 00469: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4489 - val_loss: 1.4700\n",
      "Epoch 470/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4516\n",
      "Epoch 00470: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4516 - val_loss: 1.4702\n",
      "Epoch 471/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4486\n",
      "Epoch 00471: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4486 - val_loss: 1.4693\n",
      "Epoch 472/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4490\n",
      "Epoch 00472: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4490 - val_loss: 1.4674\n",
      "Epoch 473/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4522\n",
      "Epoch 00473: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4522 - val_loss: 1.4727\n",
      "Epoch 474/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4534\n",
      "Epoch 00474: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4534 - val_loss: 1.4687\n",
      "Epoch 475/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4498\n",
      "Epoch 00475: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4498 - val_loss: 1.4699\n",
      "Epoch 476/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4494\n",
      "Epoch 00476: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4494 - val_loss: 1.4691\n",
      "Epoch 477/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4485\n",
      "Epoch 00477: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4485 - val_loss: 1.4775\n",
      "Epoch 478/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4573\n",
      "Epoch 00478: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4573 - val_loss: 1.4673\n",
      "Epoch 479/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4565\n",
      "Epoch 00479: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4565 - val_loss: 1.4718\n",
      "Epoch 480/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4505\n",
      "Epoch 00480: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4505 - val_loss: 1.4699\n",
      "Epoch 481/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4507\n",
      "Epoch 00481: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4507 - val_loss: 1.4676\n",
      "Epoch 482/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4497\n",
      "Epoch 00482: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4497 - val_loss: 1.4778\n",
      "Epoch 483/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4541\n",
      "Epoch 00483: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4541 - val_loss: 1.4767\n",
      "Epoch 484/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4539\n",
      "Epoch 00484: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4539 - val_loss: 1.4690\n",
      "Epoch 485/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4541\n",
      "Epoch 00485: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4541 - val_loss: 1.4711\n",
      "Epoch 486/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4527\n",
      "Epoch 00486: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4527 - val_loss: 1.4764\n",
      "Epoch 487/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4557\n",
      "Epoch 00487: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4557 - val_loss: 1.4694\n",
      "Epoch 488/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00488: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4515 - val_loss: 1.4708\n",
      "Epoch 489/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4498\n",
      "Epoch 00489: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4498 - val_loss: 1.4713\n",
      "Epoch 490/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4521\n",
      "Epoch 00490: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4521 - val_loss: 1.4740\n",
      "Epoch 491/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00491: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4519 - val_loss: 1.4732\n",
      "Epoch 492/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 00492: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4515 - val_loss: 1.4734\n",
      "Epoch 493/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4500\n",
      "Epoch 00493: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4500 - val_loss: 1.4721\n",
      "Epoch 494/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4516\n",
      "Epoch 00494: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4516 - val_loss: 1.4719\n",
      "Epoch 495/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4495\n",
      "Epoch 00495: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4495 - val_loss: 1.4733\n",
      "Epoch 496/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4490\n",
      "Epoch 00496: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4490 - val_loss: 1.4676\n",
      "Epoch 497/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4511\n",
      "Epoch 00497: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4511 - val_loss: 1.4741\n",
      "Epoch 498/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4505\n",
      "Epoch 00498: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4505 - val_loss: 1.4721\n",
      "Epoch 499/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00499: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4519 - val_loss: 1.4720\n",
      "Epoch 500/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4518\n",
      "Epoch 00500: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4518 - val_loss: 1.4724\n",
      "Epoch 501/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4553\n",
      "Epoch 00501: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4553 - val_loss: 1.4765\n",
      "Epoch 502/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4508\n",
      "Epoch 00502: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4508 - val_loss: 1.4716\n",
      "Epoch 503/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00503: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4492 - val_loss: 1.4700\n",
      "Epoch 504/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4521\n",
      "Epoch 00504: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4521 - val_loss: 1.4693\n",
      "Epoch 505/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4480\n",
      "Epoch 00505: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4480 - val_loss: 1.4681\n",
      "Epoch 506/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4498\n",
      "Epoch 00506: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4498 - val_loss: 1.4714\n",
      "Epoch 507/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4504\n",
      "Epoch 00507: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4504 - val_loss: 1.4692\n",
      "Epoch 508/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4488\n",
      "Epoch 00508: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4488 - val_loss: 1.4693\n",
      "Epoch 509/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4518\n",
      "Epoch 00509: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4489 - val_loss: 1.4763\n",
      "Epoch 510/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00510: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4519 - val_loss: 1.4691\n",
      "Epoch 511/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4480\n",
      "Epoch 00511: loss did not improve from 1.44793\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4480 - val_loss: 1.4654\n",
      "Epoch 512/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4478\n",
      "Epoch 00512: loss improved from 1.44793 to 1.44778, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4478 - val_loss: 1.4698\n",
      "Epoch 513/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00513: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4482 - val_loss: 1.4694\n",
      "Epoch 514/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00514: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4482 - val_loss: 1.4764\n",
      "Epoch 515/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4563\n",
      "Epoch 00515: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4563 - val_loss: 1.4718\n",
      "Epoch 516/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4531\n",
      "Epoch 00516: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4531 - val_loss: 1.4710\n",
      "Epoch 517/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4520\n",
      "Epoch 00517: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4520 - val_loss: 1.4686\n",
      "Epoch 518/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4494\n",
      "Epoch 00518: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4494 - val_loss: 1.4679\n",
      "Epoch 519/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4481\n",
      "Epoch 00519: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4481 - val_loss: 1.4675\n",
      "Epoch 520/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00520: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4492 - val_loss: 1.4685\n",
      "Epoch 521/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4495\n",
      "Epoch 00521: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4495 - val_loss: 1.4714\n",
      "Epoch 522/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4495\n",
      "Epoch 00522: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4495 - val_loss: 1.4713\n",
      "Epoch 523/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4506\n",
      "Epoch 00523: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4506 - val_loss: 1.4736\n",
      "Epoch 524/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4487\n",
      "Epoch 00524: loss did not improve from 1.44778\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4487 - val_loss: 1.4678\n",
      "Epoch 525/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00525: loss improved from 1.44778 to 1.44702, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4470 - val_loss: 1.4722\n",
      "Epoch 526/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4490\n",
      "Epoch 00526: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4490 - val_loss: 1.4713\n",
      "Epoch 527/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4479\n",
      "Epoch 00527: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4479 - val_loss: 1.4710\n",
      "Epoch 528/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4485\n",
      "Epoch 00528: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4485 - val_loss: 1.4719\n",
      "Epoch 529/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4476\n",
      "Epoch 00529: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4476 - val_loss: 1.4697\n",
      "Epoch 530/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4479\n",
      "Epoch 00530: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4479 - val_loss: 1.4724\n",
      "Epoch 531/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4488\n",
      "Epoch 00531: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4488 - val_loss: 1.4699\n",
      "Epoch 532/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4505\n",
      "Epoch 00532: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4505 - val_loss: 1.4705\n",
      "Epoch 533/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4471\n",
      "Epoch 00533: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4471 - val_loss: 1.4681\n",
      "Epoch 534/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4513\n",
      "Epoch 00534: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4513 - val_loss: 1.4709\n",
      "Epoch 535/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4483\n",
      "Epoch 00535: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4483 - val_loss: 1.4763\n",
      "Epoch 536/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4510\n",
      "Epoch 00536: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4510 - val_loss: 1.4692\n",
      "Epoch 537/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4487\n",
      "Epoch 00537: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4487 - val_loss: 1.4711\n",
      "Epoch 538/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4472\n",
      "Epoch 00538: loss did not improve from 1.44702\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4472 - val_loss: 1.4682\n",
      "Epoch 539/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4453\n",
      "Epoch 00539: loss improved from 1.44702 to 1.44528, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4453 - val_loss: 1.4705\n",
      "Epoch 540/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4472\n",
      "Epoch 00540: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4472 - val_loss: 1.4679\n",
      "Epoch 541/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4474\n",
      "Epoch 00541: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4474 - val_loss: 1.4728\n",
      "Epoch 542/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4507\n",
      "Epoch 00542: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4507 - val_loss: 1.4703\n",
      "Epoch 543/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00543: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4492 - val_loss: 1.4676\n",
      "Epoch 544/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4473\n",
      "Epoch 00544: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4473 - val_loss: 1.4734\n",
      "Epoch 545/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00545: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4482 - val_loss: 1.4712\n",
      "Epoch 546/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4516\n",
      "Epoch 00546: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4516 - val_loss: 1.4791\n",
      "Epoch 547/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4540\n",
      "Epoch 00547: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4540 - val_loss: 1.4769\n",
      "Epoch 548/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4556\n",
      "Epoch 00548: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4556 - val_loss: 1.4734\n",
      "Epoch 549/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4517\n",
      "Epoch 00549: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4517 - val_loss: 1.4787\n",
      "Epoch 550/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4518\n",
      "Epoch 00550: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4518 - val_loss: 1.4678\n",
      "Epoch 551/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4557\n",
      "Epoch 00551: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4557 - val_loss: 1.4726\n",
      "Epoch 552/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4509\n",
      "Epoch 00552: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4509 - val_loss: 1.4705\n",
      "Epoch 553/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4480\n",
      "Epoch 00553: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4480 - val_loss: 1.4697\n",
      "Epoch 554/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00554: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4470 - val_loss: 1.4682\n",
      "Epoch 555/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4471\n",
      "Epoch 00555: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4471 - val_loss: 1.4710\n",
      "Epoch 556/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4454\n",
      "Epoch 00556: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4454 - val_loss: 1.4707\n",
      "Epoch 557/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00557: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4470 - val_loss: 1.4703\n",
      "Epoch 558/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4488\n",
      "Epoch 00558: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4488 - val_loss: 1.4691\n",
      "Epoch 559/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4485\n",
      "Epoch 00559: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4485 - val_loss: 1.4724\n",
      "Epoch 560/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4477\n",
      "Epoch 00560: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4477 - val_loss: 1.4732\n",
      "Epoch 561/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4493\n",
      "Epoch 00561: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4493 - val_loss: 1.4682\n",
      "Epoch 562/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4474\n",
      "Epoch 00562: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4474 - val_loss: 1.4767\n",
      "Epoch 563/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4538\n",
      "Epoch 00563: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4538 - val_loss: 1.4744\n",
      "Epoch 564/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4581\n",
      "Epoch 00564: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4581 - val_loss: 1.4777\n",
      "Epoch 565/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4519\n",
      "Epoch 00565: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4519 - val_loss: 1.4694\n",
      "Epoch 566/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4493\n",
      "Epoch 00566: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4493 - val_loss: 1.4805\n",
      "Epoch 567/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4517\n",
      "Epoch 00567: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4517 - val_loss: 1.4743\n",
      "Epoch 568/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00568: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4525 - val_loss: 1.4751\n",
      "Epoch 569/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4506\n",
      "Epoch 00569: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4506 - val_loss: 1.4739\n",
      "Epoch 570/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4493\n",
      "Epoch 00570: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4493 - val_loss: 1.4727\n",
      "Epoch 571/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4526\n",
      "Epoch 00571: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4526 - val_loss: 1.4702\n",
      "Epoch 572/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4468\n",
      "Epoch 00572: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4468 - val_loss: 1.4709\n",
      "Epoch 573/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4484\n",
      "Epoch 00573: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4484 - val_loss: 1.4710\n",
      "Epoch 574/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4525\n",
      "Epoch 00574: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4525 - val_loss: 1.4740\n",
      "Epoch 575/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4487\n",
      "Epoch 00575: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4487 - val_loss: 1.4716\n",
      "Epoch 576/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4456\n",
      "Epoch 00576: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4456 - val_loss: 1.4690\n",
      "Epoch 577/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00577: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4464 - val_loss: 1.4711\n",
      "Epoch 578/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4479\n",
      "Epoch 00578: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4479 - val_loss: 1.4677\n",
      "Epoch 579/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4462\n",
      "Epoch 00579: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4462 - val_loss: 1.4728\n",
      "Epoch 580/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4468\n",
      "Epoch 00580: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4468 - val_loss: 1.4722\n",
      "Epoch 581/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4458\n",
      "Epoch 00581: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4458 - val_loss: 1.4697\n",
      "Epoch 582/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00582: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4461 - val_loss: 1.4738\n",
      "Epoch 583/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4479\n",
      "Epoch 00583: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4479 - val_loss: 1.4704\n",
      "Epoch 584/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4496\n",
      "Epoch 00584: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4496 - val_loss: 1.4688\n",
      "Epoch 585/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4491\n",
      "Epoch 00585: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4491 - val_loss: 1.4734\n",
      "Epoch 586/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4475\n",
      "Epoch 00586: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4475 - val_loss: 1.4729\n",
      "Epoch 587/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4462\n",
      "Epoch 00587: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4462 - val_loss: 1.4707\n",
      "Epoch 588/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4456\n",
      "Epoch 00588: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4456 - val_loss: 1.4758\n",
      "Epoch 589/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4478\n",
      "Epoch 00589: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4478 - val_loss: 1.4676\n",
      "Epoch 590/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4460\n",
      "Epoch 00590: loss did not improve from 1.44528\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4460 - val_loss: 1.4686\n",
      "Epoch 591/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00591: loss improved from 1.44528 to 1.44442, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4444 - val_loss: 1.4702\n",
      "Epoch 592/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00592: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4448 - val_loss: 1.4685\n",
      "Epoch 593/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4476\n",
      "Epoch 00593: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4476 - val_loss: 1.4722\n",
      "Epoch 594/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4475\n",
      "Epoch 00594: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4475 - val_loss: 1.4696\n",
      "Epoch 595/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4477\n",
      "Epoch 00595: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4477 - val_loss: 1.4741\n",
      "Epoch 596/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4469\n",
      "Epoch 00596: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4469 - val_loss: 1.4692\n",
      "Epoch 597/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00597: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4467 - val_loss: 1.4712\n",
      "Epoch 598/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4484\n",
      "Epoch 00598: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4484 - val_loss: 1.4692\n",
      "Epoch 599/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4460\n",
      "Epoch 00599: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4460 - val_loss: 1.4703\n",
      "Epoch 600/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00600: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4464 - val_loss: 1.4734\n",
      "Epoch 601/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4458\n",
      "Epoch 00601: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4458 - val_loss: 1.4721\n",
      "Epoch 602/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4462\n",
      "Epoch 00602: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4462 - val_loss: 1.4744\n",
      "Epoch 603/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4516\n",
      "Epoch 00603: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4516 - val_loss: 1.4688\n",
      "Epoch 604/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4491\n",
      "Epoch 00604: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4491 - val_loss: 1.4675\n",
      "Epoch 605/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00605: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4470 - val_loss: 1.4689\n",
      "Epoch 606/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4478\n",
      "Epoch 00606: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4478 - val_loss: 1.4692\n",
      "Epoch 607/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00607: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4464 - val_loss: 1.4750\n",
      "Epoch 608/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4472\n",
      "Epoch 00608: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4472 - val_loss: 1.4749\n",
      "Epoch 609/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4474\n",
      "Epoch 00609: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4474 - val_loss: 1.4667\n",
      "Epoch 610/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4474\n",
      "Epoch 00610: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4474 - val_loss: 1.4708\n",
      "Epoch 611/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4473\n",
      "Epoch 00611: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4473 - val_loss: 1.4724\n",
      "Epoch 612/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4480\n",
      "Epoch 00612: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4480 - val_loss: 1.4722\n",
      "Epoch 613/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00613: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4464 - val_loss: 1.4717\n",
      "Epoch 614/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4476\n",
      "Epoch 00614: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4476 - val_loss: 1.4709\n",
      "Epoch 615/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00615: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4470 - val_loss: 1.4758\n",
      "Epoch 616/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4481\n",
      "Epoch 00616: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4481 - val_loss: 1.4705\n",
      "Epoch 617/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4464\n",
      "Epoch 00617: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4464 - val_loss: 1.4716\n",
      "Epoch 618/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4546\n",
      "Epoch 00618: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4546 - val_loss: 1.4707\n",
      "Epoch 619/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4498\n",
      "Epoch 00619: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4498 - val_loss: 1.4697\n",
      "Epoch 620/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 00620: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4463 - val_loss: 1.4721\n",
      "Epoch 621/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4460\n",
      "Epoch 00621: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4460 - val_loss: 1.4702\n",
      "Epoch 622/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4466\n",
      "Epoch 00622: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4466 - val_loss: 1.4723\n",
      "Epoch 623/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 00623: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4463 - val_loss: 1.4754\n",
      "Epoch 624/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4497\n",
      "Epoch 00624: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4497 - val_loss: 1.4778\n",
      "Epoch 625/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4480\n",
      "Epoch 00625: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4480 - val_loss: 1.4704\n",
      "Epoch 626/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4456\n",
      "Epoch 00626: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4456 - val_loss: 1.4694\n",
      "Epoch 627/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4528\n",
      "Epoch 00627: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4528 - val_loss: 1.4720\n",
      "Epoch 628/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4491\n",
      "Epoch 00628: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4491 - val_loss: 1.4744\n",
      "Epoch 629/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4494\n",
      "Epoch 00629: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4494 - val_loss: 1.4700\n",
      "Epoch 630/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4468\n",
      "Epoch 00630: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4468 - val_loss: 1.4704\n",
      "Epoch 631/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00631: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4461 - val_loss: 1.4766\n",
      "Epoch 632/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4504\n",
      "Epoch 00632: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4504 - val_loss: 1.4821\n",
      "Epoch 633/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00633: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4492 - val_loss: 1.4793\n",
      "Epoch 634/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4514\n",
      "Epoch 00634: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4514 - val_loss: 1.4848\n",
      "Epoch 635/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4529\n",
      "Epoch 00635: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4529 - val_loss: 1.4776\n",
      "Epoch 636/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4475\n",
      "Epoch 00636: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4475 - val_loss: 1.4736\n",
      "Epoch 637/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4505\n",
      "Epoch 00637: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4505 - val_loss: 1.4774\n",
      "Epoch 638/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4494\n",
      "Epoch 00638: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4494 - val_loss: 1.4722\n",
      "Epoch 639/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4459\n",
      "Epoch 00639: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4459 - val_loss: 1.4714\n",
      "Epoch 640/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 00640: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4463 - val_loss: 1.4702\n",
      "Epoch 641/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4459\n",
      "Epoch 00641: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4459 - val_loss: 1.4729\n",
      "Epoch 642/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4493\n",
      "Epoch 00642: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4493 - val_loss: 1.4757\n",
      "Epoch 643/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4479\n",
      "Epoch 00643: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4479 - val_loss: 1.4731\n",
      "Epoch 644/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00644: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4461 - val_loss: 1.4728\n",
      "Epoch 645/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4460\n",
      "Epoch 00645: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4460 - val_loss: 1.4709\n",
      "Epoch 646/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4445\n",
      "Epoch 00646: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4445 - val_loss: 1.4714\n",
      "Epoch 647/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00647: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4448 - val_loss: 1.4685\n",
      "Epoch 648/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00648: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4467 - val_loss: 1.4719\n",
      "Epoch 649/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4459\n",
      "Epoch 00649: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4459 - val_loss: 1.4726\n",
      "Epoch 650/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4453\n",
      "Epoch 00650: loss did not improve from 1.44442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4453 - val_loss: 1.4706\n",
      "Epoch 651/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4443\n",
      "Epoch 00651: loss improved from 1.44442 to 1.44433, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4443 - val_loss: 1.4712\n",
      "Epoch 652/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 00652: loss improved from 1.44433 to 1.44367, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4437 - val_loss: 1.4703\n",
      "Epoch 653/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4447\n",
      "Epoch 00653: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4447 - val_loss: 1.4708\n",
      "Epoch 654/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00654: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4450 - val_loss: 1.4743\n",
      "Epoch 655/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4469\n",
      "Epoch 00655: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4469 - val_loss: 1.4806\n",
      "Epoch 656/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00656: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4492 - val_loss: 1.4749\n",
      "Epoch 657/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4488\n",
      "Epoch 00657: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4488 - val_loss: 1.4749\n",
      "Epoch 658/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4449\n",
      "Epoch 00658: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4449 - val_loss: 1.4687\n",
      "Epoch 659/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4443\n",
      "Epoch 00659: loss did not improve from 1.44367\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4443 - val_loss: 1.4690\n",
      "Epoch 660/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4431\n",
      "Epoch 00660: loss improved from 1.44367 to 1.44314, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4431 - val_loss: 1.4678\n",
      "Epoch 661/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 00661: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4437 - val_loss: 1.4707\n",
      "Epoch 662/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00662: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4442 - val_loss: 1.4688\n",
      "Epoch 663/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4443\n",
      "Epoch 00663: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4443 - val_loss: 1.4723\n",
      "Epoch 664/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4456\n",
      "Epoch 00664: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4456 - val_loss: 1.4700\n",
      "Epoch 665/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 00665: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4435 - val_loss: 1.4721\n",
      "Epoch 666/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4458\n",
      "Epoch 00666: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4458 - val_loss: 1.4721\n",
      "Epoch 667/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4449\n",
      "Epoch 00667: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4449 - val_loss: 1.4718\n",
      "Epoch 668/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4441\n",
      "Epoch 00668: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4441 - val_loss: 1.4719\n",
      "Epoch 669/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4449\n",
      "Epoch 00669: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4449 - val_loss: 1.4727\n",
      "Epoch 670/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00670: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4470 - val_loss: 1.4718\n",
      "Epoch 671/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4443\n",
      "Epoch 00671: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4443 - val_loss: 1.4732\n",
      "Epoch 672/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4462\n",
      "Epoch 00672: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4462 - val_loss: 1.4722\n",
      "Epoch 673/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4472\n",
      "Epoch 00673: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4472 - val_loss: 1.4697\n",
      "Epoch 674/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4455\n",
      "Epoch 00674: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4455 - val_loss: 1.4690\n",
      "Epoch 675/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4449\n",
      "Epoch 00675: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4449 - val_loss: 1.4693\n",
      "Epoch 676/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00676: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4450 - val_loss: 1.4746\n",
      "Epoch 677/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4447\n",
      "Epoch 00677: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4447 - val_loss: 1.4703\n",
      "Epoch 678/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 00678: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4463 - val_loss: 1.4702\n",
      "Epoch 679/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4457\n",
      "Epoch 00679: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4457 - val_loss: 1.4748\n",
      "Epoch 680/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4460\n",
      "Epoch 00680: loss did not improve from 1.44314\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4460 - val_loss: 1.4673\n",
      "Epoch 681/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 00681: loss improved from 1.44314 to 1.44266, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4427 - val_loss: 1.4717\n",
      "Epoch 682/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4432\n",
      "Epoch 00682: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4432 - val_loss: 1.4694\n",
      "Epoch 683/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4434\n",
      "Epoch 00683: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4434 - val_loss: 1.4700\n",
      "Epoch 684/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 00684: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4439 - val_loss: 1.4713\n",
      "Epoch 685/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 00685: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4463 - val_loss: 1.4719\n",
      "Epoch 686/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4460\n",
      "Epoch 00686: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4460 - val_loss: 1.4714\n",
      "Epoch 687/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4432\n",
      "Epoch 00687: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4432 - val_loss: 1.4729\n",
      "Epoch 688/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4471\n",
      "Epoch 00688: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4471 - val_loss: 1.4704\n",
      "Epoch 689/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00689: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4467 - val_loss: 1.4736\n",
      "Epoch 690/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4488\n",
      "Epoch 00690: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4488 - val_loss: 1.4701\n",
      "Epoch 691/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4502\n",
      "Epoch 00691: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4485 - val_loss: 1.4743\n",
      "Epoch 692/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00692: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4482 - val_loss: 1.4818\n",
      "Epoch 693/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4559\n",
      "Epoch 00693: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4559 - val_loss: 1.4718\n",
      "Epoch 694/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4500\n",
      "Epoch 00694: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4500 - val_loss: 1.4751\n",
      "Epoch 695/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4472\n",
      "Epoch 00695: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4472 - val_loss: 1.4719\n",
      "Epoch 696/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00696: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4442 - val_loss: 1.4699\n",
      "Epoch 697/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4457\n",
      "Epoch 00697: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4457 - val_loss: 1.4757\n",
      "Epoch 698/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4456\n",
      "Epoch 00698: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4456 - val_loss: 1.4717\n",
      "Epoch 699/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4431\n",
      "Epoch 00699: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4431 - val_loss: 1.4694\n",
      "Epoch 700/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00700: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4444 - val_loss: 1.4698\n",
      "Epoch 701/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 00701: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4463 - val_loss: 1.4746\n",
      "Epoch 702/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4454\n",
      "Epoch 00702: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4454 - val_loss: 1.4715\n",
      "Epoch 703/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00703: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4461 - val_loss: 1.4761\n",
      "Epoch 704/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4457\n",
      "Epoch 00704: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4457 - val_loss: 1.4728\n",
      "Epoch 705/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4482\n",
      "Epoch 00705: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4482 - val_loss: 1.4706\n",
      "Epoch 706/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4449\n",
      "Epoch 00706: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4449 - val_loss: 1.4710\n",
      "Epoch 707/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4451\n",
      "Epoch 00707: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4451 - val_loss: 1.4749\n",
      "Epoch 708/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4459\n",
      "Epoch 00708: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4459 - val_loss: 1.4715\n",
      "Epoch 709/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4458\n",
      "Epoch 00709: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4458 - val_loss: 1.4750\n",
      "Epoch 710/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4404\n",
      "Epoch 00710: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4472 - val_loss: 1.4736\n",
      "Epoch 711/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4456\n",
      "Epoch 00711: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4456 - val_loss: 1.4733\n",
      "Epoch 712/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4468\n",
      "Epoch 00712: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4468 - val_loss: 1.4721\n",
      "Epoch 713/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4473\n",
      "Epoch 00713: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4473 - val_loss: 1.4694\n",
      "Epoch 714/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4449\n",
      "Epoch 00714: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4449 - val_loss: 1.4701\n",
      "Epoch 715/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00715: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4461 - val_loss: 1.4756\n",
      "Epoch 716/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4485\n",
      "Epoch 00716: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4485 - val_loss: 1.4768\n",
      "Epoch 717/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4472\n",
      "Epoch 00717: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4472 - val_loss: 1.4749\n",
      "Epoch 718/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00718: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4448 - val_loss: 1.4695\n",
      "Epoch 719/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00719: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4442 - val_loss: 1.4697\n",
      "Epoch 720/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4459\n",
      "Epoch 00720: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4459 - val_loss: 1.4730\n",
      "Epoch 721/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00721: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4444 - val_loss: 1.4739\n",
      "Epoch 722/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4441\n",
      "Epoch 00722: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4441 - val_loss: 1.4711\n",
      "Epoch 723/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00723: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4444 - val_loss: 1.4703\n",
      "Epoch 724/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00724: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4430 - val_loss: 1.4693\n",
      "Epoch 725/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4438\n",
      "Epoch 00725: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4438 - val_loss: 1.4717\n",
      "Epoch 726/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 00726: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4437 - val_loss: 1.4720\n",
      "Epoch 727/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4456\n",
      "Epoch 00727: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4456 - val_loss: 1.4721\n",
      "Epoch 728/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4455\n",
      "Epoch 00728: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4455 - val_loss: 1.4751\n",
      "Epoch 729/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4446\n",
      "Epoch 00729: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4446 - val_loss: 1.4729\n",
      "Epoch 730/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4446\n",
      "Epoch 00730: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4446 - val_loss: 1.4695\n",
      "Epoch 731/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 00731: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4429 - val_loss: 1.4712\n",
      "Epoch 732/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4458\n",
      "Epoch 00732: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4458 - val_loss: 1.4713\n",
      "Epoch 733/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4588\n",
      "Epoch 00733: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4498 - val_loss: 1.4757\n",
      "Epoch 734/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4481\n",
      "Epoch 00734: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4481 - val_loss: 1.4753\n",
      "Epoch 735/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4489\n",
      "Epoch 00735: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4489 - val_loss: 1.4753\n",
      "Epoch 736/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 00736: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4463 - val_loss: 1.4778\n",
      "Epoch 737/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00737: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4442 - val_loss: 1.4706\n",
      "Epoch 738/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4476\n",
      "Epoch 00738: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4476 - val_loss: 1.4757\n",
      "Epoch 739/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4455\n",
      "Epoch 00739: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4455 - val_loss: 1.4679\n",
      "Epoch 740/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4434\n",
      "Epoch 00740: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4434 - val_loss: 1.4703\n",
      "Epoch 741/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4452\n",
      "Epoch 00741: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4452 - val_loss: 1.4710\n",
      "Epoch 742/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4454\n",
      "Epoch 00742: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4454 - val_loss: 1.4684\n",
      "Epoch 743/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 00743: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4439 - val_loss: 1.4769\n",
      "Epoch 744/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4485\n",
      "Epoch 00744: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4485 - val_loss: 1.4756\n",
      "Epoch 745/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4465\n",
      "Epoch 00745: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4465 - val_loss: 1.4725\n",
      "Epoch 746/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4479\n",
      "Epoch 00746: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4479 - val_loss: 1.4727\n",
      "Epoch 747/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4465\n",
      "Epoch 00747: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4465 - val_loss: 1.4752\n",
      "Epoch 748/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4480\n",
      "Epoch 00748: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4480 - val_loss: 1.4741\n",
      "Epoch 749/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4447\n",
      "Epoch 00749: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4447 - val_loss: 1.4783\n",
      "Epoch 750/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4456\n",
      "Epoch 00750: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4456 - val_loss: 1.4748\n",
      "Epoch 751/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00751: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4467 - val_loss: 1.4742\n",
      "Epoch 752/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00752: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4461 - val_loss: 1.4748\n",
      "Epoch 753/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00753: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4444 - val_loss: 1.4737\n",
      "Epoch 754/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4451\n",
      "Epoch 00754: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4451 - val_loss: 1.4831\n",
      "Epoch 755/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4509\n",
      "Epoch 00755: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4509 - val_loss: 1.4738\n",
      "Epoch 756/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4465\n",
      "Epoch 00756: loss did not improve from 1.44266\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4465 - val_loss: 1.4760\n",
      "Epoch 757/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4426\n",
      "Epoch 00757: loss improved from 1.44266 to 1.44255, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4426 - val_loss: 1.4702\n",
      "Epoch 758/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 00758: loss improved from 1.44255 to 1.44005, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4400 - val_loss: 1.4753\n",
      "Epoch 759/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4493\n",
      "Epoch 00759: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4493 - val_loss: 1.4692\n",
      "Epoch 760/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4493\n",
      "Epoch 00760: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4493 - val_loss: 1.4729\n",
      "Epoch 761/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00761: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4450 - val_loss: 1.4717\n",
      "Epoch 762/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4496\n",
      "Epoch 00762: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4496 - val_loss: 1.4745\n",
      "Epoch 763/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 00763: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4435 - val_loss: 1.4731\n",
      "Epoch 764/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4432\n",
      "Epoch 00764: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4432 - val_loss: 1.4763\n",
      "Epoch 765/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4461\n",
      "Epoch 00765: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4461 - val_loss: 1.4742\n",
      "Epoch 766/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4465\n",
      "Epoch 00766: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4465 - val_loss: 1.4776\n",
      "Epoch 767/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 00767: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4435 - val_loss: 1.4714\n",
      "Epoch 768/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4441\n",
      "Epoch 00768: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4441 - val_loss: 1.4718\n",
      "Epoch 769/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 00769: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4427 - val_loss: 1.4721\n",
      "Epoch 770/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00770: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4442 - val_loss: 1.4724\n",
      "Epoch 771/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4447\n",
      "Epoch 00771: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4447 - val_loss: 1.4775\n",
      "Epoch 772/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4473\n",
      "Epoch 00772: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4473 - val_loss: 1.4703\n",
      "Epoch 773/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4436\n",
      "Epoch 00773: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4436 - val_loss: 1.4711\n",
      "Epoch 774/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4453\n",
      "Epoch 00774: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4453 - val_loss: 1.4766\n",
      "Epoch 775/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4468\n",
      "Epoch 00775: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4468 - val_loss: 1.4815\n",
      "Epoch 776/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4503\n",
      "Epoch 00776: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4503 - val_loss: 1.4773\n",
      "Epoch 777/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4510\n",
      "Epoch 00777: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4510 - val_loss: 1.4771\n",
      "Epoch 778/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4478\n",
      "Epoch 00778: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4478 - val_loss: 1.4769\n",
      "Epoch 779/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4510\n",
      "Epoch 00779: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4510 - val_loss: 1.4750\n",
      "Epoch 780/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4473\n",
      "Epoch 00780: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4473 - val_loss: 1.4756\n",
      "Epoch 781/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4487\n",
      "Epoch 00781: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4487 - val_loss: 1.4782\n",
      "Epoch 782/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4469\n",
      "Epoch 00782: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4469 - val_loss: 1.4733\n",
      "Epoch 783/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00783: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4450 - val_loss: 1.4733\n",
      "Epoch 784/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4468\n",
      "Epoch 00784: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4468 - val_loss: 1.4740\n",
      "Epoch 785/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4492\n",
      "Epoch 00785: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4492 - val_loss: 1.4706\n",
      "Epoch 786/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4483\n",
      "Epoch 00786: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4483 - val_loss: 1.4712\n",
      "Epoch 787/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4434\n",
      "Epoch 00787: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4434 - val_loss: 1.4687\n",
      "Epoch 788/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4419\n",
      "Epoch 00788: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4419 - val_loss: 1.4739\n",
      "Epoch 789/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4425\n",
      "Epoch 00789: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4425 - val_loss: 1.4704\n",
      "Epoch 790/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 00790: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4414 - val_loss: 1.4733\n",
      "Epoch 791/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4443\n",
      "Epoch 00791: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4443 - val_loss: 1.4693\n",
      "Epoch 792/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00792: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4430 - val_loss: 1.4711\n",
      "Epoch 793/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4401\n",
      "Epoch 00793: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4401 - val_loss: 1.4700\n",
      "Epoch 794/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 00794: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4404 - val_loss: 1.4722\n",
      "Epoch 795/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 00795: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4421 - val_loss: 1.4748\n",
      "Epoch 796/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4434\n",
      "Epoch 00796: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4434 - val_loss: 1.4743\n",
      "Epoch 797/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4443\n",
      "Epoch 00797: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4443 - val_loss: 1.4779\n",
      "Epoch 798/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4441\n",
      "Epoch 00798: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4441 - val_loss: 1.4730\n",
      "Epoch 799/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4433\n",
      "Epoch 00799: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4433 - val_loss: 1.4740\n",
      "Epoch 800/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4440\n",
      "Epoch 00800: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4440 - val_loss: 1.4717\n",
      "Epoch 801/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4419\n",
      "Epoch 00801: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4419 - val_loss: 1.4708\n",
      "Epoch 802/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4425\n",
      "Epoch 00802: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4425 - val_loss: 1.4716\n",
      "Epoch 803/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 00803: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4418 - val_loss: 1.4719\n",
      "Epoch 804/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00804: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4430 - val_loss: 1.4706\n",
      "Epoch 805/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00805: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4417 - val_loss: 1.4675\n",
      "Epoch 806/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4426\n",
      "Epoch 00806: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4426 - val_loss: 1.4711\n",
      "Epoch 807/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 00807: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4435 - val_loss: 1.4726\n",
      "Epoch 808/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4431\n",
      "Epoch 00808: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4431 - val_loss: 1.4804\n",
      "Epoch 809/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4466\n",
      "Epoch 00809: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4466 - val_loss: 1.4723\n",
      "Epoch 810/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4447\n",
      "Epoch 00810: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4447 - val_loss: 1.4713\n",
      "Epoch 811/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 00811: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4420 - val_loss: 1.4735\n",
      "Epoch 812/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4454\n",
      "Epoch 00812: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4454 - val_loss: 1.4742\n",
      "Epoch 813/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4479\n",
      "Epoch 00813: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4479 - val_loss: 1.4739\n",
      "Epoch 814/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00814: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4430 - val_loss: 1.4719\n",
      "Epoch 815/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4451\n",
      "Epoch 00815: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4451 - val_loss: 1.4742\n",
      "Epoch 816/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4433\n",
      "Epoch 00816: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4433 - val_loss: 1.4722\n",
      "Epoch 817/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 00817: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4420 - val_loss: 1.4748\n",
      "Epoch 818/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 00818: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4420 - val_loss: 1.4802\n",
      "Epoch 819/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 00819: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4463 - val_loss: 1.4736\n",
      "Epoch 820/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00820: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4467 - val_loss: 1.4756\n",
      "Epoch 821/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00821: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4467 - val_loss: 1.4708\n",
      "Epoch 822/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4443\n",
      "Epoch 00822: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4443 - val_loss: 1.4723\n",
      "Epoch 823/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00823: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4430 - val_loss: 1.4717\n",
      "Epoch 824/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4422\n",
      "Epoch 00824: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4422 - val_loss: 1.4727\n",
      "Epoch 825/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00825: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4423 - val_loss: 1.4752\n",
      "Epoch 826/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00826: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4448 - val_loss: 1.4790\n",
      "Epoch 827/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4458\n",
      "Epoch 00827: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4458 - val_loss: 1.4750\n",
      "Epoch 828/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 00828: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4435 - val_loss: 1.4719\n",
      "Epoch 829/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4419\n",
      "Epoch 00829: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4419 - val_loss: 1.4718\n",
      "Epoch 830/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00830: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4424 - val_loss: 1.4775\n",
      "Epoch 831/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4454\n",
      "Epoch 00831: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4454 - val_loss: 1.4773\n",
      "Epoch 832/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4434\n",
      "Epoch 00832: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4434 - val_loss: 1.4735\n",
      "Epoch 833/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4479\n",
      "Epoch 00833: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4479 - val_loss: 1.4757\n",
      "Epoch 834/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4474\n",
      "Epoch 00834: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4474 - val_loss: 1.4777\n",
      "Epoch 835/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4459\n",
      "Epoch 00835: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4459 - val_loss: 1.4760\n",
      "Epoch 836/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4441\n",
      "Epoch 00836: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4441 - val_loss: 1.4748\n",
      "Epoch 837/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00837: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4424 - val_loss: 1.4724\n",
      "Epoch 838/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 00838: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4414 - val_loss: 1.4710\n",
      "Epoch 839/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4409\n",
      "Epoch 00839: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4409 - val_loss: 1.4742\n",
      "Epoch 840/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4416\n",
      "Epoch 00840: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4416 - val_loss: 1.4833\n",
      "Epoch 841/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4455\n",
      "Epoch 00841: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4455 - val_loss: 1.4707\n",
      "Epoch 842/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 00842: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4437 - val_loss: 1.4754\n",
      "Epoch 843/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4512\n",
      "Epoch 00843: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4454 - val_loss: 1.4714\n",
      "Epoch 844/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4467\n",
      "Epoch 00844: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4467 - val_loss: 1.4716\n",
      "Epoch 845/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4445\n",
      "Epoch 00845: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4445 - val_loss: 1.4732\n",
      "Epoch 846/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00846: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4417 - val_loss: 1.4736\n",
      "Epoch 847/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00847: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4424 - val_loss: 1.4714\n",
      "Epoch 848/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00848: loss did not improve from 1.44005\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4423 - val_loss: 1.4706\n",
      "Epoch 849/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 00849: loss improved from 1.44005 to 1.43996, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4400 - val_loss: 1.4731\n",
      "Epoch 850/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4425\n",
      "Epoch 00850: loss did not improve from 1.43996\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4425 - val_loss: 1.4739\n",
      "Epoch 851/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00851: loss did not improve from 1.43996\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4423 - val_loss: 1.4710\n",
      "Epoch 852/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 00852: loss improved from 1.43996 to 1.43930, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4393 - val_loss: 1.4714\n",
      "Epoch 853/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 00853: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4399 - val_loss: 1.4734\n",
      "Epoch 854/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 00854: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4439 - val_loss: 1.4739\n",
      "Epoch 855/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00855: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4424 - val_loss: 1.4686\n",
      "Epoch 856/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4469\n",
      "Epoch 00856: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4469 - val_loss: 1.4753\n",
      "Epoch 857/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4514\n",
      "Epoch 00857: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4514 - val_loss: 1.4729\n",
      "Epoch 858/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4470\n",
      "Epoch 00858: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4470 - val_loss: 1.4740\n",
      "Epoch 859/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00859: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4448 - val_loss: 1.4755\n",
      "Epoch 860/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 00860: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4418 - val_loss: 1.4745\n",
      "Epoch 861/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00861: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4423 - val_loss: 1.4738\n",
      "Epoch 862/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 00862: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4439 - val_loss: 1.4727\n",
      "Epoch 863/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4438\n",
      "Epoch 00863: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4438 - val_loss: 1.4754\n",
      "Epoch 864/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 00864: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4435 - val_loss: 1.4769\n",
      "Epoch 865/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4410\n",
      "Epoch 00865: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4410 - val_loss: 1.4719\n",
      "Epoch 866/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4412\n",
      "Epoch 00866: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4412 - val_loss: 1.4702\n",
      "Epoch 867/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4415\n",
      "Epoch 00867: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4415 - val_loss: 1.4736\n",
      "Epoch 868/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 00868: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4420 - val_loss: 1.4715\n",
      "Epoch 869/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00869: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4417 - val_loss: 1.4734\n",
      "Epoch 870/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4408\n",
      "Epoch 00870: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4408 - val_loss: 1.4746\n",
      "Epoch 871/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00871: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4450 - val_loss: 1.4759\n",
      "Epoch 872/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 00872: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4421 - val_loss: 1.4729\n",
      "Epoch 873/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00873: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4417 - val_loss: 1.4731\n",
      "Epoch 874/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4415\n",
      "Epoch 00874: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4415 - val_loss: 1.4720\n",
      "Epoch 875/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 00875: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4418 - val_loss: 1.4702\n",
      "Epoch 876/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4396\n",
      "Epoch 00876: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4396 - val_loss: 1.4706\n",
      "Epoch 877/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4396\n",
      "Epoch 00877: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4396 - val_loss: 1.4726\n",
      "Epoch 878/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 00878: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4395 - val_loss: 1.4737\n",
      "Epoch 879/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 00879: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4393 - val_loss: 1.4758\n",
      "Epoch 880/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00880: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4417 - val_loss: 1.4744\n",
      "Epoch 881/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 00881: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4407 - val_loss: 1.4731\n",
      "Epoch 882/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 00882: loss did not improve from 1.43930\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4400 - val_loss: 1.4722\n",
      "Epoch 883/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4313\n",
      "Epoch 00883: loss improved from 1.43930 to 1.43917, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4392 - val_loss: 1.4759\n",
      "Epoch 884/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00884: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4425 - val_loss: 1.4765\n",
      "Epoch 885/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00885: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4430 - val_loss: 1.4732\n",
      "Epoch 886/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 00886: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4404 - val_loss: 1.4747\n",
      "Epoch 887/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4419\n",
      "Epoch 00887: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4419 - val_loss: 1.4746\n",
      "Epoch 888/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4433\n",
      "Epoch 00888: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4433 - val_loss: 1.4724\n",
      "Epoch 889/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 00889: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4435 - val_loss: 1.4785\n",
      "Epoch 890/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00890: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4424 - val_loss: 1.4753\n",
      "Epoch 891/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4431\n",
      "Epoch 00891: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4431 - val_loss: 1.4725\n",
      "Epoch 892/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 00892: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4437 - val_loss: 1.4739\n",
      "Epoch 893/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4436\n",
      "Epoch 00893: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4436 - val_loss: 1.4720\n",
      "Epoch 894/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 00894: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4420 - val_loss: 1.4739\n",
      "Epoch 895/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 00895: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4411 - val_loss: 1.4733\n",
      "Epoch 896/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4408\n",
      "Epoch 00896: loss did not improve from 1.43917\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4408 - val_loss: 1.4717\n",
      "Epoch 897/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4389\n",
      "Epoch 00897: loss improved from 1.43917 to 1.43895, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4389 - val_loss: 1.4780\n",
      "Epoch 898/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4440\n",
      "Epoch 00898: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4440 - val_loss: 1.4744\n",
      "Epoch 899/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00899: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4417 - val_loss: 1.4723\n",
      "Epoch 900/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4426\n",
      "Epoch 00900: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4426 - val_loss: 1.4754\n",
      "Epoch 901/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 00901: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4400 - val_loss: 1.4739\n",
      "Epoch 902/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 00902: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4400 - val_loss: 1.4747\n",
      "Epoch 903/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 00903: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4400 - val_loss: 1.4720\n",
      "Epoch 904/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4410\n",
      "Epoch 00904: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4410 - val_loss: 1.4764\n",
      "Epoch 905/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4410\n",
      "Epoch 00905: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4410 - val_loss: 1.4754\n",
      "Epoch 906/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4423\n",
      "Epoch 00906: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4423 - val_loss: 1.4820\n",
      "Epoch 907/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4456\n",
      "Epoch 00907: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4456 - val_loss: 1.4785\n",
      "Epoch 908/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 00908: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4450 - val_loss: 1.4764\n",
      "Epoch 909/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4412\n",
      "Epoch 00909: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4412 - val_loss: 1.4733\n",
      "Epoch 910/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4401\n",
      "Epoch 00910: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4401 - val_loss: 1.4755\n",
      "Epoch 911/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00911: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4430 - val_loss: 1.4745\n",
      "Epoch 912/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 00912: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4429 - val_loss: 1.4742\n",
      "Epoch 913/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4409\n",
      "Epoch 00913: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4409 - val_loss: 1.4728\n",
      "Epoch 914/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4473\n",
      "Epoch 00914: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4473 - val_loss: 1.4781\n",
      "Epoch 915/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 00915: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4463 - val_loss: 1.4800\n",
      "Epoch 916/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4511\n",
      "Epoch 00916: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4511 - val_loss: 1.4733\n",
      "Epoch 917/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4498\n",
      "Epoch 00917: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4498 - val_loss: 1.4781\n",
      "Epoch 918/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4452\n",
      "Epoch 00918: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4452 - val_loss: 1.4769\n",
      "Epoch 919/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 00919: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4429 - val_loss: 1.4737\n",
      "Epoch 920/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4401\n",
      "Epoch 00920: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4401 - val_loss: 1.4713\n",
      "Epoch 921/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 00921: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4411 - val_loss: 1.4711\n",
      "Epoch 922/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4416\n",
      "Epoch 00922: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4416 - val_loss: 1.4719\n",
      "Epoch 923/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 00923: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4421 - val_loss: 1.4758\n",
      "Epoch 924/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4405\n",
      "Epoch 00924: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4405 - val_loss: 1.4751\n",
      "Epoch 925/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00925: loss did not improve from 1.43895\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4417 - val_loss: 1.4702\n",
      "Epoch 926/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 00926: loss improved from 1.43895 to 1.43851, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4385 - val_loss: 1.4731\n",
      "Epoch 927/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4380\n",
      "Epoch 00927: loss improved from 1.43851 to 1.43798, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4380 - val_loss: 1.4732\n",
      "Epoch 928/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 00928: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4392 - val_loss: 1.4727\n",
      "Epoch 929/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 00929: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4390 - val_loss: 1.4775\n",
      "Epoch 930/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00930: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4417 - val_loss: 1.4734\n",
      "Epoch 931/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00931: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4424 - val_loss: 1.4725\n",
      "Epoch 932/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4428\n",
      "Epoch 00932: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4428 - val_loss: 1.4739\n",
      "Epoch 933/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4406\n",
      "Epoch 00933: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4406 - val_loss: 1.4731\n",
      "Epoch 934/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 00934: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4407 - val_loss: 1.4765\n",
      "Epoch 935/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4472\n",
      "Epoch 00935: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4472 - val_loss: 1.4774\n",
      "Epoch 936/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4453\n",
      "Epoch 00936: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4453 - val_loss: 1.4735\n",
      "Epoch 937/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00937: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4430 - val_loss: 1.4726\n",
      "Epoch 938/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4403\n",
      "Epoch 00938: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4403 - val_loss: 1.4771\n",
      "Epoch 939/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4397\n",
      "Epoch 00939: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4397 - val_loss: 1.4746\n",
      "Epoch 940/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 00940: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4399 - val_loss: 1.4750\n",
      "Epoch 941/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 00941: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4411 - val_loss: 1.4774\n",
      "Epoch 942/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 00942: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4427 - val_loss: 1.4797\n",
      "Epoch 943/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 00943: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4444 - val_loss: 1.4731\n",
      "Epoch 944/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00944: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4430 - val_loss: 1.4740\n",
      "Epoch 945/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 00945: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4388 - val_loss: 1.4725\n",
      "Epoch 946/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4408\n",
      "Epoch 00946: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4408 - val_loss: 1.4704\n",
      "Epoch 947/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4406\n",
      "Epoch 00947: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4406 - val_loss: 1.4732\n",
      "Epoch 948/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4493\n",
      "Epoch 00948: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4493 - val_loss: 1.4784\n",
      "Epoch 949/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4462\n",
      "Epoch 00949: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4462 - val_loss: 1.4747\n",
      "Epoch 950/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4466\n",
      "Epoch 00950: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4466 - val_loss: 1.4773\n",
      "Epoch 951/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 00951: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4442 - val_loss: 1.4720\n",
      "Epoch 952/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4445\n",
      "Epoch 00952: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4445 - val_loss: 1.4764\n",
      "Epoch 953/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00953: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4430 - val_loss: 1.4734\n",
      "Epoch 954/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 00954: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4414 - val_loss: 1.4776\n",
      "Epoch 955/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4452\n",
      "Epoch 00955: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4452 - val_loss: 1.4712\n",
      "Epoch 956/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00956: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4448 - val_loss: 1.4752\n",
      "Epoch 957/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 00957: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4414 - val_loss: 1.4714\n",
      "Epoch 958/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00958: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4424 - val_loss: 1.4753\n",
      "Epoch 959/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 00959: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4430 - val_loss: 1.4755\n",
      "Epoch 960/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 00960: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4424 - val_loss: 1.4732\n",
      "Epoch 961/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 00961: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4421 - val_loss: 1.4750\n",
      "Epoch 962/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4408\n",
      "Epoch 00962: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4408 - val_loss: 1.4741\n",
      "Epoch 963/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4412\n",
      "Epoch 00963: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4412 - val_loss: 1.4772\n",
      "Epoch 964/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4413\n",
      "Epoch 00964: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4413 - val_loss: 1.4778\n",
      "Epoch 965/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4454\n",
      "Epoch 00965: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4454 - val_loss: 1.4739\n",
      "Epoch 966/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4594\n",
      "Epoch 00966: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4594 - val_loss: 1.4763\n",
      "Epoch 967/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4476\n",
      "Epoch 00967: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4476 - val_loss: 1.4782\n",
      "Epoch 968/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4472\n",
      "Epoch 00968: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4472 - val_loss: 1.4762\n",
      "Epoch 969/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4433\n",
      "Epoch 00969: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4433 - val_loss: 1.4711\n",
      "Epoch 970/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4428\n",
      "Epoch 00970: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4428 - val_loss: 1.4743\n",
      "Epoch 971/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00971: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4417 - val_loss: 1.4735\n",
      "Epoch 972/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 00972: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4395 - val_loss: 1.4776\n",
      "Epoch 973/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4412\n",
      "Epoch 00973: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4412 - val_loss: 1.4756\n",
      "Epoch 974/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 00974: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4407 - val_loss: 1.4731\n",
      "Epoch 975/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 00975: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4404 - val_loss: 1.4743\n",
      "Epoch 976/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 00976: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4414 - val_loss: 1.4750\n",
      "Epoch 977/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4419\n",
      "Epoch 00977: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4419 - val_loss: 1.4757\n",
      "Epoch 978/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4410\n",
      "Epoch 00978: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4410 - val_loss: 1.4750\n",
      "Epoch 979/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4409\n",
      "Epoch 00979: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4409 - val_loss: 1.4719\n",
      "Epoch 980/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4409\n",
      "Epoch 00980: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4409 - val_loss: 1.4765\n",
      "Epoch 981/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4415\n",
      "Epoch 00981: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4415 - val_loss: 1.4757\n",
      "Epoch 982/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 00982: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4404 - val_loss: 1.4726\n",
      "Epoch 983/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 00983: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4392 - val_loss: 1.4743\n",
      "Epoch 984/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 00984: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4404 - val_loss: 1.4726\n",
      "Epoch 985/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4406\n",
      "Epoch 00985: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4406 - val_loss: 1.4740\n",
      "Epoch 986/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 00986: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4417 - val_loss: 1.4753\n",
      "Epoch 987/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4406\n",
      "Epoch 00987: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4406 - val_loss: 1.4731\n",
      "Epoch 988/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 00988: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4395 - val_loss: 1.4706\n",
      "Epoch 989/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 00989: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4385 - val_loss: 1.4749\n",
      "Epoch 990/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4384\n",
      "Epoch 00990: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4384 - val_loss: 1.4758\n",
      "Epoch 991/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4394\n",
      "Epoch 00991: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4394 - val_loss: 1.4791\n",
      "Epoch 992/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 00992: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4420 - val_loss: 1.4717\n",
      "Epoch 993/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4398\n",
      "Epoch 00993: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4398 - val_loss: 1.4771\n",
      "Epoch 994/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 00994: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4418 - val_loss: 1.4718\n",
      "Epoch 995/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4425\n",
      "Epoch 00995: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4425 - val_loss: 1.4812\n",
      "Epoch 996/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4432\n",
      "Epoch 00996: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4432 - val_loss: 1.4752\n",
      "Epoch 997/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4422\n",
      "Epoch 00997: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4422 - val_loss: 1.4764\n",
      "Epoch 998/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4448\n",
      "Epoch 00998: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4448 - val_loss: 1.4792\n",
      "Epoch 999/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4405\n",
      "Epoch 00999: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4405 - val_loss: 1.4724\n",
      "Epoch 1000/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 01000: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4427 - val_loss: 1.4812\n",
      "Epoch 1001/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4457\n",
      "Epoch 01001: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4457 - val_loss: 1.4791\n",
      "Epoch 1002/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 01002: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4427 - val_loss: 1.4756\n",
      "Epoch 1003/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 01003: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4407 - val_loss: 1.4738\n",
      "Epoch 1004/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 01004: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4417 - val_loss: 1.4743\n",
      "Epoch 1005/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4435\n",
      "Epoch 01005: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4435 - val_loss: 1.4777\n",
      "Epoch 1006/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4428\n",
      "Epoch 01006: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4428 - val_loss: 1.4749\n",
      "Epoch 1007/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4405\n",
      "Epoch 01007: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4405 - val_loss: 1.4829\n",
      "Epoch 1008/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 01008: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4424 - val_loss: 1.4785\n",
      "Epoch 1009/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4396\n",
      "Epoch 01009: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4396 - val_loss: 1.4743\n",
      "Epoch 1010/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4401\n",
      "Epoch 01010: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4401 - val_loss: 1.4722\n",
      "Epoch 1011/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4394\n",
      "Epoch 01011: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4394 - val_loss: 1.4728\n",
      "Epoch 1012/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4422\n",
      "Epoch 01012: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4422 - val_loss: 1.4721\n",
      "Epoch 1013/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4419\n",
      "Epoch 01013: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4419 - val_loss: 1.4737\n",
      "Epoch 1014/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 01014: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4385 - val_loss: 1.4742\n",
      "Epoch 1015/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 01015: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4385 - val_loss: 1.4731\n",
      "Epoch 1016/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4380\n",
      "Epoch 01016: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4380 - val_loss: 1.4732\n",
      "Epoch 1017/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 01017: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4404 - val_loss: 1.4726\n",
      "Epoch 1018/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4406\n",
      "Epoch 01018: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4406 - val_loss: 1.4818\n",
      "Epoch 1019/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 01019: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4411 - val_loss: 1.4736\n",
      "Epoch 1020/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 01020: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4411 - val_loss: 1.4755\n",
      "Epoch 1021/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 01021: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4424 - val_loss: 1.4738\n",
      "Epoch 1022/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4428\n",
      "Epoch 01022: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4428 - val_loss: 1.4759\n",
      "Epoch 1023/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4398\n",
      "Epoch 01023: loss did not improve from 1.43798\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4398 - val_loss: 1.4730\n",
      "Epoch 1024/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01024: loss improved from 1.43798 to 1.43722, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4372 - val_loss: 1.4766\n",
      "Epoch 1025/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 01025: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4385 - val_loss: 1.4751\n",
      "Epoch 1026/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4386\n",
      "Epoch 01026: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4386 - val_loss: 1.4719\n",
      "Epoch 1027/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01027: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4392 - val_loss: 1.4748\n",
      "Epoch 1028/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01028: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4387 - val_loss: 1.4725\n",
      "Epoch 1029/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01029: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4377 - val_loss: 1.4748\n",
      "Epoch 1030/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4409\n",
      "Epoch 01030: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4409 - val_loss: 1.4773\n",
      "Epoch 1031/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 01031: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4420 - val_loss: 1.4737\n",
      "Epoch 1032/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 01032: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4374 - val_loss: 1.4768\n",
      "Epoch 1033/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4383\n",
      "Epoch 01033: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4383 - val_loss: 1.4719\n",
      "Epoch 1034/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 01034: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4395 - val_loss: 1.4705\n",
      "Epoch 1035/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 01035: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4393 - val_loss: 1.4770\n",
      "Epoch 1036/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01036: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4377 - val_loss: 1.4750\n",
      "Epoch 1037/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4383\n",
      "Epoch 01037: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4383 - val_loss: 1.4797\n",
      "Epoch 1038/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4391\n",
      "Epoch 01038: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4391 - val_loss: 1.4750\n",
      "Epoch 1039/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 01039: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4393 - val_loss: 1.4746\n",
      "Epoch 1040/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4397\n",
      "Epoch 01040: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4397 - val_loss: 1.4724\n",
      "Epoch 1041/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4397\n",
      "Epoch 01041: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4397 - val_loss: 1.4776\n",
      "Epoch 1042/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01042: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4388 - val_loss: 1.4737\n",
      "Epoch 1043/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01043: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4382 - val_loss: 1.4786\n",
      "Epoch 1044/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 01044: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4411 - val_loss: 1.4717\n",
      "Epoch 1045/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01045: loss did not improve from 1.43722\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4392 - val_loss: 1.4732\n",
      "Epoch 1046/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01046: loss improved from 1.43722 to 1.43698, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4370 - val_loss: 1.4722\n",
      "Epoch 1047/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01047: loss improved from 1.43698 to 1.43609, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4361 - val_loss: 1.4769\n",
      "Epoch 1048/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4384\n",
      "Epoch 01048: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4384 - val_loss: 1.4743\n",
      "Epoch 1049/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01049: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4388 - val_loss: 1.4739\n",
      "Epoch 1050/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01050: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4392 - val_loss: 1.4718\n",
      "Epoch 1051/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01051: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4382 - val_loss: 1.4741\n",
      "Epoch 1052/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 01052: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4407 - val_loss: 1.4744\n",
      "Epoch 1053/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 01053: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4421 - val_loss: 1.4800\n",
      "Epoch 1054/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 01054: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4427 - val_loss: 1.4765\n",
      "Epoch 1055/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4408\n",
      "Epoch 01055: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4408 - val_loss: 1.4773\n",
      "Epoch 1056/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01056: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4388 - val_loss: 1.4724\n",
      "Epoch 1057/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 01057: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4374 - val_loss: 1.4740\n",
      "Epoch 1058/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01058: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4370 - val_loss: 1.4752\n",
      "Epoch 1059/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4379\n",
      "Epoch 01059: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4379 - val_loss: 1.4771\n",
      "Epoch 1060/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4386\n",
      "Epoch 01060: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4386 - val_loss: 1.4849\n",
      "Epoch 1061/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4408\n",
      "Epoch 01061: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4408 - val_loss: 1.4805\n",
      "Epoch 1062/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 01062: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4418 - val_loss: 1.4736\n",
      "Epoch 1063/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 01063: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4407 - val_loss: 1.4716\n",
      "Epoch 1064/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01064: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4392 - val_loss: 1.4728\n",
      "Epoch 1065/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01065: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4388 - val_loss: 1.4759\n",
      "Epoch 1066/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4414\n",
      "Epoch 01066: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4414 - val_loss: 1.4774\n",
      "Epoch 1067/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01067: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4387 - val_loss: 1.4743\n",
      "Epoch 1068/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4375\n",
      "Epoch 01068: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4375 - val_loss: 1.4747\n",
      "Epoch 1069/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01069: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4765\n",
      "Epoch 1070/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 01070: loss did not improve from 1.43609\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4374 - val_loss: 1.4742\n",
      "Epoch 1071/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01071: loss improved from 1.43609 to 1.43583, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4358 - val_loss: 1.4741\n",
      "Epoch 1072/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4373\n",
      "Epoch 01072: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4373 - val_loss: 1.4747\n",
      "Epoch 1073/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 01073: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4385 - val_loss: 1.4744\n",
      "Epoch 1074/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4369\n",
      "Epoch 01074: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4369 - val_loss: 1.4703\n",
      "Epoch 1075/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4551\n",
      "Epoch 01075: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4551 - val_loss: 1.4761\n",
      "Epoch 1076/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4507\n",
      "Epoch 01076: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4507 - val_loss: 1.4753\n",
      "Epoch 1077/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 01077: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4463 - val_loss: 1.4738\n",
      "Epoch 1078/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4444\n",
      "Epoch 01078: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4444 - val_loss: 1.4728\n",
      "Epoch 1079/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 01079: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4420 - val_loss: 1.4729\n",
      "Epoch 1080/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4402\n",
      "Epoch 01080: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4402 - val_loss: 1.4742\n",
      "Epoch 1081/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 01081: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4404 - val_loss: 1.4766\n",
      "Epoch 1082/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 01082: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4417 - val_loss: 1.4740\n",
      "Epoch 1083/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01083: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4387 - val_loss: 1.4759\n",
      "Epoch 1084/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4398\n",
      "Epoch 01084: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4398 - val_loss: 1.4762\n",
      "Epoch 1085/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 01085: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4395 - val_loss: 1.4785\n",
      "Epoch 1086/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4421\n",
      "Epoch 01086: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4421 - val_loss: 1.4758\n",
      "Epoch 1087/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4398\n",
      "Epoch 01087: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4398 - val_loss: 1.4771\n",
      "Epoch 1088/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4406\n",
      "Epoch 01088: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4406 - val_loss: 1.4751\n",
      "Epoch 1089/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01089: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4387 - val_loss: 1.4751\n",
      "Epoch 1090/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01090: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4388 - val_loss: 1.4758\n",
      "Epoch 1091/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4383\n",
      "Epoch 01091: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4383 - val_loss: 1.4753\n",
      "Epoch 1092/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4397\n",
      "Epoch 01092: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4397 - val_loss: 1.4778\n",
      "Epoch 1093/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01093: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4390 - val_loss: 1.4776\n",
      "Epoch 1094/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4400\n",
      "Epoch 01094: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4400 - val_loss: 1.4769\n",
      "Epoch 1095/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4398\n",
      "Epoch 01095: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4398 - val_loss: 1.4754\n",
      "Epoch 1096/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 01096: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4395 - val_loss: 1.4735\n",
      "Epoch 1097/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01097: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4372 - val_loss: 1.4749\n",
      "Epoch 1098/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01098: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4382 - val_loss: 1.4730\n",
      "Epoch 1099/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4379\n",
      "Epoch 01099: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4379 - val_loss: 1.4773\n",
      "Epoch 1100/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4413\n",
      "Epoch 01100: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4413 - val_loss: 1.4800\n",
      "Epoch 1101/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4418\n",
      "Epoch 01101: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4418 - val_loss: 1.4838\n",
      "Epoch 1102/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 01102: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4427 - val_loss: 1.4750\n",
      "Epoch 1103/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 01103: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4417 - val_loss: 1.4772\n",
      "Epoch 1104/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01104: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4390 - val_loss: 1.4775\n",
      "Epoch 1105/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4381\n",
      "Epoch 01105: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4381 - val_loss: 1.4803\n",
      "Epoch 1106/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 01106: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4393 - val_loss: 1.4778\n",
      "Epoch 1107/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01107: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4390 - val_loss: 1.4751\n",
      "Epoch 1108/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4384\n",
      "Epoch 01108: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4384 - val_loss: 1.4741\n",
      "Epoch 1109/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4383\n",
      "Epoch 01109: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4383 - val_loss: 1.4738\n",
      "Epoch 1110/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01110: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4370 - val_loss: 1.4733\n",
      "Epoch 1111/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4366\n",
      "Epoch 01111: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4366 - val_loss: 1.4786\n",
      "Epoch 1112/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4408\n",
      "Epoch 01112: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4408 - val_loss: 1.4756\n",
      "Epoch 1113/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4373\n",
      "Epoch 01113: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4373 - val_loss: 1.4769\n",
      "Epoch 1114/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4380\n",
      "Epoch 01114: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4380 - val_loss: 1.4758\n",
      "Epoch 1115/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4383\n",
      "Epoch 01115: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4383 - val_loss: 1.4774\n",
      "Epoch 1116/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4389\n",
      "Epoch 01116: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4389 - val_loss: 1.4774\n",
      "Epoch 1117/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01117: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4388 - val_loss: 1.4757\n",
      "Epoch 1118/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01118: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4370 - val_loss: 1.4724\n",
      "Epoch 1119/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4380\n",
      "Epoch 01119: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4380 - val_loss: 1.4763\n",
      "Epoch 1120/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4362\n",
      "Epoch 01120: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4362 - val_loss: 1.4748\n",
      "Epoch 1121/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4376\n",
      "Epoch 01121: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4376 - val_loss: 1.4819\n",
      "Epoch 1122/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4413\n",
      "Epoch 01122: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4413 - val_loss: 1.4786\n",
      "Epoch 1123/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4417\n",
      "Epoch 01123: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4417 - val_loss: 1.4772\n",
      "Epoch 1124/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4405\n",
      "Epoch 01124: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4405 - val_loss: 1.4780\n",
      "Epoch 1125/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01125: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4382 - val_loss: 1.4718\n",
      "Epoch 1126/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01126: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4387 - val_loss: 1.4750\n",
      "Epoch 1127/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4398\n",
      "Epoch 01127: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4398 - val_loss: 1.4767\n",
      "Epoch 1128/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4402\n",
      "Epoch 01128: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4402 - val_loss: 1.4767\n",
      "Epoch 1129/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4397\n",
      "Epoch 01129: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4397 - val_loss: 1.4742\n",
      "Epoch 1130/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01130: loss did not improve from 1.43583\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4390 - val_loss: 1.4748\n",
      "Epoch 1131/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01131: loss improved from 1.43583 to 1.43565, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4357 - val_loss: 1.4761\n",
      "Epoch 1132/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01132: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4364 - val_loss: 1.4766\n",
      "Epoch 1133/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4376\n",
      "Epoch 01133: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4376 - val_loss: 1.4784\n",
      "Epoch 1134/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4380\n",
      "Epoch 01134: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4380 - val_loss: 1.4791\n",
      "Epoch 1135/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4376\n",
      "Epoch 01135: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4376 - val_loss: 1.4770\n",
      "Epoch 1136/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01136: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4387 - val_loss: 1.4827\n",
      "Epoch 1137/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4433\n",
      "Epoch 01137: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4433 - val_loss: 1.4758\n",
      "Epoch 1138/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4378\n",
      "Epoch 01138: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4378 - val_loss: 1.4757\n",
      "Epoch 1139/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01139: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4737\n",
      "Epoch 1140/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4369\n",
      "Epoch 01140: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4369 - val_loss: 1.4740\n",
      "Epoch 1141/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01141: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4377 - val_loss: 1.4741\n",
      "Epoch 1142/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01142: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4371 - val_loss: 1.4784\n",
      "Epoch 1143/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4375\n",
      "Epoch 01143: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4375 - val_loss: 1.4761\n",
      "Epoch 1144/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01144: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4363 - val_loss: 1.4747\n",
      "Epoch 1145/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01145: loss did not improve from 1.43565\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4363 - val_loss: 1.4745\n",
      "Epoch 1146/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01146: loss improved from 1.43565 to 1.43508, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4351 - val_loss: 1.4747\n",
      "Epoch 1147/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01147: loss improved from 1.43508 to 1.43423, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4342 - val_loss: 1.4753\n",
      "Epoch 1148/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01148: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4723\n",
      "Epoch 1149/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01149: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4361 - val_loss: 1.4755\n",
      "Epoch 1150/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 01150: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4393 - val_loss: 1.4776\n",
      "Epoch 1151/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 01151: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4374 - val_loss: 1.4752\n",
      "Epoch 1152/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4422\n",
      "Epoch 01152: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4367 - val_loss: 1.4756\n",
      "Epoch 1153/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01153: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4353 - val_loss: 1.4757\n",
      "Epoch 1154/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01154: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4367 - val_loss: 1.4764\n",
      "Epoch 1155/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4211\n",
      "Epoch 01155: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4369 - val_loss: 1.4783\n",
      "Epoch 1156/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4402\n",
      "Epoch 01156: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4360 - val_loss: 1.4772\n",
      "Epoch 1157/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4395\n",
      "Epoch 01157: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4395 - val_loss: 1.4843\n",
      "Epoch 1158/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4522\n",
      "Epoch 01158: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4522 - val_loss: 1.4805\n",
      "Epoch 1159/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4524\n",
      "Epoch 01159: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4524 - val_loss: 1.4796\n",
      "Epoch 1160/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4445\n",
      "Epoch 01160: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4445 - val_loss: 1.4756\n",
      "Epoch 1161/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 01161: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4420 - val_loss: 1.4753\n",
      "Epoch 1162/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4425\n",
      "Epoch 01162: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4425 - val_loss: 1.4773\n",
      "Epoch 1163/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4438\n",
      "Epoch 01163: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4438 - val_loss: 1.4879\n",
      "Epoch 1164/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01164: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4454 - val_loss: 1.4773\n",
      "Epoch 1165/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4403\n",
      "Epoch 01165: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4403 - val_loss: 1.4751\n",
      "Epoch 1166/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 01166: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4385 - val_loss: 1.4748\n",
      "Epoch 1167/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4369\n",
      "Epoch 01167: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4369 - val_loss: 1.4730\n",
      "Epoch 1168/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01168: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4759\n",
      "Epoch 1169/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01169: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4367 - val_loss: 1.4819\n",
      "Epoch 1170/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4419\n",
      "Epoch 01170: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4419 - val_loss: 1.4784\n",
      "Epoch 1171/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4412\n",
      "Epoch 01171: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4412 - val_loss: 1.4754\n",
      "Epoch 1172/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 01172: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4407 - val_loss: 1.4762\n",
      "Epoch 1173/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 01173: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4420 - val_loss: 1.4796\n",
      "Epoch 1174/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4405\n",
      "Epoch 01174: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4405 - val_loss: 1.4774\n",
      "Epoch 1175/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 01175: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4374 - val_loss: 1.4741\n",
      "Epoch 1176/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01176: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4382 - val_loss: 1.4762\n",
      "Epoch 1177/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4424\n",
      "Epoch 01177: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4424 - val_loss: 1.4810\n",
      "Epoch 1178/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4450\n",
      "Epoch 01178: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4450 - val_loss: 1.4762\n",
      "Epoch 1179/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4453\n",
      "Epoch 01179: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4453 - val_loss: 1.4782\n",
      "Epoch 1180/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4391\n",
      "Epoch 01180: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4391 - val_loss: 1.4766\n",
      "Epoch 1181/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01181: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4372 - val_loss: 1.4731\n",
      "Epoch 1182/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01182: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4371 - val_loss: 1.4752\n",
      "Epoch 1183/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01183: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4367 - val_loss: 1.4761\n",
      "Epoch 1184/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01184: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4368 - val_loss: 1.4766\n",
      "Epoch 1185/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01185: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4345 - val_loss: 1.4738\n",
      "Epoch 1186/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01186: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4365 - val_loss: 1.4762\n",
      "Epoch 1187/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01187: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4382 - val_loss: 1.4798\n",
      "Epoch 1188/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01188: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4365 - val_loss: 1.4808\n",
      "Epoch 1189/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 01189: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4374 - val_loss: 1.4769\n",
      "Epoch 1190/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01190: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4355 - val_loss: 1.4739\n",
      "Epoch 1191/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4348\n",
      "Epoch 01191: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4348 - val_loss: 1.4751\n",
      "Epoch 1192/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4354\n",
      "Epoch 01192: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4354 - val_loss: 1.4742\n",
      "Epoch 1193/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01193: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4364 - val_loss: 1.4775\n",
      "Epoch 1194/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4380\n",
      "Epoch 01194: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4380 - val_loss: 1.4837\n",
      "Epoch 1195/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01195: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4390 - val_loss: 1.4796\n",
      "Epoch 1196/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 01196: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4404 - val_loss: 1.4787\n",
      "Epoch 1197/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4394\n",
      "Epoch 01197: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4394 - val_loss: 1.4747\n",
      "Epoch 1198/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01198: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4730\n",
      "Epoch 1199/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01199: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4353 - val_loss: 1.4730\n",
      "Epoch 1200/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01200: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4357 - val_loss: 1.4775\n",
      "Epoch 1201/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4360\n",
      "Epoch 01201: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4360 - val_loss: 1.4745\n",
      "Epoch 1202/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01202: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4357 - val_loss: 1.4784\n",
      "Epoch 1203/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01203: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4387 - val_loss: 1.4826\n",
      "Epoch 1204/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4389\n",
      "Epoch 01204: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4389 - val_loss: 1.4745\n",
      "Epoch 1205/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01205: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4377 - val_loss: 1.4741\n",
      "Epoch 1206/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01206: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4377 - val_loss: 1.4763\n",
      "Epoch 1207/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4366\n",
      "Epoch 01207: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4366 - val_loss: 1.4757\n",
      "Epoch 1208/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 01208: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4382 - val_loss: 1.4769\n",
      "Epoch 1209/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01209: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4358 - val_loss: 1.4745\n",
      "Epoch 1210/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01210: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4760\n",
      "Epoch 1211/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01211: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4363 - val_loss: 1.4825\n",
      "Epoch 1212/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4422\n",
      "Epoch 01212: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4422 - val_loss: 1.4772\n",
      "Epoch 1213/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4389\n",
      "Epoch 01213: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4389 - val_loss: 1.4752\n",
      "Epoch 1214/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01214: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4361 - val_loss: 1.4783\n",
      "Epoch 1215/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01215: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4371 - val_loss: 1.4753\n",
      "Epoch 1216/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01216: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4367 - val_loss: 1.4766\n",
      "Epoch 1217/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 01217: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4411 - val_loss: 1.4780\n",
      "Epoch 1218/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01218: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4392 - val_loss: 1.4770\n",
      "Epoch 1219/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4375\n",
      "Epoch 01219: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4375 - val_loss: 1.4799\n",
      "Epoch 1220/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 01220: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4399 - val_loss: 1.4806\n",
      "Epoch 1221/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4412\n",
      "Epoch 01221: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4412 - val_loss: 1.4766\n",
      "Epoch 1222/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01222: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4358 - val_loss: 1.4745\n",
      "Epoch 1223/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01223: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4345 - val_loss: 1.4762\n",
      "Epoch 1224/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01224: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4363 - val_loss: 1.4765\n",
      "Epoch 1225/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 01225: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4399 - val_loss: 1.4763\n",
      "Epoch 1226/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4383\n",
      "Epoch 01226: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4383 - val_loss: 1.4809\n",
      "Epoch 1227/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01227: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4388 - val_loss: 1.4784\n",
      "Epoch 1228/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01228: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4372 - val_loss: 1.4772\n",
      "Epoch 1229/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01229: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4344 - val_loss: 1.4761\n",
      "Epoch 1230/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01230: loss did not improve from 1.43423\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4355 - val_loss: 1.4782\n",
      "Epoch 1231/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4340\n",
      "Epoch 01231: loss improved from 1.43423 to 1.43404, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4340 - val_loss: 1.4763\n",
      "Epoch 1232/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01232: loss did not improve from 1.43404\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4344 - val_loss: 1.4762\n",
      "Epoch 1233/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01233: loss did not improve from 1.43404\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4788\n",
      "Epoch 1234/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4384\n",
      "Epoch 01234: loss did not improve from 1.43404\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4384 - val_loss: 1.4766\n",
      "Epoch 1235/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4380\n",
      "Epoch 01235: loss did not improve from 1.43404\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4380 - val_loss: 1.4757\n",
      "Epoch 1236/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 01236: loss did not improve from 1.43404\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4374 - val_loss: 1.4773\n",
      "Epoch 1237/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4381\n",
      "Epoch 01237: loss did not improve from 1.43404\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4381 - val_loss: 1.4769\n",
      "Epoch 1238/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01238: loss improved from 1.43404 to 1.43345, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4334 - val_loss: 1.4784\n",
      "Epoch 1239/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01239: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4346 - val_loss: 1.4791\n",
      "Epoch 1240/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01240: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4363 - val_loss: 1.4781\n",
      "Epoch 1241/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01241: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4336 - val_loss: 1.4755\n",
      "Epoch 1242/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01242: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4345 - val_loss: 1.4757\n",
      "Epoch 1243/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4340\n",
      "Epoch 01243: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4340 - val_loss: 1.4767\n",
      "Epoch 1244/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4349\n",
      "Epoch 01244: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4349 - val_loss: 1.4801\n",
      "Epoch 1245/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4401\n",
      "Epoch 01245: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4401 - val_loss: 1.4750\n",
      "Epoch 1246/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01246: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4364 - val_loss: 1.4753\n",
      "Epoch 1247/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4354\n",
      "Epoch 01247: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4354 - val_loss: 1.4770\n",
      "Epoch 1248/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01248: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4770\n",
      "Epoch 1249/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01249: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4357 - val_loss: 1.4762\n",
      "Epoch 1250/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01250: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4357 - val_loss: 1.4740\n",
      "Epoch 1251/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01251: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4359 - val_loss: 1.4754\n",
      "Epoch 1252/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4362\n",
      "Epoch 01252: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4362 - val_loss: 1.4787\n",
      "Epoch 1253/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4362\n",
      "Epoch 01253: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4362 - val_loss: 1.4829\n",
      "Epoch 1254/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4376\n",
      "Epoch 01254: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4376 - val_loss: 1.4779\n",
      "Epoch 1255/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01255: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4357 - val_loss: 1.4760\n",
      "Epoch 1256/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01256: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4727\n",
      "Epoch 1257/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4378\n",
      "Epoch 01257: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4378 - val_loss: 1.4752\n",
      "Epoch 1258/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01258: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4356 - val_loss: 1.4748\n",
      "Epoch 1259/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01259: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4356 - val_loss: 1.4755\n",
      "Epoch 1260/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01260: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4359 - val_loss: 1.4785\n",
      "Epoch 1261/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01261: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4358 - val_loss: 1.4776\n",
      "Epoch 1262/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01262: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4364 - val_loss: 1.4795\n",
      "Epoch 1263/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01263: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4344 - val_loss: 1.4783\n",
      "Epoch 1264/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01264: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4350 - val_loss: 1.4778\n",
      "Epoch 1265/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01265: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4372 - val_loss: 1.4783\n",
      "Epoch 1266/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4379\n",
      "Epoch 01266: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4379 - val_loss: 1.4820\n",
      "Epoch 1267/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01267: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4370 - val_loss: 1.4795\n",
      "Epoch 1268/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01268: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4356 - val_loss: 1.4763\n",
      "Epoch 1269/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01269: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4357 - val_loss: 1.4770\n",
      "Epoch 1270/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4366\n",
      "Epoch 01270: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4366 - val_loss: 1.4775\n",
      "Epoch 1271/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01271: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4365 - val_loss: 1.4766\n",
      "Epoch 1272/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01272: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4363 - val_loss: 1.4799\n",
      "Epoch 1273/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4369\n",
      "Epoch 01273: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4369 - val_loss: 1.4796\n",
      "Epoch 1274/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01274: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4746\n",
      "Epoch 1275/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01275: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4341 - val_loss: 1.4780\n",
      "Epoch 1276/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01276: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4353 - val_loss: 1.4793\n",
      "Epoch 1277/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4362\n",
      "Epoch 01277: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4362 - val_loss: 1.4794\n",
      "Epoch 1278/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01278: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4346 - val_loss: 1.4771\n",
      "Epoch 1279/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01279: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4343 - val_loss: 1.4797\n",
      "Epoch 1280/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01280: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4355 - val_loss: 1.4774\n",
      "Epoch 1281/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01281: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4764\n",
      "Epoch 1282/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4383\n",
      "Epoch 01282: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4383 - val_loss: 1.4803\n",
      "Epoch 1283/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4409\n",
      "Epoch 01283: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4409 - val_loss: 1.4763\n",
      "Epoch 1284/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4378\n",
      "Epoch 01284: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4378 - val_loss: 1.4767\n",
      "Epoch 1285/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01285: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4344 - val_loss: 1.4769\n",
      "Epoch 1286/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4348\n",
      "Epoch 01286: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4348 - val_loss: 1.4772\n",
      "Epoch 1287/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01287: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4336 - val_loss: 1.4759\n",
      "Epoch 1288/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01288: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4356 - val_loss: 1.4771\n",
      "Epoch 1289/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4375\n",
      "Epoch 01289: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4375 - val_loss: 1.4790\n",
      "Epoch 1290/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01290: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4367 - val_loss: 1.4817\n",
      "Epoch 1291/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01291: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4355 - val_loss: 1.4798\n",
      "Epoch 1292/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4362\n",
      "Epoch 01292: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4362 - val_loss: 1.4805\n",
      "Epoch 1293/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01293: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4364 - val_loss: 1.4743\n",
      "Epoch 1294/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 01294: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4347 - val_loss: 1.4767\n",
      "Epoch 1295/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01295: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4749\n",
      "Epoch 1296/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01296: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4350 - val_loss: 1.4768\n",
      "Epoch 1297/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01297: loss did not improve from 1.43345\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4758\n",
      "Epoch 1298/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01298: loss improved from 1.43345 to 1.43342, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4334 - val_loss: 1.4796\n",
      "Epoch 1299/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4327\n",
      "Epoch 01299: loss improved from 1.43342 to 1.43267, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4327 - val_loss: 1.4761\n",
      "Epoch 1300/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01300: loss improved from 1.43267 to 1.43179, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4318 - val_loss: 1.4770\n",
      "Epoch 1301/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01301: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4328 - val_loss: 1.4775\n",
      "Epoch 1302/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01302: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4335 - val_loss: 1.4772\n",
      "Epoch 1303/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01303: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4343 - val_loss: 1.4797\n",
      "Epoch 1304/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4360\n",
      "Epoch 01304: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4360 - val_loss: 1.4758\n",
      "Epoch 1305/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01305: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4342 - val_loss: 1.4860\n",
      "Epoch 1306/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4369\n",
      "Epoch 01306: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4369 - val_loss: 1.4798\n",
      "Epoch 1307/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4415\n",
      "Epoch 01307: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4415 - val_loss: 1.4776\n",
      "Epoch 1308/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 01308: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4347 - val_loss: 1.4755\n",
      "Epoch 1309/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01309: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4338 - val_loss: 1.4758\n",
      "Epoch 1310/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01310: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4363 - val_loss: 1.4761\n",
      "Epoch 1311/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01311: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4365 - val_loss: 1.4793\n",
      "Epoch 1312/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01312: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4392 - val_loss: 1.4782\n",
      "Epoch 1313/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4412\n",
      "Epoch 01313: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4412 - val_loss: 1.4785\n",
      "Epoch 1314/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4384\n",
      "Epoch 01314: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4384 - val_loss: 1.4773\n",
      "Epoch 1315/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01315: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4357 - val_loss: 1.4767\n",
      "Epoch 1316/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01316: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4778\n",
      "Epoch 1317/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4376\n",
      "Epoch 01317: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4376 - val_loss: 1.4758\n",
      "Epoch 1318/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01318: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4748\n",
      "Epoch 1319/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4339\n",
      "Epoch 01319: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4339 - val_loss: 1.4742\n",
      "Epoch 1320/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4378\n",
      "Epoch 01320: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4378 - val_loss: 1.4781\n",
      "Epoch 1321/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4489\n",
      "Epoch 01321: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4489 - val_loss: 1.4759\n",
      "Epoch 1322/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4484\n",
      "Epoch 01322: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4484 - val_loss: 1.4783\n",
      "Epoch 1323/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4439\n",
      "Epoch 01323: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4439 - val_loss: 1.4771\n",
      "Epoch 1324/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4402\n",
      "Epoch 01324: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4402 - val_loss: 1.4803\n",
      "Epoch 1325/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4388\n",
      "Epoch 01325: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4388 - val_loss: 1.4776\n",
      "Epoch 1326/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01326: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4371 - val_loss: 1.4777\n",
      "Epoch 1327/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4386\n",
      "Epoch 01327: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4386 - val_loss: 1.4786\n",
      "Epoch 1328/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01328: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4372 - val_loss: 1.4825\n",
      "Epoch 1329/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01329: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4372 - val_loss: 1.4783\n",
      "Epoch 1330/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01330: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4351 - val_loss: 1.4762\n",
      "Epoch 1331/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01331: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4358 - val_loss: 1.4810\n",
      "Epoch 1332/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01332: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4766\n",
      "Epoch 1333/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4349\n",
      "Epoch 01333: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4349 - val_loss: 1.4770\n",
      "Epoch 1334/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01334: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4330 - val_loss: 1.4784\n",
      "Epoch 1335/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4352\n",
      "Epoch 01335: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4352 - val_loss: 1.4790\n",
      "Epoch 1336/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01336: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4350 - val_loss: 1.4764\n",
      "Epoch 1337/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01337: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4338 - val_loss: 1.4783\n",
      "Epoch 1338/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01338: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4343 - val_loss: 1.4769\n",
      "Epoch 1339/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01339: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4335 - val_loss: 1.4776\n",
      "Epoch 1340/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4340\n",
      "Epoch 01340: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4340 - val_loss: 1.4793\n",
      "Epoch 1341/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4348\n",
      "Epoch 01341: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4348 - val_loss: 1.4768\n",
      "Epoch 1342/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01342: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4363 - val_loss: 1.4742\n",
      "Epoch 1343/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01343: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4320 - val_loss: 1.4746\n",
      "Epoch 1344/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01344: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4785\n",
      "Epoch 1345/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01345: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4334 - val_loss: 1.4749\n",
      "Epoch 1346/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01346: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4817\n",
      "Epoch 1347/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01347: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4363 - val_loss: 1.4822\n",
      "Epoch 1348/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01348: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4357 - val_loss: 1.4837\n",
      "Epoch 1349/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4408\n",
      "Epoch 01349: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4408 - val_loss: 1.4764\n",
      "Epoch 1350/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4390\n",
      "Epoch 01350: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4390 - val_loss: 1.4750\n",
      "Epoch 1351/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01351: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4359 - val_loss: 1.4781\n",
      "Epoch 1352/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01352: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4353 - val_loss: 1.4767\n",
      "Epoch 1353/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01353: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4361 - val_loss: 1.4769\n",
      "Epoch 1354/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4397\n",
      "Epoch 01354: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4397 - val_loss: 1.4775\n",
      "Epoch 1355/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01355: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4371 - val_loss: 1.4770\n",
      "Epoch 1356/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01356: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4358 - val_loss: 1.4804\n",
      "Epoch 1357/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4372\n",
      "Epoch 01357: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4372 - val_loss: 1.4765\n",
      "Epoch 1358/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01358: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4762\n",
      "Epoch 1359/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01359: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4325 - val_loss: 1.4748\n",
      "Epoch 1360/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01360: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4332 - val_loss: 1.4761\n",
      "Epoch 1361/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01361: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4350 - val_loss: 1.4775\n",
      "Epoch 1362/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01362: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4342 - val_loss: 1.4764\n",
      "Epoch 1363/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01363: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4786\n",
      "Epoch 1364/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4365\n",
      "Epoch 01364: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4365 - val_loss: 1.4788\n",
      "Epoch 1365/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01365: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4332 - val_loss: 1.4756\n",
      "Epoch 1366/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01366: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4328 - val_loss: 1.4808\n",
      "Epoch 1367/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01367: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4344 - val_loss: 1.4791\n",
      "Epoch 1368/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01368: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4790\n",
      "Epoch 1369/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01369: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4344 - val_loss: 1.4784\n",
      "Epoch 1370/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01370: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4353 - val_loss: 1.4770\n",
      "Epoch 1371/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01371: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4341 - val_loss: 1.4743\n",
      "Epoch 1372/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01372: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4331 - val_loss: 1.4780\n",
      "Epoch 1373/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01373: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4331 - val_loss: 1.4786\n",
      "Epoch 1374/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01374: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4330 - val_loss: 1.4771\n",
      "Epoch 1375/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4324\n",
      "Epoch 01375: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4324 - val_loss: 1.4789\n",
      "Epoch 1376/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01376: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4747\n",
      "Epoch 1377/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4322\n",
      "Epoch 01377: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4322 - val_loss: 1.4769\n",
      "Epoch 1378/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 01378: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4337 - val_loss: 1.4791\n",
      "Epoch 1379/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01379: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4344 - val_loss: 1.4783\n",
      "Epoch 1380/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01380: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4341 - val_loss: 1.4843\n",
      "Epoch 1381/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01381: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4357 - val_loss: 1.4802\n",
      "Epoch 1382/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01382: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4744\n",
      "Epoch 1383/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01383: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4334 - val_loss: 1.4777\n",
      "Epoch 1384/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01384: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4328 - val_loss: 1.4799\n",
      "Epoch 1385/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4339\n",
      "Epoch 01385: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4339 - val_loss: 1.4773\n",
      "Epoch 1386/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4481\n",
      "Epoch 01386: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4481 - val_loss: 1.4829\n",
      "Epoch 1387/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4437\n",
      "Epoch 01387: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4437 - val_loss: 1.4787\n",
      "Epoch 1388/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4404\n",
      "Epoch 01388: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4404 - val_loss: 1.4778\n",
      "Epoch 1389/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4377\n",
      "Epoch 01389: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4377 - val_loss: 1.4789\n",
      "Epoch 1390/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01390: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4355 - val_loss: 1.4783\n",
      "Epoch 1391/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01391: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4346 - val_loss: 1.4780\n",
      "Epoch 1392/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01392: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4335 - val_loss: 1.4780\n",
      "Epoch 1393/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01393: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4343 - val_loss: 1.4754\n",
      "Epoch 1394/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4339\n",
      "Epoch 01394: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4339 - val_loss: 1.4795\n",
      "Epoch 1395/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 01395: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4357 - val_loss: 1.4780\n",
      "Epoch 1396/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4364\n",
      "Epoch 01396: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4364 - val_loss: 1.4782\n",
      "Epoch 1397/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01397: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4343 - val_loss: 1.4766\n",
      "Epoch 1398/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01398: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4341 - val_loss: 1.4771\n",
      "Epoch 1399/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01399: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4358 - val_loss: 1.4822\n",
      "Epoch 1400/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4397\n",
      "Epoch 01400: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4397 - val_loss: 1.4811\n",
      "Epoch 1401/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4381\n",
      "Epoch 01401: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4381 - val_loss: 1.4768\n",
      "Epoch 1402/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4329\n",
      "Epoch 01402: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4329 - val_loss: 1.4786\n",
      "Epoch 1403/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4322\n",
      "Epoch 01403: loss did not improve from 1.43179\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4322 - val_loss: 1.4784\n",
      "Epoch 1404/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01404: loss improved from 1.43179 to 1.43152, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4315 - val_loss: 1.4777\n",
      "Epoch 1405/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01405: loss did not improve from 1.43152\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4319 - val_loss: 1.4769\n",
      "Epoch 1406/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01406: loss did not improve from 1.43152\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4328 - val_loss: 1.4777\n",
      "Epoch 1407/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01407: loss did not improve from 1.43152\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4326 - val_loss: 1.4781\n",
      "Epoch 1408/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01408: loss did not improve from 1.43152\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4355 - val_loss: 1.4829\n",
      "Epoch 1409/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4362\n",
      "Epoch 01409: loss did not improve from 1.43152\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4362 - val_loss: 1.4821\n",
      "Epoch 1410/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01410: loss did not improve from 1.43152\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4344 - val_loss: 1.4785\n",
      "Epoch 1411/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01411: loss did not improve from 1.43152\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4363 - val_loss: 1.4798\n",
      "Epoch 1412/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01412: loss did not improve from 1.43152\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4331 - val_loss: 1.4799\n",
      "Epoch 1413/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01413: loss did not improve from 1.43152\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4320 - val_loss: 1.4800\n",
      "Epoch 1414/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01414: loss did not improve from 1.43152\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4334 - val_loss: 1.4791\n",
      "Epoch 1415/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01415: loss improved from 1.43152 to 1.43090, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4309 - val_loss: 1.4783\n",
      "Epoch 1416/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01416: loss improved from 1.43090 to 1.43060, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4306 - val_loss: 1.4789\n",
      "Epoch 1417/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01417: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4342 - val_loss: 1.4831\n",
      "Epoch 1418/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01418: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4346 - val_loss: 1.4796\n",
      "Epoch 1419/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01419: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4323 - val_loss: 1.4803\n",
      "Epoch 1420/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01420: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4314 - val_loss: 1.4777\n",
      "Epoch 1421/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01421: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4326 - val_loss: 1.4808\n",
      "Epoch 1422/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 01422: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4337 - val_loss: 1.4756\n",
      "Epoch 1423/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01423: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4314 - val_loss: 1.4779\n",
      "Epoch 1424/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01424: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4319 - val_loss: 1.4764\n",
      "Epoch 1425/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01425: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4311 - val_loss: 1.4790\n",
      "Epoch 1426/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01426: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4326 - val_loss: 1.4779\n",
      "Epoch 1427/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4348\n",
      "Epoch 01427: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4348 - val_loss: 1.4761\n",
      "Epoch 1428/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01428: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4765\n",
      "Epoch 1429/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01429: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4351 - val_loss: 1.4756\n",
      "Epoch 1430/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01430: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4331 - val_loss: 1.4775\n",
      "Epoch 1431/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4340\n",
      "Epoch 01431: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4340 - val_loss: 1.4783\n",
      "Epoch 1432/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01432: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4315 - val_loss: 1.4790\n",
      "Epoch 1433/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01433: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4325 - val_loss: 1.4784\n",
      "Epoch 1434/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4313\n",
      "Epoch 01434: loss did not improve from 1.43060\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4313 - val_loss: 1.4756\n",
      "Epoch 1435/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01435: loss improved from 1.43060 to 1.43058, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4306 - val_loss: 1.4782\n",
      "Epoch 1436/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01436: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4326 - val_loss: 1.4818\n",
      "Epoch 1437/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01437: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4355 - val_loss: 1.4805\n",
      "Epoch 1438/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4371\n",
      "Epoch 01438: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4371 - val_loss: 1.4833\n",
      "Epoch 1439/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01439: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4351 - val_loss: 1.4770\n",
      "Epoch 1440/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4340\n",
      "Epoch 01440: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4340 - val_loss: 1.4802\n",
      "Epoch 1441/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 01441: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4337 - val_loss: 1.4799\n",
      "Epoch 1442/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01442: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4338 - val_loss: 1.4802\n",
      "Epoch 1443/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4333\n",
      "Epoch 01443: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4333 - val_loss: 1.4768\n",
      "Epoch 1444/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01444: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4343 - val_loss: 1.4810\n",
      "Epoch 1445/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01445: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4326 - val_loss: 1.4802\n",
      "Epoch 1446/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01446: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4334 - val_loss: 1.4798\n",
      "Epoch 1447/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01447: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4344 - val_loss: 1.4805\n",
      "Epoch 1448/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01448: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4361 - val_loss: 1.4806\n",
      "Epoch 1449/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 01449: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4347 - val_loss: 1.4822\n",
      "Epoch 1450/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01450: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4353 - val_loss: 1.4769\n",
      "Epoch 1451/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4324\n",
      "Epoch 01451: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4324 - val_loss: 1.4752\n",
      "Epoch 1452/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01452: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4323 - val_loss: 1.4782\n",
      "Epoch 1453/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01453: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4311 - val_loss: 1.4765\n",
      "Epoch 1454/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01454: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4320 - val_loss: 1.4781\n",
      "Epoch 1455/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01455: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4334 - val_loss: 1.4768\n",
      "Epoch 1456/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01456: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4315 - val_loss: 1.4780\n",
      "Epoch 1457/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01457: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4334 - val_loss: 1.4810\n",
      "Epoch 1458/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4358\n",
      "Epoch 01458: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4358 - val_loss: 1.4877\n",
      "Epoch 1459/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4384\n",
      "Epoch 01459: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4384 - val_loss: 1.4844\n",
      "Epoch 1460/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01460: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4343 - val_loss: 1.4812\n",
      "Epoch 1461/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4370\n",
      "Epoch 01461: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4370 - val_loss: 1.4791\n",
      "Epoch 1462/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4349\n",
      "Epoch 01462: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4346 - val_loss: 1.4789\n",
      "Epoch 1463/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01463: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4334 - val_loss: 1.4813\n",
      "Epoch 1464/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4354\n",
      "Epoch 01464: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4354 - val_loss: 1.4774\n",
      "Epoch 1465/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01465: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4334 - val_loss: 1.4778\n",
      "Epoch 1466/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01466: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4759\n",
      "Epoch 1467/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01467: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4330 - val_loss: 1.4810\n",
      "Epoch 1468/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4339\n",
      "Epoch 01468: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4339 - val_loss: 1.4799\n",
      "Epoch 1469/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01469: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4342 - val_loss: 1.4793\n",
      "Epoch 1470/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01470: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4318 - val_loss: 1.4794\n",
      "Epoch 1471/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01471: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4314 - val_loss: 1.4776\n",
      "Epoch 1472/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01472: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4338 - val_loss: 1.4795\n",
      "Epoch 1473/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4348\n",
      "Epoch 01473: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4348 - val_loss: 1.4852\n",
      "Epoch 1474/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01474: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4335 - val_loss: 1.4806\n",
      "Epoch 1475/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01475: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4308 - val_loss: 1.4769\n",
      "Epoch 1476/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01476: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4311 - val_loss: 1.4768\n",
      "Epoch 1477/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01477: loss did not improve from 1.43058\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4323 - val_loss: 1.4780\n",
      "Epoch 1478/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01478: loss improved from 1.43058 to 1.43017, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4302 - val_loss: 1.4795\n",
      "Epoch 1479/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01479: loss did not improve from 1.43017\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4310 - val_loss: 1.4792\n",
      "Epoch 1480/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01480: loss did not improve from 1.43017\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4323 - val_loss: 1.4811\n",
      "Epoch 1481/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01481: loss did not improve from 1.43017\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4341 - val_loss: 1.4789\n",
      "Epoch 1482/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01482: loss did not improve from 1.43017\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4315 - val_loss: 1.4798\n",
      "Epoch 1483/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01483: loss did not improve from 1.43017\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4323 - val_loss: 1.4832\n",
      "Epoch 1484/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01484: loss did not improve from 1.43017\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4328 - val_loss: 1.4831\n",
      "Epoch 1485/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01485: loss did not improve from 1.43017\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4353 - val_loss: 1.4803\n",
      "Epoch 1486/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01486: loss did not improve from 1.43017\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4325 - val_loss: 1.4790\n",
      "Epoch 1487/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01487: loss improved from 1.43017 to 1.42942, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4294 - val_loss: 1.4761\n",
      "Epoch 1488/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 01488: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4296 - val_loss: 1.4788\n",
      "Epoch 1489/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01489: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4308 - val_loss: 1.4786\n",
      "Epoch 1490/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01490: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4319 - val_loss: 1.4810\n",
      "Epoch 1491/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01491: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4303 - val_loss: 1.4783\n",
      "Epoch 1492/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01492: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4325 - val_loss: 1.4759\n",
      "Epoch 1493/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01493: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4350 - val_loss: 1.4794\n",
      "Epoch 1494/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01494: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4353 - val_loss: 1.4803\n",
      "Epoch 1495/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4333\n",
      "Epoch 01495: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4333 - val_loss: 1.4862\n",
      "Epoch 1496/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01496: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4361 - val_loss: 1.4795\n",
      "Epoch 1497/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01497: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4325 - val_loss: 1.4786\n",
      "Epoch 1498/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01498: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4308 - val_loss: 1.4814\n",
      "Epoch 1499/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4327\n",
      "Epoch 01499: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4327 - val_loss: 1.4793\n",
      "Epoch 1500/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01500: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4326 - val_loss: 1.4834\n",
      "Epoch 1501/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01501: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4335 - val_loss: 1.4790\n",
      "Epoch 1502/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01502: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4325 - val_loss: 1.4784\n",
      "Epoch 1503/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01503: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4350 - val_loss: 1.4769\n",
      "Epoch 1504/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4312\n",
      "Epoch 01504: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4312 - val_loss: 1.4805\n",
      "Epoch 1505/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01505: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4784\n",
      "Epoch 1506/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01506: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4335 - val_loss: 1.4783\n",
      "Epoch 1507/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01507: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4320 - val_loss: 1.4782\n",
      "Epoch 1508/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01508: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4306 - val_loss: 1.4761\n",
      "Epoch 1509/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01509: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4338 - val_loss: 1.4829\n",
      "Epoch 1510/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4392\n",
      "Epoch 01510: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4392 - val_loss: 1.4919\n",
      "Epoch 1511/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 01511: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4407 - val_loss: 1.4805\n",
      "Epoch 1512/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 01512: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4347 - val_loss: 1.4785\n",
      "Epoch 1513/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01513: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4367 - val_loss: 1.4816\n",
      "Epoch 1514/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4329\n",
      "Epoch 01514: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4329 - val_loss: 1.4819\n",
      "Epoch 1515/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4321\n",
      "Epoch 01515: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4321 - val_loss: 1.4773\n",
      "Epoch 1516/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01516: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4300 - val_loss: 1.4773\n",
      "Epoch 1517/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4312\n",
      "Epoch 01517: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4312 - val_loss: 1.4789\n",
      "Epoch 1518/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01518: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4318 - val_loss: 1.4794\n",
      "Epoch 1519/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01519: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4314 - val_loss: 1.4891\n",
      "Epoch 1520/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4339\n",
      "Epoch 01520: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4339 - val_loss: 1.4854\n",
      "Epoch 1521/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01521: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4346 - val_loss: 1.4862\n",
      "Epoch 1522/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4322\n",
      "Epoch 01522: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4322 - val_loss: 1.4777\n",
      "Epoch 1523/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01523: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4325 - val_loss: 1.4806\n",
      "Epoch 1524/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01524: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4326 - val_loss: 1.4827\n",
      "Epoch 1525/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01525: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4341 - val_loss: 1.4795\n",
      "Epoch 1526/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01526: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4308 - val_loss: 1.4767\n",
      "Epoch 1527/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01527: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4368 - val_loss: 1.4839\n",
      "Epoch 1528/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4429\n",
      "Epoch 01528: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4429 - val_loss: 1.4818\n",
      "Epoch 1529/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4379\n",
      "Epoch 01529: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4379 - val_loss: 1.4799\n",
      "Epoch 1530/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01530: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4793\n",
      "Epoch 1531/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4352\n",
      "Epoch 01531: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4352 - val_loss: 1.4795\n",
      "Epoch 1532/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01532: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4342 - val_loss: 1.4773\n",
      "Epoch 1533/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4347\n",
      "Epoch 01533: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4347 - val_loss: 1.4812\n",
      "Epoch 1534/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01534: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4320 - val_loss: 1.4794\n",
      "Epoch 1535/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01535: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4786\n",
      "Epoch 1536/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01536: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4328 - val_loss: 1.4796\n",
      "Epoch 1537/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4327\n",
      "Epoch 01537: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4327 - val_loss: 1.4786\n",
      "Epoch 1538/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01538: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4798\n",
      "Epoch 1539/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01539: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4318 - val_loss: 1.4798\n",
      "Epoch 1540/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01540: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4305 - val_loss: 1.4814\n",
      "Epoch 1541/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4322\n",
      "Epoch 01541: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4322 - val_loss: 1.4806\n",
      "Epoch 1542/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01542: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4305 - val_loss: 1.4809\n",
      "Epoch 1543/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4402\n",
      "Epoch 01543: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4402 - val_loss: 1.4832\n",
      "Epoch 1544/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4367\n",
      "Epoch 01544: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4367 - val_loss: 1.4793\n",
      "Epoch 1545/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01545: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4331 - val_loss: 1.4830\n",
      "Epoch 1546/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4354\n",
      "Epoch 01546: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4354 - val_loss: 1.4784\n",
      "Epoch 1547/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01547: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4325 - val_loss: 1.4805\n",
      "Epoch 1548/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01548: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4331 - val_loss: 1.4800\n",
      "Epoch 1549/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01549: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4341 - val_loss: 1.4807\n",
      "Epoch 1550/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01550: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4311 - val_loss: 1.4793\n",
      "Epoch 1551/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4301\n",
      "Epoch 01551: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4301 - val_loss: 1.4787\n",
      "Epoch 1552/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01552: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4295 - val_loss: 1.4787\n",
      "Epoch 1553/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4321\n",
      "Epoch 01553: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4321 - val_loss: 1.4842\n",
      "Epoch 1554/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 01554: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4346 - val_loss: 1.4814\n",
      "Epoch 1555/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01555: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4317 - val_loss: 1.4826\n",
      "Epoch 1556/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4341\n",
      "Epoch 01556: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4341 - val_loss: 1.4764\n",
      "Epoch 1557/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01557: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4335 - val_loss: 1.4785\n",
      "Epoch 1558/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4336\n",
      "Epoch 01558: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4336 - val_loss: 1.4795\n",
      "Epoch 1559/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4348\n",
      "Epoch 01559: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4348 - val_loss: 1.4825\n",
      "Epoch 1560/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01560: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4334 - val_loss: 1.4844\n",
      "Epoch 1561/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4386\n",
      "Epoch 01561: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4386 - val_loss: 1.4783\n",
      "Epoch 1562/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01562: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4803\n",
      "Epoch 1563/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01563: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4317 - val_loss: 1.4818\n",
      "Epoch 1564/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01564: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4306 - val_loss: 1.4782\n",
      "Epoch 1565/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01565: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4311 - val_loss: 1.4773\n",
      "Epoch 1566/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4321\n",
      "Epoch 01566: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4321 - val_loss: 1.4801\n",
      "Epoch 1567/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01567: loss did not improve from 1.42942\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4298 - val_loss: 1.4769\n",
      "Epoch 1568/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01568: loss improved from 1.42942 to 1.42931, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4293 - val_loss: 1.4806\n",
      "Epoch 1569/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01569: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4319 - val_loss: 1.4838\n",
      "Epoch 1570/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 01570: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4359 - val_loss: 1.4882\n",
      "Epoch 1571/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01571: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4334 - val_loss: 1.4783\n",
      "Epoch 1572/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01572: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4318 - val_loss: 1.4807\n",
      "Epoch 1573/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4330\n",
      "Epoch 01573: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4330 - val_loss: 1.4841\n",
      "Epoch 1574/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01574: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4318 - val_loss: 1.4801\n",
      "Epoch 1575/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01575: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4305 - val_loss: 1.4783\n",
      "Epoch 1576/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01576: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4308 - val_loss: 1.4794\n",
      "Epoch 1577/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01577: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4300 - val_loss: 1.4807\n",
      "Epoch 1578/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 01578: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4296 - val_loss: 1.4793\n",
      "Epoch 1579/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01579: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4295 - val_loss: 1.4795\n",
      "Epoch 1580/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01580: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4306 - val_loss: 1.4785\n",
      "Epoch 1581/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01581: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4295 - val_loss: 1.4795\n",
      "Epoch 1582/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01582: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4303 - val_loss: 1.4792\n",
      "Epoch 1583/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01583: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4353 - val_loss: 1.4822\n",
      "Epoch 1584/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01584: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4808\n",
      "Epoch 1585/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4327\n",
      "Epoch 01585: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4327 - val_loss: 1.4817\n",
      "Epoch 1586/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4327\n",
      "Epoch 01586: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4327 - val_loss: 1.4793\n",
      "Epoch 1587/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01587: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4299 - val_loss: 1.4801\n",
      "Epoch 1588/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01588: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4317 - val_loss: 1.4802\n",
      "Epoch 1589/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01589: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4303 - val_loss: 1.4797\n",
      "Epoch 1590/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01590: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4306 - val_loss: 1.4811\n",
      "Epoch 1591/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01591: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4305 - val_loss: 1.4805\n",
      "Epoch 1592/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01592: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4298 - val_loss: 1.4783\n",
      "Epoch 1593/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 01593: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4411 - val_loss: 1.4822\n",
      "Epoch 1594/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4381\n",
      "Epoch 01594: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4381 - val_loss: 1.4801\n",
      "Epoch 1595/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01595: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4325 - val_loss: 1.4774\n",
      "Epoch 1596/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01596: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4310 - val_loss: 1.4913\n",
      "Epoch 1597/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 01597: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4351 - val_loss: 1.4795\n",
      "Epoch 1598/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01598: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4314 - val_loss: 1.4806\n",
      "Epoch 1599/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01599: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4768\n",
      "Epoch 1600/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01600: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4323 - val_loss: 1.4831\n",
      "Epoch 1601/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 01601: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4337 - val_loss: 1.4816\n",
      "Epoch 1602/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01602: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4317 - val_loss: 1.4778\n",
      "Epoch 1603/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4301\n",
      "Epoch 01603: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4301 - val_loss: 1.4787\n",
      "Epoch 1604/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01604: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4311 - val_loss: 1.4795\n",
      "Epoch 1605/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01605: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4325 - val_loss: 1.4801\n",
      "Epoch 1606/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01606: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4323 - val_loss: 1.4783\n",
      "Epoch 1607/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01607: loss did not improve from 1.42931\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4298 - val_loss: 1.4789\n",
      "Epoch 1608/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01608: loss improved from 1.42931 to 1.42791, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4279 - val_loss: 1.4786\n",
      "Epoch 1609/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 01609: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4290 - val_loss: 1.4780\n",
      "Epoch 1610/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01610: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4298 - val_loss: 1.4781\n",
      "Epoch 1611/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01611: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4279 - val_loss: 1.4808\n",
      "Epoch 1612/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 01612: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4289 - val_loss: 1.4827\n",
      "Epoch 1613/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01613: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4314 - val_loss: 1.4810\n",
      "Epoch 1614/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01614: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4306 - val_loss: 1.4818\n",
      "Epoch 1615/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4301\n",
      "Epoch 01615: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4301 - val_loss: 1.4787\n",
      "Epoch 1616/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01616: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4295 - val_loss: 1.4773\n",
      "Epoch 1617/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01617: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4295 - val_loss: 1.4790\n",
      "Epoch 1618/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4312\n",
      "Epoch 01618: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4312 - val_loss: 1.4804\n",
      "Epoch 1619/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01619: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4332 - val_loss: 1.4779\n",
      "Epoch 1620/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01620: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4310 - val_loss: 1.4794\n",
      "Epoch 1621/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01621: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4303 - val_loss: 1.4792\n",
      "Epoch 1622/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01622: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4293 - val_loss: 1.4759\n",
      "Epoch 1623/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01623: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4785\n",
      "Epoch 1624/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01624: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4294 - val_loss: 1.4836\n",
      "Epoch 1625/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01625: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4810\n",
      "Epoch 1626/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01626: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4315 - val_loss: 1.4785\n",
      "Epoch 1627/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 01627: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4296 - val_loss: 1.4789\n",
      "Epoch 1628/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01628: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4307 - val_loss: 1.4837\n",
      "Epoch 1629/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4312\n",
      "Epoch 01629: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4312 - val_loss: 1.4799\n",
      "Epoch 1630/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01630: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4308 - val_loss: 1.4810\n",
      "Epoch 1631/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4323\n",
      "Epoch 01631: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4323 - val_loss: 1.4805\n",
      "Epoch 1632/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01632: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4306 - val_loss: 1.4805\n",
      "Epoch 1633/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01633: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4293 - val_loss: 1.4798\n",
      "Epoch 1634/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 01634: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4283 - val_loss: 1.4794\n",
      "Epoch 1635/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 01635: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4287 - val_loss: 1.4787\n",
      "Epoch 1636/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01636: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4291 - val_loss: 1.4866\n",
      "Epoch 1637/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01637: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4307 - val_loss: 1.4810\n",
      "Epoch 1638/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 01638: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4287 - val_loss: 1.4778\n",
      "Epoch 1639/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4378\n",
      "Epoch 01639: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4378 - val_loss: 1.4783\n",
      "Epoch 1640/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01640: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4325 - val_loss: 1.4814\n",
      "Epoch 1641/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4327\n",
      "Epoch 01641: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4327 - val_loss: 1.4810\n",
      "Epoch 1642/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01642: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4772\n",
      "Epoch 1643/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 01643: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4287 - val_loss: 1.4799\n",
      "Epoch 1644/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01644: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4279 - val_loss: 1.4801\n",
      "Epoch 1645/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01645: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4803\n",
      "Epoch 1646/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 01646: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4296 - val_loss: 1.4814\n",
      "Epoch 1647/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01647: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4305 - val_loss: 1.4805\n",
      "Epoch 1648/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01648: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4309 - val_loss: 1.4847\n",
      "Epoch 1649/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4311\n",
      "Epoch 01649: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4311 - val_loss: 1.4804\n",
      "Epoch 1650/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01650: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4344 - val_loss: 1.4798\n",
      "Epoch 1651/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4343\n",
      "Epoch 01651: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4343 - val_loss: 1.4782\n",
      "Epoch 1652/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01652: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4308 - val_loss: 1.4840\n",
      "Epoch 1653/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4329\n",
      "Epoch 01653: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4329 - val_loss: 1.4877\n",
      "Epoch 1654/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4430\n",
      "Epoch 01654: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4430 - val_loss: 1.4831\n",
      "Epoch 1655/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4393\n",
      "Epoch 01655: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4393 - val_loss: 1.4852\n",
      "Epoch 1656/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4349\n",
      "Epoch 01656: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4349 - val_loss: 1.4843\n",
      "Epoch 1657/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01657: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4350 - val_loss: 1.4804\n",
      "Epoch 1658/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4353\n",
      "Epoch 01658: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4353 - val_loss: 1.4825\n",
      "Epoch 1659/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01659: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4300 - val_loss: 1.4793\n",
      "Epoch 1660/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01660: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4295 - val_loss: 1.4831\n",
      "Epoch 1661/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01661: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4314 - val_loss: 1.4799\n",
      "Epoch 1662/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01662: loss did not improve from 1.42791\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4300 - val_loss: 1.4806\n",
      "Epoch 1663/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01663: loss improved from 1.42791 to 1.42711, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4271 - val_loss: 1.4829\n",
      "Epoch 1664/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01664: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4278 - val_loss: 1.4805\n",
      "Epoch 1665/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01665: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4273 - val_loss: 1.4803\n",
      "Epoch 1666/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01666: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4325 - val_loss: 1.4816\n",
      "Epoch 1667/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01667: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4807\n",
      "Epoch 1668/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4313\n",
      "Epoch 01668: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4313 - val_loss: 1.4793\n",
      "Epoch 1669/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01669: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4315 - val_loss: 1.4790\n",
      "Epoch 1670/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01670: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4276 - val_loss: 1.4775\n",
      "Epoch 1671/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4282\n",
      "Epoch 01671: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4282 - val_loss: 1.4786\n",
      "Epoch 1672/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4304\n",
      "Epoch 01672: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4304 - val_loss: 1.4795\n",
      "Epoch 1673/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01673: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4272 - val_loss: 1.4808\n",
      "Epoch 1674/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01674: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4297 - val_loss: 1.4813\n",
      "Epoch 1675/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01675: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4793\n",
      "Epoch 1676/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01676: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4285 - val_loss: 1.4819\n",
      "Epoch 1677/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01677: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4307 - val_loss: 1.4824\n",
      "Epoch 1678/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 01678: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4296 - val_loss: 1.4847\n",
      "Epoch 1679/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01679: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4787\n",
      "Epoch 1680/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 01680: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4283 - val_loss: 1.4811\n",
      "Epoch 1681/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4280\n",
      "Epoch 01681: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4280 - val_loss: 1.4782\n",
      "Epoch 1682/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 01682: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4289 - val_loss: 1.4805\n",
      "Epoch 1683/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01683: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4300 - val_loss: 1.4877\n",
      "Epoch 1684/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4342\n",
      "Epoch 01684: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4342 - val_loss: 1.4868\n",
      "Epoch 1685/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4349\n",
      "Epoch 01685: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4349 - val_loss: 1.4898\n",
      "Epoch 1686/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4327\n",
      "Epoch 01686: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4327 - val_loss: 1.4837\n",
      "Epoch 1687/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01687: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4310 - val_loss: 1.4830\n",
      "Epoch 1688/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4334\n",
      "Epoch 01688: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4334 - val_loss: 1.4783\n",
      "Epoch 1689/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01689: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4295 - val_loss: 1.4815\n",
      "Epoch 1690/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4312\n",
      "Epoch 01690: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4312 - val_loss: 1.4817\n",
      "Epoch 1691/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01691: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4314 - val_loss: 1.4815\n",
      "Epoch 1692/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 01692: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4275 - val_loss: 1.4807\n",
      "Epoch 1693/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 01693: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4283 - val_loss: 1.4788\n",
      "Epoch 1694/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4481\n",
      "Epoch 01694: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4481 - val_loss: 1.4816\n",
      "Epoch 1695/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4471\n",
      "Epoch 01695: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4471 - val_loss: 1.4811\n",
      "Epoch 1696/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4436\n",
      "Epoch 01696: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4436 - val_loss: 1.4831\n",
      "Epoch 1697/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4385\n",
      "Epoch 01697: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4385 - val_loss: 1.4815\n",
      "Epoch 1698/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4344\n",
      "Epoch 01698: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4344 - val_loss: 1.4782\n",
      "Epoch 1699/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4327\n",
      "Epoch 01699: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4327 - val_loss: 1.4821\n",
      "Epoch 1700/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4322\n",
      "Epoch 01700: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4322 - val_loss: 1.4817\n",
      "Epoch 1701/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01701: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4318 - val_loss: 1.4850\n",
      "Epoch 1702/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4339\n",
      "Epoch 01702: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4339 - val_loss: 1.4785\n",
      "Epoch 1703/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01703: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4320 - val_loss: 1.4781\n",
      "Epoch 1704/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01704: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4299 - val_loss: 1.4819\n",
      "Epoch 1705/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01705: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4314 - val_loss: 1.4805\n",
      "Epoch 1706/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4428\n",
      "Epoch 01706: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4428 - val_loss: 1.4836\n",
      "Epoch 1707/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4389\n",
      "Epoch 01707: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4389 - val_loss: 1.4810\n",
      "Epoch 1708/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4348\n",
      "Epoch 01708: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4348 - val_loss: 1.4829\n",
      "Epoch 1709/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01709: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4325 - val_loss: 1.4793\n",
      "Epoch 1710/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01710: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4802\n",
      "Epoch 1711/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01711: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4297 - val_loss: 1.4783\n",
      "Epoch 1712/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4376\n",
      "Epoch 01712: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4376 - val_loss: 1.4818\n",
      "Epoch 1713/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4361\n",
      "Epoch 01713: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4361 - val_loss: 1.4798\n",
      "Epoch 1714/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4331\n",
      "Epoch 01714: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4331 - val_loss: 1.4781\n",
      "Epoch 1715/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01715: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4297 - val_loss: 1.4784\n",
      "Epoch 1716/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4304\n",
      "Epoch 01716: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4304 - val_loss: 1.4840\n",
      "Epoch 1717/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01717: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4307 - val_loss: 1.4807\n",
      "Epoch 1718/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01718: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4307 - val_loss: 1.4822\n",
      "Epoch 1719/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01719: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4317 - val_loss: 1.4785\n",
      "Epoch 1720/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4324\n",
      "Epoch 01720: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4324 - val_loss: 1.4809\n",
      "Epoch 1721/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4316\n",
      "Epoch 01721: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4316 - val_loss: 1.4819\n",
      "Epoch 1722/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4321\n",
      "Epoch 01722: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4321 - val_loss: 1.4858\n",
      "Epoch 1723/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01723: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4325 - val_loss: 1.4814\n",
      "Epoch 1724/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4322\n",
      "Epoch 01724: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4322 - val_loss: 1.4798\n",
      "Epoch 1725/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01725: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4332 - val_loss: 1.4793\n",
      "Epoch 1726/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01726: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4299 - val_loss: 1.4800\n",
      "Epoch 1727/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01727: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4299 - val_loss: 1.4813\n",
      "Epoch 1728/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01728: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4286 - val_loss: 1.4829\n",
      "Epoch 1729/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01729: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4295 - val_loss: 1.4811\n",
      "Epoch 1730/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4312\n",
      "Epoch 01730: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4312 - val_loss: 1.4826\n",
      "Epoch 1731/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01731: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4305 - val_loss: 1.4830\n",
      "Epoch 1732/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4292\n",
      "Epoch 01732: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4292 - val_loss: 1.4789\n",
      "Epoch 1733/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4280\n",
      "Epoch 01733: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4280 - val_loss: 1.4835\n",
      "Epoch 1734/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01734: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4356 - val_loss: 1.4815\n",
      "Epoch 1735/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01735: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4338 - val_loss: 1.4815\n",
      "Epoch 1736/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01736: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4310 - val_loss: 1.4808\n",
      "Epoch 1737/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 01737: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4296 - val_loss: 1.4818\n",
      "Epoch 1738/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01738: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4307 - val_loss: 1.4814\n",
      "Epoch 1739/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01739: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4300 - val_loss: 1.4833\n",
      "Epoch 1740/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01740: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4309 - val_loss: 1.4809\n",
      "Epoch 1741/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01741: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4317 - val_loss: 1.4821\n",
      "Epoch 1742/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01742: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4310 - val_loss: 1.4820\n",
      "Epoch 1743/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01743: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4276 - val_loss: 1.4844\n",
      "Epoch 1744/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01744: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4286 - val_loss: 1.4841\n",
      "Epoch 1745/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01745: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4791\n",
      "Epoch 1746/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01746: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4278 - val_loss: 1.4800\n",
      "Epoch 1747/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01747: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4285 - val_loss: 1.4775\n",
      "Epoch 1748/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01748: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4274 - val_loss: 1.4792\n",
      "Epoch 1749/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01749: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4288 - val_loss: 1.4871\n",
      "Epoch 1750/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4407\n",
      "Epoch 01750: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4407 - val_loss: 1.4828\n",
      "Epoch 1751/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4363\n",
      "Epoch 01751: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4363 - val_loss: 1.4833\n",
      "Epoch 1752/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01752: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4325 - val_loss: 1.4943\n",
      "Epoch 1753/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4355\n",
      "Epoch 01753: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4355 - val_loss: 1.4826\n",
      "Epoch 1754/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01754: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4315 - val_loss: 1.4815\n",
      "Epoch 1755/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4292\n",
      "Epoch 01755: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4292 - val_loss: 1.4810\n",
      "Epoch 1756/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01756: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4279 - val_loss: 1.4834\n",
      "Epoch 1757/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01757: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4315 - val_loss: 1.4817\n",
      "Epoch 1758/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01758: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4306 - val_loss: 1.4818\n",
      "Epoch 1759/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4326\n",
      "Epoch 01759: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4326 - val_loss: 1.4850\n",
      "Epoch 1760/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01760: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4309 - val_loss: 1.4828\n",
      "Epoch 1761/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01761: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4799\n",
      "Epoch 1762/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01762: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4307 - val_loss: 1.4793\n",
      "Epoch 1763/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01763: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4288 - val_loss: 1.4813\n",
      "Epoch 1764/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01764: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4303 - val_loss: 1.4826\n",
      "Epoch 1765/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01765: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4298 - val_loss: 1.4836\n",
      "Epoch 1766/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01766: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4805\n",
      "Epoch 1767/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01767: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4302 - val_loss: 1.4815\n",
      "Epoch 1768/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4391\n",
      "Epoch 01768: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4391 - val_loss: 1.4807\n",
      "Epoch 1769/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4339\n",
      "Epoch 01769: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4339 - val_loss: 1.4882\n",
      "Epoch 1770/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01770: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4796\n",
      "Epoch 1771/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01771: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4303 - val_loss: 1.4794\n",
      "Epoch 1772/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01772: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4294 - val_loss: 1.4800\n",
      "Epoch 1773/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 01773: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4296 - val_loss: 1.4873\n",
      "Epoch 1774/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01774: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4306 - val_loss: 1.4828\n",
      "Epoch 1775/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01775: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4822\n",
      "Epoch 1776/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01776: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4278 - val_loss: 1.4831\n",
      "Epoch 1777/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01777: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4288 - val_loss: 1.4851\n",
      "Epoch 1778/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01778: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4297 - val_loss: 1.4800\n",
      "Epoch 1779/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01779: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4299 - val_loss: 1.4839\n",
      "Epoch 1780/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4312\n",
      "Epoch 01780: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4312 - val_loss: 1.4848\n",
      "Epoch 1781/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01781: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4315 - val_loss: 1.4835\n",
      "Epoch 1782/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 01782: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4287 - val_loss: 1.4785\n",
      "Epoch 1783/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4287\n",
      "Epoch 01783: loss did not improve from 1.42711\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4287 - val_loss: 1.4804\n",
      "Epoch 1784/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4262\n",
      "Epoch 01784: loss improved from 1.42711 to 1.42620, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4262 - val_loss: 1.4813\n",
      "Epoch 1785/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 01785: loss did not improve from 1.42620\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4267 - val_loss: 1.4826\n",
      "Epoch 1786/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 01786: loss did not improve from 1.42620\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4263 - val_loss: 1.4829\n",
      "Epoch 1787/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01787: loss did not improve from 1.42620\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4265 - val_loss: 1.4790\n",
      "Epoch 1788/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01788: loss did not improve from 1.42620\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4278 - val_loss: 1.4828\n",
      "Epoch 1789/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01789: loss did not improve from 1.42620\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4271 - val_loss: 1.4825\n",
      "Epoch 1790/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01790: loss did not improve from 1.42620\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4824\n",
      "Epoch 1791/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01791: loss did not improve from 1.42620\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4320 - val_loss: 1.4821\n",
      "Epoch 1792/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01792: loss did not improve from 1.42620\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4800\n",
      "Epoch 1793/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 01793: loss did not improve from 1.42620\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4297 - val_loss: 1.4785\n",
      "Epoch 1794/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 01794: loss improved from 1.42620 to 1.42517, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4252 - val_loss: 1.4812\n",
      "Epoch 1795/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01795: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4272 - val_loss: 1.4806\n",
      "Epoch 1796/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01796: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4274 - val_loss: 1.4828\n",
      "Epoch 1797/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01797: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4306 - val_loss: 1.4824\n",
      "Epoch 1798/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4314\n",
      "Epoch 01798: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4314 - val_loss: 1.4833\n",
      "Epoch 1799/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01799: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4303 - val_loss: 1.4815\n",
      "Epoch 1800/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 01800: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4289 - val_loss: 1.4796\n",
      "Epoch 1801/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 01801: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4294 - val_loss: 1.4789\n",
      "Epoch 1802/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01802: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4281 - val_loss: 1.4815\n",
      "Epoch 1803/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01803: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4273 - val_loss: 1.4820\n",
      "Epoch 1804/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01804: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4269 - val_loss: 1.4818\n",
      "Epoch 1805/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4262\n",
      "Epoch 01805: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4262 - val_loss: 1.4866\n",
      "Epoch 1806/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01806: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4269 - val_loss: 1.4819\n",
      "Epoch 1807/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4298\n",
      "Epoch 01807: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4298 - val_loss: 1.4839\n",
      "Epoch 1808/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01808: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4276 - val_loss: 1.4828\n",
      "Epoch 1809/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01809: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4265 - val_loss: 1.4836\n",
      "Epoch 1810/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01810: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4302 - val_loss: 1.4850\n",
      "Epoch 1811/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4324\n",
      "Epoch 01811: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4324 - val_loss: 1.4929\n",
      "Epoch 1812/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4350\n",
      "Epoch 01812: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4350 - val_loss: 1.4823\n",
      "Epoch 1813/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 01813: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4820\n",
      "Epoch 1814/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01814: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4281 - val_loss: 1.4842\n",
      "Epoch 1815/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4325\n",
      "Epoch 01815: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4325 - val_loss: 1.4849\n",
      "Epoch 1816/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4362\n",
      "Epoch 01816: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4362 - val_loss: 1.4877\n",
      "Epoch 1817/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 01817: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4338 - val_loss: 1.4844\n",
      "Epoch 1818/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 01818: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4290 - val_loss: 1.4791\n",
      "Epoch 1819/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01819: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4865\n",
      "Epoch 1820/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01820: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4318 - val_loss: 1.4815\n",
      "Epoch 1821/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 01821: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4306 - val_loss: 1.4796\n",
      "Epoch 1822/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01822: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4818\n",
      "Epoch 1823/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4313\n",
      "Epoch 01823: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4313 - val_loss: 1.4823\n",
      "Epoch 1824/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4300\n",
      "Epoch 01824: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4300 - val_loss: 1.4831\n",
      "Epoch 1825/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 01825: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4290 - val_loss: 1.4820\n",
      "Epoch 1826/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01826: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4272 - val_loss: 1.4820\n",
      "Epoch 1827/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 01827: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4277 - val_loss: 1.4825\n",
      "Epoch 1828/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01828: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4281 - val_loss: 1.4896\n",
      "Epoch 1829/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4345\n",
      "Epoch 01829: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4345 - val_loss: 1.4844\n",
      "Epoch 1830/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 01830: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4279 - val_loss: 1.4808\n",
      "Epoch 1831/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 01831: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4267 - val_loss: 1.4856\n",
      "Epoch 1832/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01832: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4302 - val_loss: 1.4819\n",
      "Epoch 1833/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 01833: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4288 - val_loss: 1.4816\n",
      "Epoch 1834/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01834: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4276 - val_loss: 1.4835\n",
      "Epoch 1835/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 01835: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4296 - val_loss: 1.4779\n",
      "Epoch 1836/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01836: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4284 - val_loss: 1.4798\n",
      "Epoch 1837/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 01837: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4264 - val_loss: 1.4822\n",
      "Epoch 1838/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4307\n",
      "Epoch 01838: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4307 - val_loss: 1.4840\n",
      "Epoch 1839/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 01839: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4305 - val_loss: 1.4839\n",
      "Epoch 1840/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01840: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4274 - val_loss: 1.4821\n",
      "Epoch 1841/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 01841: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4303 - val_loss: 1.4857\n",
      "Epoch 1842/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01842: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4812\n",
      "Epoch 1843/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4270\n",
      "Epoch 01843: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4270 - val_loss: 1.4775\n",
      "Epoch 1844/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01844: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4265 - val_loss: 1.4804\n",
      "Epoch 1845/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 01845: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4855\n",
      "Epoch 1846/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01846: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4278 - val_loss: 1.4820\n",
      "Epoch 1847/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4312\n",
      "Epoch 01847: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4312 - val_loss: 1.4833\n",
      "Epoch 1848/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4291\n",
      "Epoch 01848: loss did not improve from 1.42517\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4291 - val_loss: 1.4815\n",
      "Epoch 1849/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 01849: loss improved from 1.42517 to 1.42479, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4248 - val_loss: 1.4804\n",
      "Epoch 1850/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 01850: loss did not improve from 1.42479\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4263 - val_loss: 1.4802\n",
      "Epoch 1851/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 01851: loss improved from 1.42479 to 1.42470, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4247 - val_loss: 1.4837\n",
      "Epoch 1852/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01852: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4302 - val_loss: 1.4821\n",
      "Epoch 1853/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01853: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4286 - val_loss: 1.4807\n",
      "Epoch 1854/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01854: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4286 - val_loss: 1.4834\n",
      "Epoch 1855/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01855: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4281 - val_loss: 1.4829\n",
      "Epoch 1856/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01856: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4265 - val_loss: 1.4818\n",
      "Epoch 1857/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01857: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4309 - val_loss: 1.4852\n",
      "Epoch 1858/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01858: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4271 - val_loss: 1.4821\n",
      "Epoch 1859/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01859: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4827\n",
      "Epoch 1860/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4254\n",
      "Epoch 01860: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4254 - val_loss: 1.4869\n",
      "Epoch 1861/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4282\n",
      "Epoch 01861: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4282 - val_loss: 1.4810\n",
      "Epoch 1862/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 01862: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4276 - val_loss: 1.4842\n",
      "Epoch 1863/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 01863: loss did not improve from 1.42470\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4263 - val_loss: 1.4793\n",
      "Epoch 1864/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 01864: loss improved from 1.42470 to 1.42442, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4244 - val_loss: 1.4839\n",
      "Epoch 1865/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01865: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4271 - val_loss: 1.4816\n",
      "Epoch 1866/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 01866: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4253 - val_loss: 1.4815\n",
      "Epoch 1867/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 01867: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4258 - val_loss: 1.4833\n",
      "Epoch 1868/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 01868: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4857\n",
      "Epoch 1869/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01869: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4319 - val_loss: 1.4817\n",
      "Epoch 1870/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4316\n",
      "Epoch 01870: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4316 - val_loss: 1.4880\n",
      "Epoch 1871/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01871: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4284 - val_loss: 1.4843\n",
      "Epoch 1872/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01872: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4286 - val_loss: 1.4846\n",
      "Epoch 1873/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01873: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4285 - val_loss: 1.4808\n",
      "Epoch 1874/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 01874: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4277 - val_loss: 1.4838\n",
      "Epoch 1875/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 01875: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4275 - val_loss: 1.4841\n",
      "Epoch 1876/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01876: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4285 - val_loss: 1.4817\n",
      "Epoch 1877/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01877: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4265 - val_loss: 1.4827\n",
      "Epoch 1878/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01878: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4269 - val_loss: 1.4825\n",
      "Epoch 1879/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 01879: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4249 - val_loss: 1.4859\n",
      "Epoch 1880/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 01880: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4800\n",
      "Epoch 1881/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 01881: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4253 - val_loss: 1.4874\n",
      "Epoch 1882/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4356\n",
      "Epoch 01882: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4356 - val_loss: 1.4835\n",
      "Epoch 1883/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4387\n",
      "Epoch 01883: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4387 - val_loss: 1.4875\n",
      "Epoch 1884/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4335\n",
      "Epoch 01884: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4335 - val_loss: 1.4807\n",
      "Epoch 1885/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01885: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4271 - val_loss: 1.4800\n",
      "Epoch 1886/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4254\n",
      "Epoch 01886: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4254 - val_loss: 1.4821\n",
      "Epoch 1887/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 01887: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4831\n",
      "Epoch 1888/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 01888: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4261 - val_loss: 1.4822\n",
      "Epoch 1889/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4292\n",
      "Epoch 01889: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4292 - val_loss: 1.4822\n",
      "Epoch 1890/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 01890: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4267 - val_loss: 1.4814\n",
      "Epoch 1891/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 01891: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4257 - val_loss: 1.4807\n",
      "Epoch 1892/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 01892: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4259 - val_loss: 1.4803\n",
      "Epoch 1893/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 01893: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4263 - val_loss: 1.4810\n",
      "Epoch 1894/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01894: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4825\n",
      "Epoch 1895/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 01895: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4249 - val_loss: 1.4877\n",
      "Epoch 1896/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01896: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4309 - val_loss: 1.4816\n",
      "Epoch 1897/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01897: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4286 - val_loss: 1.4822\n",
      "Epoch 1898/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 01898: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4271 - val_loss: 1.4843\n",
      "Epoch 1899/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 01899: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4258 - val_loss: 1.4862\n",
      "Epoch 1900/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 01900: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4277 - val_loss: 1.4856\n",
      "Epoch 1901/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01901: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4265 - val_loss: 1.4818\n",
      "Epoch 1902/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4295\n",
      "Epoch 01902: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4295 - val_loss: 1.4876\n",
      "Epoch 1903/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4320\n",
      "Epoch 01903: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4320 - val_loss: 1.4848\n",
      "Epoch 1904/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4319\n",
      "Epoch 01904: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4319 - val_loss: 1.4837\n",
      "Epoch 1905/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01905: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4278 - val_loss: 1.4812\n",
      "Epoch 1906/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 01906: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4275 - val_loss: 1.4856\n",
      "Epoch 1907/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 01907: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4290 - val_loss: 1.4844\n",
      "Epoch 1908/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 01908: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4267 - val_loss: 1.4850\n",
      "Epoch 1909/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 01909: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4251 - val_loss: 1.4883\n",
      "Epoch 1910/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01910: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4265 - val_loss: 1.4849\n",
      "Epoch 1911/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 01911: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4261 - val_loss: 1.4830\n",
      "Epoch 1912/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 01912: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4269 - val_loss: 1.4848\n",
      "Epoch 1913/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 01913: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4268 - val_loss: 1.4843\n",
      "Epoch 1914/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4265\n",
      "Epoch 01914: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4265 - val_loss: 1.4855\n",
      "Epoch 1915/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 01915: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4249 - val_loss: 1.4834\n",
      "Epoch 1916/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 01916: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4251 - val_loss: 1.4836\n",
      "Epoch 1917/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4254\n",
      "Epoch 01917: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4254 - val_loss: 1.4851\n",
      "Epoch 1918/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 01918: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4816\n",
      "Epoch 1919/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 01919: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4796\n",
      "Epoch 1920/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 01920: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4249 - val_loss: 1.4835\n",
      "Epoch 1921/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 01921: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4261 - val_loss: 1.4852\n",
      "Epoch 1922/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01922: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4281 - val_loss: 1.4856\n",
      "Epoch 1923/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4302\n",
      "Epoch 01923: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4302 - val_loss: 1.4833\n",
      "Epoch 1924/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 01924: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4257 - val_loss: 1.4856\n",
      "Epoch 1925/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01925: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4278 - val_loss: 1.4850\n",
      "Epoch 1926/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4292\n",
      "Epoch 01926: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4292 - val_loss: 1.4846\n",
      "Epoch 1927/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 01927: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4289 - val_loss: 1.4815\n",
      "Epoch 1928/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 01928: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4258 - val_loss: 1.4837\n",
      "Epoch 1929/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01929: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4851\n",
      "Epoch 1930/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 01930: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4263 - val_loss: 1.4837\n",
      "Epoch 1931/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 01931: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4274 - val_loss: 1.4831\n",
      "Epoch 1932/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 01932: loss did not improve from 1.42442\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4806\n",
      "Epoch 1933/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 01933: loss improved from 1.42442 to 1.42379, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4238 - val_loss: 1.4823\n",
      "Epoch 1934/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01934: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4273 - val_loss: 1.4829\n",
      "Epoch 1935/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 01935: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4328 - val_loss: 1.4877\n",
      "Epoch 1936/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4318\n",
      "Epoch 01936: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4318 - val_loss: 1.4863\n",
      "Epoch 1937/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4270\n",
      "Epoch 01937: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4270 - val_loss: 1.4802\n",
      "Epoch 1938/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4262\n",
      "Epoch 01938: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4262 - val_loss: 1.4856\n",
      "Epoch 1939/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 01939: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4264 - val_loss: 1.4812\n",
      "Epoch 1940/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 01940: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4839\n",
      "Epoch 1941/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 01941: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4259 - val_loss: 1.4831\n",
      "Epoch 1942/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 01942: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4257 - val_loss: 1.4846\n",
      "Epoch 1943/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 01943: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4247 - val_loss: 1.4886\n",
      "Epoch 1944/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01944: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4284 - val_loss: 1.4843\n",
      "Epoch 1945/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 01945: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4275 - val_loss: 1.4850\n",
      "Epoch 1946/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4315\n",
      "Epoch 01946: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4315 - val_loss: 1.4848\n",
      "Epoch 1947/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 01947: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4844\n",
      "Epoch 1948/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 01948: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4286 - val_loss: 1.4827\n",
      "Epoch 1949/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 01949: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4264 - val_loss: 1.4835\n",
      "Epoch 1950/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 01950: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4253 - val_loss: 1.4816\n",
      "Epoch 1951/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 01951: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4837\n",
      "Epoch 1952/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01952: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4820\n",
      "Epoch 1953/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 01953: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4281 - val_loss: 1.4831\n",
      "Epoch 1954/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 01954: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4278 - val_loss: 1.4862\n",
      "Epoch 1955/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01955: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4830\n",
      "Epoch 1956/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 01956: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4838\n",
      "Epoch 1957/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 01957: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4855\n",
      "Epoch 1958/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01958: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4272 - val_loss: 1.4843\n",
      "Epoch 1959/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 01959: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4260 - val_loss: 1.4793\n",
      "Epoch 1960/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01960: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4816\n",
      "Epoch 1961/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 01961: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4259 - val_loss: 1.4803\n",
      "Epoch 1962/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 01962: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4272 - val_loss: 1.4835\n",
      "Epoch 1963/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 01963: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4268 - val_loss: 1.4822\n",
      "Epoch 1964/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 01964: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4256 - val_loss: 1.4827\n",
      "Epoch 1965/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 01965: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4855\n",
      "Epoch 1966/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 01966: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4837\n",
      "Epoch 1967/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4304\n",
      "Epoch 01967: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4304 - val_loss: 1.4857\n",
      "Epoch 1968/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 01968: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4264 - val_loss: 1.4839\n",
      "Epoch 1969/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01969: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4285 - val_loss: 1.4804\n",
      "Epoch 1970/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4309\n",
      "Epoch 01970: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4309 - val_loss: 1.4856\n",
      "Epoch 1971/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01971: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4299 - val_loss: 1.4833\n",
      "Epoch 1972/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4270\n",
      "Epoch 01972: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4270 - val_loss: 1.4826\n",
      "Epoch 1973/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 01973: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4260 - val_loss: 1.4818\n",
      "Epoch 1974/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 01974: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4259 - val_loss: 1.4822\n",
      "Epoch 1975/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 01975: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4913\n",
      "Epoch 1976/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 01976: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4317 - val_loss: 1.4850\n",
      "Epoch 1977/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 01977: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4289 - val_loss: 1.4854\n",
      "Epoch 1978/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 01978: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4264 - val_loss: 1.4818\n",
      "Epoch 1979/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 01979: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4795\n",
      "Epoch 1980/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 01980: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4257 - val_loss: 1.4827\n",
      "Epoch 1981/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 01981: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4268 - val_loss: 1.4828\n",
      "Epoch 1982/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 01982: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4267 - val_loss: 1.4820\n",
      "Epoch 1983/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 01983: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4257 - val_loss: 1.4820\n",
      "Epoch 1984/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 01984: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4240 - val_loss: 1.4842\n",
      "Epoch 1985/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 01985: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4241 - val_loss: 1.4846\n",
      "Epoch 1986/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 01986: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4260 - val_loss: 1.4847\n",
      "Epoch 1987/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 01987: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4267 - val_loss: 1.4819\n",
      "Epoch 1988/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 01988: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4268 - val_loss: 1.4792\n",
      "Epoch 1989/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 01989: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4285 - val_loss: 1.4833\n",
      "Epoch 1990/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4299\n",
      "Epoch 01990: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4299 - val_loss: 1.4915\n",
      "Epoch 1991/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4340\n",
      "Epoch 01991: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4340 - val_loss: 1.4903\n",
      "Epoch 1992/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4304\n",
      "Epoch 01992: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4304 - val_loss: 1.4814\n",
      "Epoch 1993/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 01993: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4831\n",
      "Epoch 1994/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4284\n",
      "Epoch 01994: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4284 - val_loss: 1.4903\n",
      "Epoch 1995/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 01995: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4310 - val_loss: 1.4878\n",
      "Epoch 1996/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 01996: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4290 - val_loss: 1.4846\n",
      "Epoch 1997/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4308\n",
      "Epoch 01997: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4308 - val_loss: 1.4863\n",
      "Epoch 1998/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4304\n",
      "Epoch 01998: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4304 - val_loss: 1.4835\n",
      "Epoch 1999/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 01999: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4261 - val_loss: 1.4829\n",
      "Epoch 2000/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 02000: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4281 - val_loss: 1.4836\n",
      "Epoch 2001/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4303\n",
      "Epoch 02001: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4303 - val_loss: 1.4845\n",
      "Epoch 2002/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 02002: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4277 - val_loss: 1.4821\n",
      "Epoch 2003/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02003: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4251 - val_loss: 1.4815\n",
      "Epoch 2004/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 02004: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4288 - val_loss: 1.4820\n",
      "Epoch 2005/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 02005: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4274 - val_loss: 1.4828\n",
      "Epoch 2006/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02006: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4259 - val_loss: 1.4833\n",
      "Epoch 2007/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4242\n",
      "Epoch 02007: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4242 - val_loss: 1.4820\n",
      "Epoch 2008/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 02008: loss did not improve from 1.42379\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4252 - val_loss: 1.4834\n",
      "Epoch 2009/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02009: loss improved from 1.42379 to 1.42279, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4228 - val_loss: 1.4831\n",
      "Epoch 2010/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 02010: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4243 - val_loss: 1.4815\n",
      "Epoch 2011/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02011: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4234 - val_loss: 1.4817\n",
      "Epoch 2012/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02012: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4846\n",
      "Epoch 2013/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02013: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4258 - val_loss: 1.4864\n",
      "Epoch 2014/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4289\n",
      "Epoch 02014: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4289 - val_loss: 1.4845\n",
      "Epoch 2015/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 02015: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4271 - val_loss: 1.4808\n",
      "Epoch 2016/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02016: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4259 - val_loss: 1.4852\n",
      "Epoch 2017/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02017: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4258 - val_loss: 1.4823\n",
      "Epoch 2018/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 02018: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4275 - val_loss: 1.4840\n",
      "Epoch 2019/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 02019: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4884\n",
      "Epoch 2020/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4280\n",
      "Epoch 02020: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4280 - val_loss: 1.4832\n",
      "Epoch 2021/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02021: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4829\n",
      "Epoch 2022/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 02022: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4288 - val_loss: 1.4872\n",
      "Epoch 2023/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4283\n",
      "Epoch 02023: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4283 - val_loss: 1.4830\n",
      "Epoch 2024/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02024: loss did not improve from 1.42279\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4836\n",
      "Epoch 2025/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02025: loss improved from 1.42279 to 1.42240, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4224 - val_loss: 1.4832\n",
      "Epoch 2026/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02026: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4239 - val_loss: 1.4849\n",
      "Epoch 2027/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02027: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4244 - val_loss: 1.4881\n",
      "Epoch 2028/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 02028: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4288 - val_loss: 1.4859\n",
      "Epoch 2029/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 02029: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4252 - val_loss: 1.4823\n",
      "Epoch 2030/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02030: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4247 - val_loss: 1.4884\n",
      "Epoch 2031/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 02031: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4267 - val_loss: 1.4842\n",
      "Epoch 2032/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 02032: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4243 - val_loss: 1.4835\n",
      "Epoch 2033/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02033: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4250 - val_loss: 1.4815\n",
      "Epoch 2034/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 02034: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4248 - val_loss: 1.4848\n",
      "Epoch 2035/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02035: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4259 - val_loss: 1.4875\n",
      "Epoch 2036/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 02036: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4290 - val_loss: 1.4883\n",
      "Epoch 2037/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 02037: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4243 - val_loss: 1.4837\n",
      "Epoch 2038/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02038: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4241 - val_loss: 1.4882\n",
      "Epoch 2039/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 02039: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4275 - val_loss: 1.4864\n",
      "Epoch 2040/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4280\n",
      "Epoch 02040: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4280 - val_loss: 1.4869\n",
      "Epoch 2041/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02041: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4852\n",
      "Epoch 2042/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 02042: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4264 - val_loss: 1.4887\n",
      "Epoch 2043/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 02043: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4257 - val_loss: 1.4872\n",
      "Epoch 2044/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4278\n",
      "Epoch 02044: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4278 - val_loss: 1.4824\n",
      "Epoch 2045/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 02045: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4257 - val_loss: 1.4846\n",
      "Epoch 2046/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02046: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4240 - val_loss: 1.4827\n",
      "Epoch 2047/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4384\n",
      "Epoch 02047: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4384 - val_loss: 1.4837\n",
      "Epoch 2048/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4382\n",
      "Epoch 02048: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4382 - val_loss: 1.4809\n",
      "Epoch 2049/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4321\n",
      "Epoch 02049: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4321 - val_loss: 1.4871\n",
      "Epoch 2050/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4306\n",
      "Epoch 02050: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4306 - val_loss: 1.4876\n",
      "Epoch 2051/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 02051: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4290 - val_loss: 1.4848\n",
      "Epoch 2052/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4275\n",
      "Epoch 02052: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4275 - val_loss: 1.4824\n",
      "Epoch 2053/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 02053: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4274 - val_loss: 1.4830\n",
      "Epoch 2054/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 02054: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4252 - val_loss: 1.4834\n",
      "Epoch 2055/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02055: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4865\n",
      "Epoch 2056/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02056: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4829\n",
      "Epoch 2057/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02057: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4239 - val_loss: 1.4842\n",
      "Epoch 2058/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 02058: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4243 - val_loss: 1.4825\n",
      "Epoch 2059/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 02059: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4286 - val_loss: 1.4854\n",
      "Epoch 2060/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4339\n",
      "Epoch 02060: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4339 - val_loss: 1.4878\n",
      "Epoch 2061/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4310\n",
      "Epoch 02061: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4310 - val_loss: 1.4838\n",
      "Epoch 2062/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4296\n",
      "Epoch 02062: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4296 - val_loss: 1.4828\n",
      "Epoch 2063/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 02063: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4288 - val_loss: 1.4867\n",
      "Epoch 2064/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4274\n",
      "Epoch 02064: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4274 - val_loss: 1.4830\n",
      "Epoch 2065/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 02065: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4279 - val_loss: 1.4840\n",
      "Epoch 2066/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4290\n",
      "Epoch 02066: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4290 - val_loss: 1.4858\n",
      "Epoch 2067/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 02067: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4277 - val_loss: 1.4851\n",
      "Epoch 2068/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02068: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4244 - val_loss: 1.4850\n",
      "Epoch 2069/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02069: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4251 - val_loss: 1.4822\n",
      "Epoch 2070/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02070: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4241 - val_loss: 1.4867\n",
      "Epoch 2071/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 02071: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4263 - val_loss: 1.4829\n",
      "Epoch 2072/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02072: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4259 - val_loss: 1.4856\n",
      "Epoch 2073/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 02073: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4277 - val_loss: 1.4835\n",
      "Epoch 2074/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4262\n",
      "Epoch 02074: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4262 - val_loss: 1.4819\n",
      "Epoch 2075/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 02075: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4243 - val_loss: 1.4841\n",
      "Epoch 2076/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 02076: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4252 - val_loss: 1.4846\n",
      "Epoch 2077/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 02077: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4276 - val_loss: 1.4846\n",
      "Epoch 2078/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 02078: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4286 - val_loss: 1.4839\n",
      "Epoch 2079/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4273\n",
      "Epoch 02079: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4273 - val_loss: 1.4835\n",
      "Epoch 2080/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 02080: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4253 - val_loss: 1.4812\n",
      "Epoch 2081/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4267\n",
      "Epoch 02081: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4267 - val_loss: 1.4855\n",
      "Epoch 2082/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 02082: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4263 - val_loss: 1.4823\n",
      "Epoch 2083/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02083: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4865\n",
      "Epoch 2084/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02084: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4238 - val_loss: 1.4834\n",
      "Epoch 2085/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02085: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4838\n",
      "Epoch 2086/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4231\n",
      "Epoch 02086: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4231 - val_loss: 1.4831\n",
      "Epoch 2087/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4254\n",
      "Epoch 02087: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4254 - val_loss: 1.4856\n",
      "Epoch 2088/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02088: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4241 - val_loss: 1.4867\n",
      "Epoch 2089/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4262\n",
      "Epoch 02089: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4262 - val_loss: 1.4865\n",
      "Epoch 2090/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4301\n",
      "Epoch 02090: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4301 - val_loss: 1.4842\n",
      "Epoch 2091/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02091: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4255 - val_loss: 1.4884\n",
      "Epoch 2092/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02092: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4256 - val_loss: 1.4845\n",
      "Epoch 2093/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4254\n",
      "Epoch 02093: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4254 - val_loss: 1.4828\n",
      "Epoch 2094/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4248\n",
      "Epoch 02094: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4246 - val_loss: 1.4847\n",
      "Epoch 2095/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02095: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4228 - val_loss: 1.4874\n",
      "Epoch 2096/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 02096: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4279 - val_loss: 1.4848\n",
      "Epoch 2097/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4304\n",
      "Epoch 02097: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4304 - val_loss: 1.4833\n",
      "Epoch 2098/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4277\n",
      "Epoch 02098: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4277 - val_loss: 1.4853\n",
      "Epoch 2099/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02099: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4842\n",
      "Epoch 2100/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02100: loss did not improve from 1.42240\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4824\n",
      "Epoch 2101/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02101: loss improved from 1.42240 to 1.42197, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4220 - val_loss: 1.4808\n",
      "Epoch 2102/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 02102: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4243 - val_loss: 1.4824\n",
      "Epoch 2103/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 02103: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4248 - val_loss: 1.4850\n",
      "Epoch 2104/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02104: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4238 - val_loss: 1.4862\n",
      "Epoch 2105/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 02105: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4249 - val_loss: 1.4856\n",
      "Epoch 2106/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02106: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4258 - val_loss: 1.4848\n",
      "Epoch 2107/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02107: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4837\n",
      "Epoch 2108/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02108: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4258 - val_loss: 1.4846\n",
      "Epoch 2109/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 02109: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4249 - val_loss: 1.4846\n",
      "Epoch 2110/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02110: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4238 - val_loss: 1.4865\n",
      "Epoch 2111/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 02111: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4253 - val_loss: 1.4855\n",
      "Epoch 2112/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 02112: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4243 - val_loss: 1.4839\n",
      "Epoch 2113/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4231\n",
      "Epoch 02113: loss did not improve from 1.42197\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4231 - val_loss: 1.4820\n",
      "Epoch 2114/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02114: loss improved from 1.42197 to 1.42121, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4212 - val_loss: 1.4829\n",
      "Epoch 2115/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02115: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4820\n",
      "Epoch 2116/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02116: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4228 - val_loss: 1.4855\n",
      "Epoch 2117/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02117: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4246 - val_loss: 1.4878\n",
      "Epoch 2118/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02118: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4261 - val_loss: 1.4867\n",
      "Epoch 2119/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02119: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4822\n",
      "Epoch 2120/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4230\n",
      "Epoch 02120: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4230 - val_loss: 1.4856\n",
      "Epoch 2121/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02121: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4870\n",
      "Epoch 2122/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02122: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4238 - val_loss: 1.4867\n",
      "Epoch 2123/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 02123: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4248 - val_loss: 1.4865\n",
      "Epoch 2124/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02124: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4840\n",
      "Epoch 2125/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02125: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4883\n",
      "Epoch 2126/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4227\n",
      "Epoch 02126: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4227 - val_loss: 1.4845\n",
      "Epoch 2127/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02127: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4840\n",
      "Epoch 2128/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02128: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4241 - val_loss: 1.4837\n",
      "Epoch 2129/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4237\n",
      "Epoch 02129: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4237 - val_loss: 1.4847\n",
      "Epoch 2130/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02130: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4229 - val_loss: 1.4837\n",
      "Epoch 2131/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02131: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4215 - val_loss: 1.4840\n",
      "Epoch 2132/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 02132: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4257 - val_loss: 1.4829\n",
      "Epoch 2133/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02133: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4235 - val_loss: 1.4850\n",
      "Epoch 2134/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02134: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4215 - val_loss: 1.4848\n",
      "Epoch 2135/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02135: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4817\n",
      "Epoch 2136/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02136: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4222 - val_loss: 1.4831\n",
      "Epoch 2137/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02137: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4855\n",
      "Epoch 2138/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4231\n",
      "Epoch 02138: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4231 - val_loss: 1.4858\n",
      "Epoch 2139/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02139: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4236 - val_loss: 1.4827\n",
      "Epoch 2140/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02140: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4258 - val_loss: 1.4928\n",
      "Epoch 2141/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4279\n",
      "Epoch 02141: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4279 - val_loss: 1.4850\n",
      "Epoch 2142/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02142: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4247 - val_loss: 1.4834\n",
      "Epoch 2143/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02143: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4829\n",
      "Epoch 2144/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02144: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4246 - val_loss: 1.4868\n",
      "Epoch 2145/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02145: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4238 - val_loss: 1.4869\n",
      "Epoch 2146/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02146: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4217 - val_loss: 1.4847\n",
      "Epoch 2147/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4219\n",
      "Epoch 02147: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4219 - val_loss: 1.4840\n",
      "Epoch 2148/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02148: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4234 - val_loss: 1.4864\n",
      "Epoch 2149/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4270\n",
      "Epoch 02149: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4270 - val_loss: 1.4874\n",
      "Epoch 2150/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 02150: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4253 - val_loss: 1.4858\n",
      "Epoch 2151/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02151: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4244 - val_loss: 1.4847\n",
      "Epoch 2152/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4359\n",
      "Epoch 02152: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4359 - val_loss: 1.4863\n",
      "Epoch 2153/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 02153: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4337 - val_loss: 1.4844\n",
      "Epoch 2154/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 02154: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4297 - val_loss: 1.4896\n",
      "Epoch 2155/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4280\n",
      "Epoch 02155: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 1.4280 - val_loss: 1.4862\n",
      "Epoch 2156/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02156: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4837\n",
      "Epoch 2157/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02157: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4251 - val_loss: 1.4818\n",
      "Epoch 2158/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02158: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4235 - val_loss: 1.4833\n",
      "Epoch 2159/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 02159: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4257 - val_loss: 1.4825\n",
      "Epoch 2160/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 02160: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4252 - val_loss: 1.4841\n",
      "Epoch 2161/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02161: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4238 - val_loss: 1.4880\n",
      "Epoch 2162/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 02162: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4260 - val_loss: 1.4850\n",
      "Epoch 2163/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 02163: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4260 - val_loss: 1.4829\n",
      "Epoch 2164/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02164: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4861\n",
      "Epoch 2165/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02165: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4229 - val_loss: 1.4856\n",
      "Epoch 2166/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02166: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4826\n",
      "Epoch 2167/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02167: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4827\n",
      "Epoch 2168/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 02168: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4243 - val_loss: 1.4849\n",
      "Epoch 2169/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02169: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4228 - val_loss: 1.4833\n",
      "Epoch 2170/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02170: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4840\n",
      "Epoch 2171/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02171: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4236 - val_loss: 1.4903\n",
      "Epoch 2172/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 02172: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4264 - val_loss: 1.4925\n",
      "Epoch 2173/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02173: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4245 - val_loss: 1.4826\n",
      "Epoch 2174/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02174: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4223 - val_loss: 1.4851\n",
      "Epoch 2175/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4242\n",
      "Epoch 02175: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4242 - val_loss: 1.4840\n",
      "Epoch 2176/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02176: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4238 - val_loss: 1.4854\n",
      "Epoch 2177/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02177: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4239 - val_loss: 1.4853\n",
      "Epoch 2178/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02178: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4858\n",
      "Epoch 2179/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02179: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4822\n",
      "Epoch 2180/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02180: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4234 - val_loss: 1.4876\n",
      "Epoch 2181/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 02181: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4259 - val_loss: 1.4874\n",
      "Epoch 2182/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4281\n",
      "Epoch 02182: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4281 - val_loss: 1.4851\n",
      "Epoch 2183/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4288\n",
      "Epoch 02183: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4288 - val_loss: 1.4833\n",
      "Epoch 2184/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4242\n",
      "Epoch 02184: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4242 - val_loss: 1.4860\n",
      "Epoch 2185/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 02185: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4257 - val_loss: 1.4827\n",
      "Epoch 2186/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4242\n",
      "Epoch 02186: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4242 - val_loss: 1.4834\n",
      "Epoch 2187/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02187: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4241 - val_loss: 1.4845\n",
      "Epoch 2188/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02188: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4234 - val_loss: 1.4853\n",
      "Epoch 2189/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 02189: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4257 - val_loss: 1.4844\n",
      "Epoch 2190/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02190: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4877\n",
      "Epoch 2191/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 02191: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4870\n",
      "Epoch 2192/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 02192: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4276 - val_loss: 1.4843\n",
      "Epoch 2193/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02193: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4258 - val_loss: 1.4857\n",
      "Epoch 2194/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02194: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4891\n",
      "Epoch 2195/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4237\n",
      "Epoch 02195: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4237 - val_loss: 1.4847\n",
      "Epoch 2196/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02196: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4240 - val_loss: 1.4866\n",
      "Epoch 2197/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 02197: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4249 - val_loss: 1.4844\n",
      "Epoch 2198/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02198: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4239 - val_loss: 1.4844\n",
      "Epoch 2199/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02199: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4220 - val_loss: 1.4853\n",
      "Epoch 2200/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4242\n",
      "Epoch 02200: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4242 - val_loss: 1.4859\n",
      "Epoch 2201/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02201: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4229 - val_loss: 1.4871\n",
      "Epoch 2202/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4268\n",
      "Epoch 02202: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4268 - val_loss: 1.4882\n",
      "Epoch 2203/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 02203: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4264 - val_loss: 1.4847\n",
      "Epoch 2204/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4230\n",
      "Epoch 02204: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4230 - val_loss: 1.4833\n",
      "Epoch 2205/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02205: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4223 - val_loss: 1.4881\n",
      "Epoch 2206/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02206: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4222 - val_loss: 1.4844\n",
      "Epoch 2207/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02207: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4857\n",
      "Epoch 2208/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02208: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4823\n",
      "Epoch 2209/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02209: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4850\n",
      "Epoch 2210/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 02210: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4285 - val_loss: 1.4851\n",
      "Epoch 2211/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 02211: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4297 - val_loss: 1.4831\n",
      "Epoch 2212/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 02212: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4252 - val_loss: 1.4879\n",
      "Epoch 2213/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02213: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4875\n",
      "Epoch 2214/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4257\n",
      "Epoch 02214: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4257 - val_loss: 1.4946\n",
      "Epoch 2215/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4280\n",
      "Epoch 02215: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4280 - val_loss: 1.4859\n",
      "Epoch 2216/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 02216: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4260 - val_loss: 1.4840\n",
      "Epoch 2217/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02217: loss did not improve from 1.42121\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4244 - val_loss: 1.4842\n",
      "Epoch 2218/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02218: loss improved from 1.42121 to 1.42119, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4212 - val_loss: 1.4832\n",
      "Epoch 2219/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4242\n",
      "Epoch 02219: loss did not improve from 1.42119\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4242 - val_loss: 1.4843\n",
      "Epoch 2220/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02220: loss did not improve from 1.42119\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4228 - val_loss: 1.4861\n",
      "Epoch 2221/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02221: loss did not improve from 1.42119\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4223 - val_loss: 1.4866\n",
      "Epoch 2222/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02222: loss did not improve from 1.42119\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4866\n",
      "Epoch 2223/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02223: loss did not improve from 1.42119\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4228 - val_loss: 1.4884\n",
      "Epoch 2224/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02224: loss did not improve from 1.42119\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4240 - val_loss: 1.4868\n",
      "Epoch 2225/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4240\n",
      "Epoch 02225: loss did not improve from 1.42119\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4240 - val_loss: 1.4829\n",
      "Epoch 2226/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02226: loss did not improve from 1.42119\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4834\n",
      "Epoch 2227/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02227: loss improved from 1.42119 to 1.42082, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4208 - val_loss: 1.4830\n",
      "Epoch 2228/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02228: loss did not improve from 1.42082\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4212 - val_loss: 1.4844\n",
      "Epoch 2229/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02229: loss did not improve from 1.42082\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4212 - val_loss: 1.4852\n",
      "Epoch 2230/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02230: loss improved from 1.42082 to 1.42073, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4207 - val_loss: 1.4877\n",
      "Epoch 2231/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02231: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4878\n",
      "Epoch 2232/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4237\n",
      "Epoch 02232: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4237 - val_loss: 1.4898\n",
      "Epoch 2233/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02233: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4244 - val_loss: 1.4848\n",
      "Epoch 2234/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4227\n",
      "Epoch 02234: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4227 - val_loss: 1.4872\n",
      "Epoch 2235/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02235: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4846\n",
      "Epoch 2236/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02236: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4236 - val_loss: 1.4836\n",
      "Epoch 2237/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02237: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4235 - val_loss: 1.4844\n",
      "Epoch 2238/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02238: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4216 - val_loss: 1.4851\n",
      "Epoch 2239/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02239: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4239 - val_loss: 1.4890\n",
      "Epoch 2240/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 02240: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4919\n",
      "Epoch 2241/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4252\n",
      "Epoch 02241: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4252 - val_loss: 1.4852\n",
      "Epoch 2242/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02242: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4218 - val_loss: 1.4856\n",
      "Epoch 2243/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4232\n",
      "Epoch 02243: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4232 - val_loss: 1.4875\n",
      "Epoch 2244/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02244: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4236 - val_loss: 1.4888\n",
      "Epoch 2245/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4129\n",
      "Epoch 02245: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4242 - val_loss: 1.4858\n",
      "Epoch 2246/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02246: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4218 - val_loss: 1.4827\n",
      "Epoch 2247/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02247: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4225 - val_loss: 1.4867\n",
      "Epoch 2248/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02248: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4236 - val_loss: 1.4841\n",
      "Epoch 2249/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4131\n",
      "Epoch 02249: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4223 - val_loss: 1.4837\n",
      "Epoch 2250/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02250: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4218 - val_loss: 1.4887\n",
      "Epoch 2251/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4241\n",
      "Epoch 02251: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4248 - val_loss: 1.4871\n",
      "Epoch 2252/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02252: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4226 - val_loss: 1.4873\n",
      "Epoch 2253/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02253: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4212 - val_loss: 1.4929\n",
      "Epoch 2254/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4280\n",
      "Epoch 02254: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4274 - val_loss: 1.4893\n",
      "Epoch 2255/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02255: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4250 - val_loss: 1.4884\n",
      "Epoch 2256/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4219\n",
      "Epoch 02256: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4219 - val_loss: 1.4870\n",
      "Epoch 2257/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02257: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4235 - val_loss: 1.4855\n",
      "Epoch 2258/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4227\n",
      "Epoch 02258: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4227 - val_loss: 1.4847\n",
      "Epoch 2259/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02259: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4234 - val_loss: 1.4854\n",
      "Epoch 2260/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02260: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4218 - val_loss: 1.4858\n",
      "Epoch 2261/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02261: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4851\n",
      "Epoch 2262/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02262: loss did not improve from 1.42073\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4211 - val_loss: 1.4817\n",
      "Epoch 2263/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4302\n",
      "Epoch 02263: loss improved from 1.42073 to 1.42049, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4205 - val_loss: 1.4849\n",
      "Epoch 2264/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4112\n",
      "Epoch 02264: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4223 - val_loss: 1.4871\n",
      "Epoch 2265/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02265: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4233 - val_loss: 1.4890\n",
      "Epoch 2266/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02266: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4210 - val_loss: 1.4841\n",
      "Epoch 2267/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 02267: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4243 - val_loss: 1.4863\n",
      "Epoch 2268/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02268: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4238 - val_loss: 1.4836\n",
      "Epoch 2269/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02269: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4245 - val_loss: 1.4886\n",
      "Epoch 2270/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 02270: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4272 - val_loss: 1.4855\n",
      "Epoch 2271/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02271: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4251 - val_loss: 1.4890\n",
      "Epoch 2272/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02272: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4229 - val_loss: 1.4838\n",
      "Epoch 2273/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02273: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4209 - val_loss: 1.4873\n",
      "Epoch 2274/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02274: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4239 - val_loss: 1.4846\n",
      "Epoch 2275/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02275: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4208 - val_loss: 1.4870\n",
      "Epoch 2276/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02276: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4222 - val_loss: 1.4840\n",
      "Epoch 2277/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4333\n",
      "Epoch 02277: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4333 - val_loss: 1.4854\n",
      "Epoch 2278/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 02278: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4317 - val_loss: 1.4829\n",
      "Epoch 2279/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 02279: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4269 - val_loss: 1.4868\n",
      "Epoch 2280/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02280: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4251 - val_loss: 1.4887\n",
      "Epoch 2281/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02281: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4234 - val_loss: 1.4891\n",
      "Epoch 2282/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02282: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4244 - val_loss: 1.4860\n",
      "Epoch 2283/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02283: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4848\n",
      "Epoch 2284/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4248\n",
      "Epoch 02284: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4248 - val_loss: 1.4831\n",
      "Epoch 2285/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4231\n",
      "Epoch 02285: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4231 - val_loss: 1.4859\n",
      "Epoch 2286/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02286: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4228 - val_loss: 1.4871\n",
      "Epoch 2287/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02287: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4244 - val_loss: 1.4846\n",
      "Epoch 2288/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4230\n",
      "Epoch 02288: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4230 - val_loss: 1.4919\n",
      "Epoch 2289/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4237\n",
      "Epoch 02289: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4237 - val_loss: 1.4839\n",
      "Epoch 2290/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02290: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4852\n",
      "Epoch 2291/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02291: loss did not improve from 1.42049\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4222 - val_loss: 1.4859\n",
      "Epoch 2292/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02292: loss improved from 1.42049 to 1.42043, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4204 - val_loss: 1.4865\n",
      "Epoch 2293/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02293: loss improved from 1.42043 to 1.41919, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4192 - val_loss: 1.4841\n",
      "Epoch 2294/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02294: loss improved from 1.41919 to 1.41903, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4190 - val_loss: 1.4853\n",
      "Epoch 2295/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02295: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4202 - val_loss: 1.4865\n",
      "Epoch 2296/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02296: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4818\n",
      "Epoch 2297/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4369\n",
      "Epoch 02297: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4369 - val_loss: 1.4850\n",
      "Epoch 2298/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 02298: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4374 - val_loss: 1.4926\n",
      "Epoch 2299/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4316\n",
      "Epoch 02299: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4316 - val_loss: 1.4869\n",
      "Epoch 2300/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02300: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4256 - val_loss: 1.4844\n",
      "Epoch 2301/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4254\n",
      "Epoch 02301: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4254 - val_loss: 1.4841\n",
      "Epoch 2302/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02302: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4226 - val_loss: 1.4869\n",
      "Epoch 2303/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02303: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4238 - val_loss: 1.4881\n",
      "Epoch 2304/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02304: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4250 - val_loss: 1.4849\n",
      "Epoch 2305/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02305: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4234 - val_loss: 1.4833\n",
      "Epoch 2306/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02306: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4258 - val_loss: 1.4879\n",
      "Epoch 2307/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02307: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4246 - val_loss: 1.4928\n",
      "Epoch 2308/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02308: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4253 - val_loss: 1.4912\n",
      "Epoch 2309/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4249\n",
      "Epoch 02309: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4249 - val_loss: 1.4870\n",
      "Epoch 2310/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02310: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4245 - val_loss: 1.4847\n",
      "Epoch 2311/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4272\n",
      "Epoch 02311: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4272 - val_loss: 1.4879\n",
      "Epoch 2312/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02312: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4245 - val_loss: 1.4905\n",
      "Epoch 2313/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02313: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4226 - val_loss: 1.4870\n",
      "Epoch 2314/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02314: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4874\n",
      "Epoch 2315/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02315: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4873\n",
      "Epoch 2316/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02316: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4225 - val_loss: 1.4862\n",
      "Epoch 2317/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02317: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4916\n",
      "Epoch 2318/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02318: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4251 - val_loss: 1.4870\n",
      "Epoch 2319/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02319: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4221 - val_loss: 1.4868\n",
      "Epoch 2320/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4270\n",
      "Epoch 02320: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4270 - val_loss: 1.4851\n",
      "Epoch 2321/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4258\n",
      "Epoch 02321: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4258 - val_loss: 1.4876\n",
      "Epoch 2322/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02322: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4228 - val_loss: 1.4863\n",
      "Epoch 2323/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02323: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4879\n",
      "Epoch 2324/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4227\n",
      "Epoch 02324: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4227 - val_loss: 1.4850\n",
      "Epoch 2325/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02325: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4872\n",
      "Epoch 2326/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02326: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4207 - val_loss: 1.4882\n",
      "Epoch 2327/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02327: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4201 - val_loss: 1.4836\n",
      "Epoch 2328/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02328: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4200 - val_loss: 1.4870\n",
      "Epoch 2329/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02329: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4200 - val_loss: 1.4834\n",
      "Epoch 2330/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02330: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4200 - val_loss: 1.4833\n",
      "Epoch 2331/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02331: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4207 - val_loss: 1.4840\n",
      "Epoch 2332/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02332: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4220 - val_loss: 1.4857\n",
      "Epoch 2333/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02333: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4222 - val_loss: 1.4831\n",
      "Epoch 2334/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02334: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4238 - val_loss: 1.4851\n",
      "Epoch 2335/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02335: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4224 - val_loss: 1.4866\n",
      "Epoch 2336/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02336: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4199 - val_loss: 1.4867\n",
      "Epoch 2337/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02337: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4196 - val_loss: 1.4874\n",
      "Epoch 2338/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02338: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4195 - val_loss: 1.4870\n",
      "Epoch 2339/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02339: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4191 - val_loss: 1.4878\n",
      "Epoch 2340/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4234\n",
      "Epoch 02340: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4234 - val_loss: 1.4914\n",
      "Epoch 2341/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4264\n",
      "Epoch 02341: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4264 - val_loss: 1.4894\n",
      "Epoch 2342/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4251\n",
      "Epoch 02342: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4251 - val_loss: 1.4928\n",
      "Epoch 2343/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 02343: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4260 - val_loss: 1.4908\n",
      "Epoch 2344/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4305\n",
      "Epoch 02344: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4305 - val_loss: 1.4880\n",
      "Epoch 2345/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02345: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4239 - val_loss: 1.4881\n",
      "Epoch 2346/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02346: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4221 - val_loss: 1.4874\n",
      "Epoch 2347/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02347: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4220 - val_loss: 1.4877\n",
      "Epoch 2348/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02348: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4909\n",
      "Epoch 2349/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02349: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4212 - val_loss: 1.4882\n",
      "Epoch 2350/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02350: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4247 - val_loss: 1.4839\n",
      "Epoch 2351/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02351: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4214 - val_loss: 1.4859\n",
      "Epoch 2352/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02352: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4211 - val_loss: 1.4864\n",
      "Epoch 2353/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02353: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4216 - val_loss: 1.4880\n",
      "Epoch 2354/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02354: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4215 - val_loss: 1.4921\n",
      "Epoch 2355/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02355: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4873\n",
      "Epoch 2356/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02356: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4209 - val_loss: 1.4882\n",
      "Epoch 2357/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02357: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4238 - val_loss: 1.4877\n",
      "Epoch 2358/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02358: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4222 - val_loss: 1.4898\n",
      "Epoch 2359/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02359: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4226 - val_loss: 1.4862\n",
      "Epoch 2360/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02360: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4229 - val_loss: 1.4860\n",
      "Epoch 2361/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02361: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4211 - val_loss: 1.4873\n",
      "Epoch 2362/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02362: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4212 - val_loss: 1.4854\n",
      "Epoch 2363/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4219\n",
      "Epoch 02363: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4219 - val_loss: 1.4867\n",
      "Epoch 2364/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02364: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4208 - val_loss: 1.4915\n",
      "Epoch 2365/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4230\n",
      "Epoch 02365: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4230 - val_loss: 1.4871\n",
      "Epoch 2366/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02366: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4199 - val_loss: 1.4876\n",
      "Epoch 2367/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02367: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4200 - val_loss: 1.4856\n",
      "Epoch 2368/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02368: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4196 - val_loss: 1.4877\n",
      "Epoch 2369/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02369: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4853\n",
      "Epoch 2370/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02370: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4229 - val_loss: 1.4873\n",
      "Epoch 2371/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02371: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4236 - val_loss: 1.4853\n",
      "Epoch 2372/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02372: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4197 - val_loss: 1.4857\n",
      "Epoch 2373/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02373: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4197 - val_loss: 1.4864\n",
      "Epoch 2374/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02374: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4881\n",
      "Epoch 2375/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02375: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4235 - val_loss: 1.4878\n",
      "Epoch 2376/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02376: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4211 - val_loss: 1.4889\n",
      "Epoch 2377/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02377: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4197 - val_loss: 1.4865\n",
      "Epoch 2378/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02378: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4875\n",
      "Epoch 2379/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02379: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4872\n",
      "Epoch 2380/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4227\n",
      "Epoch 02380: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4227 - val_loss: 1.4897\n",
      "Epoch 2381/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02381: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4228 - val_loss: 1.4853\n",
      "Epoch 2382/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02382: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4209 - val_loss: 1.4869\n",
      "Epoch 2383/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02383: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4203 - val_loss: 1.4911\n",
      "Epoch 2384/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02384: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4222 - val_loss: 1.4872\n",
      "Epoch 2385/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4229\n",
      "Epoch 02385: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4229 - val_loss: 1.4895\n",
      "Epoch 2386/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4255\n",
      "Epoch 02386: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4255 - val_loss: 1.4867\n",
      "Epoch 2387/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4219\n",
      "Epoch 02387: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4219 - val_loss: 1.4847\n",
      "Epoch 2388/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02388: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4196 - val_loss: 1.4894\n",
      "Epoch 2389/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02389: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4875\n",
      "Epoch 2390/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02390: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4220 - val_loss: 1.4858\n",
      "Epoch 2391/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4389\n",
      "Epoch 02391: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4389 - val_loss: 1.4897\n",
      "Epoch 2392/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4337\n",
      "Epoch 02392: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4337 - val_loss: 1.4879\n",
      "Epoch 2393/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4293\n",
      "Epoch 02393: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4293 - val_loss: 1.4868\n",
      "Epoch 2394/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4253\n",
      "Epoch 02394: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4253 - val_loss: 1.4903\n",
      "Epoch 2395/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4263\n",
      "Epoch 02395: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4263 - val_loss: 1.4861\n",
      "Epoch 2396/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02396: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4224 - val_loss: 1.4928\n",
      "Epoch 2397/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02397: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4236 - val_loss: 1.4901\n",
      "Epoch 2398/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02398: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4228 - val_loss: 1.4903\n",
      "Epoch 2399/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02399: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4216 - val_loss: 1.4870\n",
      "Epoch 2400/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02400: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4211 - val_loss: 1.4862\n",
      "Epoch 2401/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02401: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4202 - val_loss: 1.4842\n",
      "Epoch 2402/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02402: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4205 - val_loss: 1.4912\n",
      "Epoch 2403/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02403: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4250 - val_loss: 1.4873\n",
      "Epoch 2404/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02404: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4866\n",
      "Epoch 2405/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02405: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4869\n",
      "Epoch 2406/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02406: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4205 - val_loss: 1.4879\n",
      "Epoch 2407/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02407: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4202 - val_loss: 1.4864\n",
      "Epoch 2408/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02408: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4222 - val_loss: 1.4883\n",
      "Epoch 2409/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4139\n",
      "Epoch 02409: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4215 - val_loss: 1.4869\n",
      "Epoch 2410/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02410: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4201 - val_loss: 1.4880\n",
      "Epoch 2411/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02411: loss did not improve from 1.41903\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - val_loss: 1.4867\n",
      "Epoch 2412/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02412: loss improved from 1.41903 to 1.41885, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4188 - val_loss: 1.4853\n",
      "Epoch 2413/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02413: loss improved from 1.41885 to 1.41877, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4188 - val_loss: 1.4865\n",
      "Epoch 2414/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4134\n",
      "Epoch 02414: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4200 - val_loss: 1.4900\n",
      "Epoch 2415/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02415: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4233 - val_loss: 1.4878\n",
      "Epoch 2416/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4260\n",
      "Epoch 02416: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4198 - val_loss: 1.4845\n",
      "Epoch 2417/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02417: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4194 - val_loss: 1.4868\n",
      "Epoch 2418/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02418: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4200 - val_loss: 1.4914\n",
      "Epoch 2419/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02419: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4226 - val_loss: 1.4899\n",
      "Epoch 2420/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02420: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4870\n",
      "Epoch 2421/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02421: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4194 - val_loss: 1.4882\n",
      "Epoch 2422/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02422: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4931\n",
      "Epoch 2423/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02423: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4222 - val_loss: 1.4865\n",
      "Epoch 2424/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4127\n",
      "Epoch 02424: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4204 - val_loss: 1.4856\n",
      "Epoch 2425/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02425: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4196 - val_loss: 1.4857\n",
      "Epoch 2426/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4206\n",
      "Epoch 02426: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4206 - val_loss: 1.4893\n",
      "Epoch 2427/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4261\n",
      "Epoch 02427: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4261 - val_loss: 1.4879\n",
      "Epoch 2428/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02428: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4908\n",
      "Epoch 2429/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02429: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4214 - val_loss: 1.4869\n",
      "Epoch 2430/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02430: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4213 - val_loss: 1.4856\n",
      "Epoch 2431/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02431: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4193 - val_loss: 1.4894\n",
      "Epoch 2432/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02432: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4200 - val_loss: 1.4895\n",
      "Epoch 2433/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02433: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4203 - val_loss: 1.4885\n",
      "Epoch 2434/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02434: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4886\n",
      "Epoch 2435/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4206\n",
      "Epoch 02435: loss did not improve from 1.41877\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4206 - val_loss: 1.4857\n",
      "Epoch 2436/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02436: loss improved from 1.41877 to 1.41831, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4183 - val_loss: 1.4890\n",
      "Epoch 2437/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02437: loss did not improve from 1.41831\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4207 - val_loss: 1.4902\n",
      "Epoch 2438/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02438: loss did not improve from 1.41831\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4206 - val_loss: 1.4888\n",
      "Epoch 2439/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02439: loss did not improve from 1.41831\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4201 - val_loss: 1.4884\n",
      "Epoch 2440/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02440: loss did not improve from 1.41831\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4192 - val_loss: 1.4877\n",
      "Epoch 2441/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02441: loss improved from 1.41831 to 1.41806, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 1.4181 - val_loss: 1.4894\n",
      "Epoch 2442/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02442: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4198 - val_loss: 1.4897\n",
      "Epoch 2443/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02443: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4197 - val_loss: 1.4870\n",
      "Epoch 2444/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4129\n",
      "Epoch 02444: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4191 - val_loss: 1.4869\n",
      "Epoch 2445/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02445: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4208 - val_loss: 1.4939\n",
      "Epoch 2446/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4266\n",
      "Epoch 02446: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4234 - val_loss: 1.4921\n",
      "Epoch 2447/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 02447: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4243 - val_loss: 1.4916\n",
      "Epoch 2448/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02448: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4244 - val_loss: 1.4880\n",
      "Epoch 2449/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02449: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4205 - val_loss: 1.4873\n",
      "Epoch 2450/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4206\n",
      "Epoch 02450: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4206 - val_loss: 1.4898\n",
      "Epoch 2451/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02451: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4211 - val_loss: 1.4901\n",
      "Epoch 2452/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4222\n",
      "Epoch 02452: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4222 - val_loss: 1.4865\n",
      "Epoch 2453/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02453: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4224 - val_loss: 1.4869\n",
      "Epoch 2454/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02454: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4221 - val_loss: 1.4865\n",
      "Epoch 2455/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02455: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4201 - val_loss: 1.4866\n",
      "Epoch 2456/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02456: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4865\n",
      "Epoch 2457/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02457: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4212 - val_loss: 1.4907\n",
      "Epoch 2458/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02458: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4214 - val_loss: 1.4887\n",
      "Epoch 2459/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02459: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4202 - val_loss: 1.4877\n",
      "Epoch 2460/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02460: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4831\n",
      "Epoch 2461/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02461: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4191 - val_loss: 1.4873\n",
      "Epoch 2462/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02462: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4196 - val_loss: 1.4883\n",
      "Epoch 2463/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4219\n",
      "Epoch 02463: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4219 - val_loss: 1.4874\n",
      "Epoch 2464/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02464: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4213 - val_loss: 1.4873\n",
      "Epoch 2465/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02465: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4872\n",
      "Epoch 2466/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02466: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4192 - val_loss: 1.4856\n",
      "Epoch 2467/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02467: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4181 - val_loss: 1.4902\n",
      "Epoch 2468/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02468: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4872\n",
      "Epoch 2469/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02469: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4198 - val_loss: 1.4884\n",
      "Epoch 2470/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4212\n",
      "Epoch 02470: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4212 - val_loss: 1.4866\n",
      "Epoch 2471/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02471: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4197 - val_loss: 1.4917\n",
      "Epoch 2472/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02472: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4223 - val_loss: 1.4850\n",
      "Epoch 2473/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02473: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4211 - val_loss: 1.4855\n",
      "Epoch 2474/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02474: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4215 - val_loss: 1.4851\n",
      "Epoch 2475/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02475: loss did not improve from 1.41806\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4194 - val_loss: 1.4879\n",
      "Epoch 2476/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02476: loss improved from 1.41806 to 1.41648, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4165 - val_loss: 1.4855\n",
      "Epoch 2477/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02477: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4168 - val_loss: 1.4861\n",
      "Epoch 2478/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02478: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4189 - val_loss: 1.4877\n",
      "Epoch 2479/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02479: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4188 - val_loss: 1.4858\n",
      "Epoch 2480/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02480: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4192 - val_loss: 1.4903\n",
      "Epoch 2481/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02481: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4182 - val_loss: 1.4914\n",
      "Epoch 2482/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02482: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4868\n",
      "Epoch 2483/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02483: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4184 - val_loss: 1.4854\n",
      "Epoch 2484/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02484: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4188 - val_loss: 1.4872\n",
      "Epoch 2485/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02485: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4189 - val_loss: 1.4880\n",
      "Epoch 2486/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02486: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4186 - val_loss: 1.4886\n",
      "Epoch 2487/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02487: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4878\n",
      "Epoch 2488/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02488: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4207 - val_loss: 1.4911\n",
      "Epoch 2489/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4230\n",
      "Epoch 02489: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4230 - val_loss: 1.4943\n",
      "Epoch 2490/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4239\n",
      "Epoch 02490: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4239 - val_loss: 1.4910\n",
      "Epoch 2491/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02491: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4886\n",
      "Epoch 2492/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02492: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4226 - val_loss: 1.4873\n",
      "Epoch 2493/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02493: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4195 - val_loss: 1.4869\n",
      "Epoch 2494/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02494: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4193 - val_loss: 1.4869\n",
      "Epoch 2495/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02495: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4852\n",
      "Epoch 2496/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02496: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4176 - val_loss: 1.4831\n",
      "Epoch 2497/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4171\n",
      "Epoch 02497: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4171 - val_loss: 1.4868\n",
      "Epoch 2498/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02498: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4919\n",
      "Epoch 2499/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02499: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4201 - val_loss: 1.4891\n",
      "Epoch 2500/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02500: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4200 - val_loss: 1.4910\n",
      "Epoch 2501/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02501: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4217 - val_loss: 1.4886\n",
      "Epoch 2502/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02502: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4190 - val_loss: 1.4917\n",
      "Epoch 2503/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02503: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4202 - val_loss: 1.4867\n",
      "Epoch 2504/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02504: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4885\n",
      "Epoch 2505/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02505: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4188 - val_loss: 1.4868\n",
      "Epoch 2506/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4227\n",
      "Epoch 02506: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4227 - val_loss: 1.4886\n",
      "Epoch 2507/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02507: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4213 - val_loss: 1.4898\n",
      "Epoch 2508/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02508: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4235 - val_loss: 1.4880\n",
      "Epoch 2509/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02509: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4208 - val_loss: 1.4907\n",
      "Epoch 2510/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02510: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4914\n",
      "Epoch 2511/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02511: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4878\n",
      "Epoch 2512/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02512: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4203 - val_loss: 1.4875\n",
      "Epoch 2513/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02513: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4197 - val_loss: 1.4912\n",
      "Epoch 2514/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02514: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4199 - val_loss: 1.4908\n",
      "Epoch 2515/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02515: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4196 - val_loss: 1.4927\n",
      "Epoch 2516/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02516: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4182 - val_loss: 1.4878\n",
      "Epoch 2517/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02517: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4186 - val_loss: 1.4889\n",
      "Epoch 2518/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02518: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4208 - val_loss: 1.4881\n",
      "Epoch 2519/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02519: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4194 - val_loss: 1.4846\n",
      "Epoch 2520/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02520: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4188 - val_loss: 1.4954\n",
      "Epoch 2521/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4230\n",
      "Epoch 02521: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4230 - val_loss: 1.4881\n",
      "Epoch 2522/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4185\n",
      "Epoch 02522: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4185 - val_loss: 1.4878\n",
      "Epoch 2523/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02523: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4175 - val_loss: 1.4910\n",
      "Epoch 2524/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02524: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4198 - val_loss: 1.4884\n",
      "Epoch 2525/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02525: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4216 - val_loss: 1.4904\n",
      "Epoch 2526/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02526: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4213 - val_loss: 1.4896\n",
      "Epoch 2527/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02527: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4196 - val_loss: 1.4875\n",
      "Epoch 2528/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02528: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4839\n",
      "Epoch 2529/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02529: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4208 - val_loss: 1.4896\n",
      "Epoch 2530/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02530: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4916\n",
      "Epoch 2531/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02531: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4195 - val_loss: 1.4884\n",
      "Epoch 2532/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02532: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4191 - val_loss: 1.4898\n",
      "Epoch 2533/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02533: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4200 - val_loss: 1.4899\n",
      "Epoch 2534/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02534: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4203 - val_loss: 1.4899\n",
      "Epoch 2535/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02535: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4182 - val_loss: 1.4853\n",
      "Epoch 2536/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02536: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4168 - val_loss: 1.4865\n",
      "Epoch 2537/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02537: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4168 - val_loss: 1.4851\n",
      "Epoch 2538/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02538: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4910\n",
      "Epoch 2539/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02539: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4915\n",
      "Epoch 2540/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4194\n",
      "Epoch 02540: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4194 - val_loss: 1.4916\n",
      "Epoch 2541/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02541: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4220 - val_loss: 1.4886\n",
      "Epoch 2542/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02542: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4884\n",
      "Epoch 2543/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02543: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4216 - val_loss: 1.4854\n",
      "Epoch 2544/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4267\n",
      "Epoch 02544: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4195 - val_loss: 1.4854\n",
      "Epoch 2545/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4368\n",
      "Epoch 02545: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4368 - val_loss: 1.4859\n",
      "Epoch 2546/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4317\n",
      "Epoch 02546: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4317 - val_loss: 1.4883\n",
      "Epoch 2547/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4271\n",
      "Epoch 02547: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4271 - val_loss: 1.4895\n",
      "Epoch 2548/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 02548: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4243 - val_loss: 1.4862\n",
      "Epoch 2549/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02549: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4853\n",
      "Epoch 2550/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02550: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4217 - val_loss: 1.4871\n",
      "Epoch 2551/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02551: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4199 - val_loss: 1.4883\n",
      "Epoch 2552/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02552: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4872\n",
      "Epoch 2553/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02553: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4198 - val_loss: 1.4862\n",
      "Epoch 2554/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02554: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4198 - val_loss: 1.4862\n",
      "Epoch 2555/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02555: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4863\n",
      "Epoch 2556/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02556: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4192 - val_loss: 1.4895\n",
      "Epoch 2557/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4217\n",
      "Epoch 02557: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4217 - val_loss: 1.4903\n",
      "Epoch 2558/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4209\n",
      "Epoch 02558: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4209 - val_loss: 1.4930\n",
      "Epoch 2559/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4228\n",
      "Epoch 02559: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4228 - val_loss: 1.4880\n",
      "Epoch 2560/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02560: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4215 - val_loss: 1.4895\n",
      "Epoch 2561/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02561: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4204 - val_loss: 1.4875\n",
      "Epoch 2562/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4206\n",
      "Epoch 02562: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4206 - val_loss: 1.4864\n",
      "Epoch 2563/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02563: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4844\n",
      "Epoch 2564/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02564: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4180 - val_loss: 1.4895\n",
      "Epoch 2565/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02565: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4191 - val_loss: 1.4903\n",
      "Epoch 2566/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02566: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4172 - val_loss: 1.4855\n",
      "Epoch 2567/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4316\n",
      "Epoch 02567: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4316 - val_loss: 1.4852\n",
      "Epoch 2568/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4294\n",
      "Epoch 02568: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4294 - val_loss: 1.4858\n",
      "Epoch 2569/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4250\n",
      "Epoch 02569: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4250 - val_loss: 1.4867\n",
      "Epoch 2570/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4218\n",
      "Epoch 02570: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4218 - val_loss: 1.4870\n",
      "Epoch 2571/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02571: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4210 - val_loss: 1.4876\n",
      "Epoch 2572/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02572: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4191 - val_loss: 1.4896\n",
      "Epoch 2573/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02573: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4211 - val_loss: 1.4886\n",
      "Epoch 2574/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02574: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4226 - val_loss: 1.4899\n",
      "Epoch 2575/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02575: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4860\n",
      "Epoch 2576/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02576: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4211 - val_loss: 1.4918\n",
      "Epoch 2577/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02577: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4216 - val_loss: 1.4918\n",
      "Epoch 2578/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02578: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4235 - val_loss: 1.4966\n",
      "Epoch 2579/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4246\n",
      "Epoch 02579: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4246 - val_loss: 1.4878\n",
      "Epoch 2580/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02580: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4224 - val_loss: 1.4892\n",
      "Epoch 2581/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4226\n",
      "Epoch 02581: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4226 - val_loss: 1.4872\n",
      "Epoch 2582/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4177\n",
      "Epoch 02582: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4177 - val_loss: 1.4879\n",
      "Epoch 2583/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4219\n",
      "Epoch 02583: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4219 - val_loss: 1.4885\n",
      "Epoch 2584/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02584: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4184 - val_loss: 1.4879\n",
      "Epoch 2585/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02585: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4181 - val_loss: 1.4876\n",
      "Epoch 2586/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02586: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4172 - val_loss: 1.4937\n",
      "Epoch 2587/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4236\n",
      "Epoch 02587: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4236 - val_loss: 1.4925\n",
      "Epoch 2588/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4244\n",
      "Epoch 02588: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4244 - val_loss: 1.4911\n",
      "Epoch 2589/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 02589: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4210 - val_loss: 1.4900\n",
      "Epoch 2590/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02590: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4172 - val_loss: 1.4868\n",
      "Epoch 2591/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02591: loss did not improve from 1.41648\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4165 - val_loss: 1.4866\n",
      "Epoch 2592/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4158\n",
      "Epoch 02592: loss improved from 1.41648 to 1.41576, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4158 - val_loss: 1.4865\n",
      "Epoch 2593/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02593: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4175 - val_loss: 1.4864\n",
      "Epoch 2594/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02594: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4172 - val_loss: 1.4860\n",
      "Epoch 2595/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02595: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4165 - val_loss: 1.4886\n",
      "Epoch 2596/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02596: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4204 - val_loss: 1.4883\n",
      "Epoch 2597/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02597: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4200 - val_loss: 1.4893\n",
      "Epoch 2598/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02598: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4188 - val_loss: 1.4894\n",
      "Epoch 2599/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4260\n",
      "Epoch 02599: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4260 - val_loss: 1.4878\n",
      "Epoch 2600/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02600: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4878\n",
      "Epoch 2601/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02601: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4204 - val_loss: 1.4878\n",
      "Epoch 2602/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02602: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4188 - val_loss: 1.4892\n",
      "Epoch 2603/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4187\n",
      "Epoch 02603: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4187 - val_loss: 1.4895\n",
      "Epoch 2604/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02604: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4878\n",
      "Epoch 2605/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02605: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4165 - val_loss: 1.4875\n",
      "Epoch 2606/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02606: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4167 - val_loss: 1.4887\n",
      "Epoch 2607/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4276\n",
      "Epoch 02607: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4276 - val_loss: 1.4913\n",
      "Epoch 2608/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02608: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4892\n",
      "Epoch 2609/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02609: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4886\n",
      "Epoch 2610/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02610: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4208 - val_loss: 1.4876\n",
      "Epoch 2611/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02611: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4186 - val_loss: 1.4888\n",
      "Epoch 2612/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02612: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4179 - val_loss: 1.4898\n",
      "Epoch 2613/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02613: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4895\n",
      "Epoch 2614/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02614: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4216 - val_loss: 1.4904\n",
      "Epoch 2615/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02615: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4199 - val_loss: 1.4869\n",
      "Epoch 2616/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02616: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4874\n",
      "Epoch 2617/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02617: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4168 - val_loss: 1.4899\n",
      "Epoch 2618/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02618: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4175 - val_loss: 1.4890\n",
      "Epoch 2619/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02619: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4173 - val_loss: 1.4920\n",
      "Epoch 2620/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4162\n",
      "Epoch 02620: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4162 - val_loss: 1.4895\n",
      "Epoch 2621/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02621: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4167 - val_loss: 1.4907\n",
      "Epoch 2622/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02622: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4203 - val_loss: 1.4909\n",
      "Epoch 2623/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02623: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4905\n",
      "Epoch 2624/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02624: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4197 - val_loss: 1.4887\n",
      "Epoch 2625/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02625: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4888\n",
      "Epoch 2626/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02626: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4181 - val_loss: 1.4866\n",
      "Epoch 2627/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02627: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4179 - val_loss: 1.4906\n",
      "Epoch 2628/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02628: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4190 - val_loss: 1.4887\n",
      "Epoch 2629/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.3996\n",
      "Epoch 02629: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4179 - val_loss: 1.4910\n",
      "Epoch 2630/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02630: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4167 - val_loss: 1.4908\n",
      "Epoch 2631/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02631: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4167 - val_loss: 1.4898\n",
      "Epoch 2632/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4265\n",
      "Epoch 02632: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4175 - val_loss: 1.4919\n",
      "Epoch 2633/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02633: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4181 - val_loss: 1.4928\n",
      "Epoch 2634/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02634: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4179 - val_loss: 1.4901\n",
      "Epoch 2635/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02635: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4198 - val_loss: 1.4884\n",
      "Epoch 2636/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02636: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4188 - val_loss: 1.4887\n",
      "Epoch 2637/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02637: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4168 - val_loss: 1.4908\n",
      "Epoch 2638/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02638: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4896\n",
      "Epoch 2639/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02639: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4181 - val_loss: 1.4893\n",
      "Epoch 2640/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02640: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4167 - val_loss: 1.4890\n",
      "Epoch 2641/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02641: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4182 - val_loss: 1.4880\n",
      "Epoch 2642/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02642: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4167 - val_loss: 1.4864\n",
      "Epoch 2643/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02643: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4181 - val_loss: 1.4876\n",
      "Epoch 2644/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02644: loss did not improve from 1.41576\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4159 - val_loss: 1.4881\n",
      "Epoch 2645/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4144\n",
      "Epoch 02645: loss improved from 1.41576 to 1.41443, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4144 - val_loss: 1.4905\n",
      "Epoch 2646/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02646: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4906\n",
      "Epoch 2647/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02647: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4184 - val_loss: 1.4912\n",
      "Epoch 2648/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4180\n",
      "Epoch 02648: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4180 - val_loss: 1.4865\n",
      "Epoch 2649/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02649: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4157 - val_loss: 1.4890\n",
      "Epoch 2650/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4158\n",
      "Epoch 02650: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4158 - val_loss: 1.4916\n",
      "Epoch 2651/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02651: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4192 - val_loss: 1.4943\n",
      "Epoch 2652/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4196\n",
      "Epoch 02652: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4196 - val_loss: 1.4917\n",
      "Epoch 2653/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02653: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4178 - val_loss: 1.4949\n",
      "Epoch 2654/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02654: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4188 - val_loss: 1.4912\n",
      "Epoch 2655/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02655: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4167 - val_loss: 1.4896\n",
      "Epoch 2656/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02656: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4901\n",
      "Epoch 2657/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4230\n",
      "Epoch 02657: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4230 - val_loss: 1.4880\n",
      "Epoch 2658/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 02658: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4247 - val_loss: 1.4949\n",
      "Epoch 2659/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02659: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4220 - val_loss: 1.4874\n",
      "Epoch 2660/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02660: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4205 - val_loss: 1.4893\n",
      "Epoch 2661/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02661: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4950\n",
      "Epoch 2662/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4216\n",
      "Epoch 02662: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4216 - val_loss: 1.4867\n",
      "Epoch 2663/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02663: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4859\n",
      "Epoch 2664/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02664: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4178 - val_loss: 1.4892\n",
      "Epoch 2665/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02665: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4180 - val_loss: 1.4934\n",
      "Epoch 2666/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 02666: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4220 - val_loss: 1.4899\n",
      "Epoch 2667/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4225\n",
      "Epoch 02667: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4225 - val_loss: 1.4892\n",
      "Epoch 2668/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 02668: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4215 - val_loss: 1.4881\n",
      "Epoch 2669/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02669: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4178 - val_loss: 1.4904\n",
      "Epoch 2670/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02670: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4169 - val_loss: 1.4930\n",
      "Epoch 2671/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02671: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4205 - val_loss: 1.4883\n",
      "Epoch 2672/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4131\n",
      "Epoch 02672: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4193 - val_loss: 1.4946\n",
      "Epoch 2673/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02673: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4208 - val_loss: 1.4940\n",
      "Epoch 2674/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4185\n",
      "Epoch 02674: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4185 - val_loss: 1.4920\n",
      "Epoch 2675/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02675: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4176 - val_loss: 1.4882\n",
      "Epoch 2676/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02676: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4184 - val_loss: 1.4878\n",
      "Epoch 2677/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02677: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4207 - val_loss: 1.4932\n",
      "Epoch 2678/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02678: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4189 - val_loss: 1.4914\n",
      "Epoch 2679/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02679: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4168 - val_loss: 1.4877\n",
      "Epoch 2680/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02680: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4173 - val_loss: 1.4895\n",
      "Epoch 2681/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02681: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4169 - val_loss: 1.4901\n",
      "Epoch 2682/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02682: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4170 - val_loss: 1.4916\n",
      "Epoch 2683/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4161\n",
      "Epoch 02683: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4161 - val_loss: 1.4914\n",
      "Epoch 2684/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02684: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4190 - val_loss: 1.4887\n",
      "Epoch 2685/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4171\n",
      "Epoch 02685: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4171 - val_loss: 1.4886\n",
      "Epoch 2686/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02686: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4166 - val_loss: 1.4957\n",
      "Epoch 2687/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02687: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4207 - val_loss: 1.4925\n",
      "Epoch 2688/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02688: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4872\n",
      "Epoch 2689/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4161\n",
      "Epoch 02689: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4161 - val_loss: 1.4885\n",
      "Epoch 2690/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02690: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4170 - val_loss: 1.4881\n",
      "Epoch 2691/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4160\n",
      "Epoch 02691: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4160 - val_loss: 1.4883\n",
      "Epoch 2692/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02692: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4164 - val_loss: 1.4874\n",
      "Epoch 2693/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02693: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4170 - val_loss: 1.4888\n",
      "Epoch 2694/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02694: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4166 - val_loss: 1.4867\n",
      "Epoch 2695/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4162\n",
      "Epoch 02695: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4162 - val_loss: 1.4900\n",
      "Epoch 2696/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02696: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4184 - val_loss: 1.4875\n",
      "Epoch 2697/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02697: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4167 - val_loss: 1.4922\n",
      "Epoch 2698/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02698: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4172 - val_loss: 1.4883\n",
      "Epoch 2699/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02699: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4151 - val_loss: 1.4888\n",
      "Epoch 2700/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4174\n",
      "Epoch 02700: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4174 - val_loss: 1.4902\n",
      "Epoch 2701/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02701: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4193 - val_loss: 1.4889\n",
      "Epoch 2702/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02702: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4175 - val_loss: 1.4875\n",
      "Epoch 2703/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4163\n",
      "Epoch 02703: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4163 - val_loss: 1.4900\n",
      "Epoch 2704/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02704: loss did not improve from 1.41443\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4157 - val_loss: 1.4875\n",
      "Epoch 2705/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4130\n",
      "Epoch 02705: loss improved from 1.41443 to 1.41305, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4130 - val_loss: 1.4911\n",
      "Epoch 2706/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02706: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4152 - val_loss: 1.4875\n",
      "Epoch 2707/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02707: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4176 - val_loss: 1.4895\n",
      "Epoch 2708/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02708: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4167 - val_loss: 1.4888\n",
      "Epoch 2709/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02709: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4167 - val_loss: 1.4903\n",
      "Epoch 2710/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4174\n",
      "Epoch 02710: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4174 - val_loss: 1.4926\n",
      "Epoch 2711/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02711: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4199 - val_loss: 1.4915\n",
      "Epoch 2712/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02712: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4890\n",
      "Epoch 2713/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4171\n",
      "Epoch 02713: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4171 - val_loss: 1.4867\n",
      "Epoch 2714/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02714: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4176 - val_loss: 1.4883\n",
      "Epoch 2715/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02715: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4183 - val_loss: 1.4936\n",
      "Epoch 2716/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4198\n",
      "Epoch 02716: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4198 - val_loss: 1.4919\n",
      "Epoch 2717/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02717: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4964\n",
      "Epoch 2718/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02718: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4211 - val_loss: 1.4925\n",
      "Epoch 2719/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02719: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4202 - val_loss: 1.4913\n",
      "Epoch 2720/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02720: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4168 - val_loss: 1.4883\n",
      "Epoch 2721/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4150\n",
      "Epoch 02721: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4150 - val_loss: 1.4899\n",
      "Epoch 2722/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4153\n",
      "Epoch 02722: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4157 - val_loss: 1.4880\n",
      "Epoch 2723/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02723: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4873\n",
      "Epoch 2724/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02724: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4164 - val_loss: 1.4913\n",
      "Epoch 2725/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02725: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4191 - val_loss: 1.4931\n",
      "Epoch 2726/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4223\n",
      "Epoch 02726: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4223 - val_loss: 1.4974\n",
      "Epoch 2727/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4203\n",
      "Epoch 02727: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4203 - val_loss: 1.4899\n",
      "Epoch 2728/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02728: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4200 - val_loss: 1.4910\n",
      "Epoch 2729/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02729: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4165 - val_loss: 1.4919\n",
      "Epoch 2730/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02730: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4166 - val_loss: 1.4894\n",
      "Epoch 2731/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02731: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4149 - val_loss: 1.4920\n",
      "Epoch 2732/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02732: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4182 - val_loss: 1.4900\n",
      "Epoch 2733/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02733: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4152 - val_loss: 1.4923\n",
      "Epoch 2734/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02734: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4157 - val_loss: 1.4912\n",
      "Epoch 2735/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02735: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4147 - val_loss: 1.4904\n",
      "Epoch 2736/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02736: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4188 - val_loss: 1.4934\n",
      "Epoch 2737/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4188\n",
      "Epoch 02737: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4188 - val_loss: 1.4948\n",
      "Epoch 2738/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4207\n",
      "Epoch 02738: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4207 - val_loss: 1.4903\n",
      "Epoch 2739/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02739: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4167 - val_loss: 1.4887\n",
      "Epoch 2740/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02740: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4165 - val_loss: 1.4916\n",
      "Epoch 2741/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02741: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4164 - val_loss: 1.4908\n",
      "Epoch 2742/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02742: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4184 - val_loss: 1.4895\n",
      "Epoch 2743/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02743: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4165 - val_loss: 1.4913\n",
      "Epoch 2744/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02744: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4167 - val_loss: 1.4894\n",
      "Epoch 2745/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4155\n",
      "Epoch 02745: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4155 - val_loss: 1.4925\n",
      "Epoch 2746/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4160\n",
      "Epoch 02746: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4160 - val_loss: 1.4906\n",
      "Epoch 2747/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02747: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4170 - val_loss: 1.4909\n",
      "Epoch 2748/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02748: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4166 - val_loss: 1.4916\n",
      "Epoch 2749/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02749: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4166 - val_loss: 1.4949\n",
      "Epoch 2750/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4185\n",
      "Epoch 02750: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4185 - val_loss: 1.4955\n",
      "Epoch 2751/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02751: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4193 - val_loss: 1.4933\n",
      "Epoch 2752/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02752: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4191 - val_loss: 1.4945\n",
      "Epoch 2753/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02753: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4178 - val_loss: 1.4931\n",
      "Epoch 2754/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02754: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4205 - val_loss: 1.4935\n",
      "Epoch 2755/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4187\n",
      "Epoch 02755: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4187 - val_loss: 1.4894\n",
      "Epoch 2756/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4158\n",
      "Epoch 02756: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4158 - val_loss: 1.4907\n",
      "Epoch 2757/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02757: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4912\n",
      "Epoch 2758/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02758: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4169 - val_loss: 1.4888\n",
      "Epoch 2759/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02759: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4168 - val_loss: 1.4896\n",
      "Epoch 2760/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02760: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4940\n",
      "Epoch 2761/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02761: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4195 - val_loss: 1.4933\n",
      "Epoch 2762/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02762: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4184 - val_loss: 1.4919\n",
      "Epoch 2763/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02763: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4173 - val_loss: 1.4871\n",
      "Epoch 2764/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02764: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4164 - val_loss: 1.4915\n",
      "Epoch 2765/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4171\n",
      "Epoch 02765: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4171 - val_loss: 1.4885\n",
      "Epoch 2766/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02766: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4164 - val_loss: 1.4903\n",
      "Epoch 2767/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02767: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4169 - val_loss: 1.4902\n",
      "Epoch 2768/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02768: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4164 - val_loss: 1.4913\n",
      "Epoch 2769/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4153\n",
      "Epoch 02769: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4153 - val_loss: 1.4901\n",
      "Epoch 2770/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02770: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4149 - val_loss: 1.4906\n",
      "Epoch 2771/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4171\n",
      "Epoch 02771: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4171 - val_loss: 1.4897\n",
      "Epoch 2772/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4162\n",
      "Epoch 02772: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4162 - val_loss: 1.4863\n",
      "Epoch 2773/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4140\n",
      "Epoch 02773: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4140 - val_loss: 1.4906\n",
      "Epoch 2774/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4140\n",
      "Epoch 02774: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4140 - val_loss: 1.4923\n",
      "Epoch 2775/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02775: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4154 - val_loss: 1.4932\n",
      "Epoch 2776/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02776: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4165 - val_loss: 1.4911\n",
      "Epoch 2777/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4153\n",
      "Epoch 02777: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4153 - val_loss: 1.4896\n",
      "Epoch 2778/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02778: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4166 - val_loss: 1.4871\n",
      "Epoch 2779/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02779: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4149 - val_loss: 1.4879\n",
      "Epoch 2780/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02780: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4149 - val_loss: 1.4868\n",
      "Epoch 2781/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02781: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4152 - val_loss: 1.4896\n",
      "Epoch 2782/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02782: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4159 - val_loss: 1.4908\n",
      "Epoch 2783/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02783: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4151 - val_loss: 1.4901\n",
      "Epoch 2784/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4158\n",
      "Epoch 02784: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4158 - val_loss: 1.4898\n",
      "Epoch 2785/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02785: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4149 - val_loss: 1.4918\n",
      "Epoch 2786/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4171\n",
      "Epoch 02786: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4171 - val_loss: 1.4914\n",
      "Epoch 2787/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02787: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4164 - val_loss: 1.4881\n",
      "Epoch 2788/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4332\n",
      "Epoch 02788: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4332 - val_loss: 1.4895\n",
      "Epoch 2789/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4312\n",
      "Epoch 02789: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4312 - val_loss: 1.4882\n",
      "Epoch 2790/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 02790: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4235 - val_loss: 1.4870\n",
      "Epoch 2791/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02791: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4195 - val_loss: 1.4910\n",
      "Epoch 2792/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02792: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4869\n",
      "Epoch 2793/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4208\n",
      "Epoch 02793: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4208 - val_loss: 1.4888\n",
      "Epoch 2794/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4201\n",
      "Epoch 02794: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4201 - val_loss: 1.4872\n",
      "Epoch 2795/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02795: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4190 - val_loss: 1.4882\n",
      "Epoch 2796/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4238\n",
      "Epoch 02796: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4238 - val_loss: 1.4947\n",
      "Epoch 2797/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4221\n",
      "Epoch 02797: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4221 - val_loss: 1.4966\n",
      "Epoch 2798/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4224\n",
      "Epoch 02798: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4224 - val_loss: 1.4905\n",
      "Epoch 2799/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4181\n",
      "Epoch 02799: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4181 - val_loss: 1.4903\n",
      "Epoch 2800/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02800: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4159 - val_loss: 1.4893\n",
      "Epoch 2801/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02801: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4159 - val_loss: 1.4901\n",
      "Epoch 2802/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02802: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4170 - val_loss: 1.4914\n",
      "Epoch 2803/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02803: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4173 - val_loss: 1.4879\n",
      "Epoch 2804/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02804: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4169 - val_loss: 1.4882\n",
      "Epoch 2805/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 02805: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4164 - val_loss: 1.4894\n",
      "Epoch 2806/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02806: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4168 - val_loss: 1.4914\n",
      "Epoch 2807/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02807: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4159 - val_loss: 1.4963\n",
      "Epoch 2808/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4202\n",
      "Epoch 02808: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4202 - val_loss: 1.4926\n",
      "Epoch 2809/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02809: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4165 - val_loss: 1.4943\n",
      "Epoch 2810/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4245\n",
      "Epoch 02810: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4245 - val_loss: 1.4876\n",
      "Epoch 2811/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02811: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4186 - val_loss: 1.4899\n",
      "Epoch 2812/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4160\n",
      "Epoch 02812: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4160 - val_loss: 1.4891\n",
      "Epoch 2813/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02813: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4892\n",
      "Epoch 2814/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02814: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4179 - val_loss: 1.4918\n",
      "Epoch 2815/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4161\n",
      "Epoch 02815: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4161 - val_loss: 1.4895\n",
      "Epoch 2816/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4160\n",
      "Epoch 02816: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4160 - val_loss: 1.4924\n",
      "Epoch 2817/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4148\n",
      "Epoch 02817: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4148 - val_loss: 1.4888\n",
      "Epoch 2818/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4141\n",
      "Epoch 02818: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4141 - val_loss: 1.4913\n",
      "Epoch 2819/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 02819: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4189 - val_loss: 1.4920\n",
      "Epoch 2820/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02820: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4186 - val_loss: 1.4929\n",
      "Epoch 2821/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02821: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4173 - val_loss: 1.4932\n",
      "Epoch 2822/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4197\n",
      "Epoch 02822: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4197 - val_loss: 1.4985\n",
      "Epoch 2823/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4285\n",
      "Epoch 02823: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4285 - val_loss: 1.4905\n",
      "Epoch 2824/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4214\n",
      "Epoch 02824: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4214 - val_loss: 1.4910\n",
      "Epoch 2825/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 02825: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4182 - val_loss: 1.4927\n",
      "Epoch 2826/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02826: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4173 - val_loss: 1.4903\n",
      "Epoch 2827/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4145\n",
      "Epoch 02827: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4145 - val_loss: 1.4948\n",
      "Epoch 2828/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02828: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4159 - val_loss: 1.4897\n",
      "Epoch 2829/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4145\n",
      "Epoch 02829: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4145 - val_loss: 1.4906\n",
      "Epoch 2830/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4155\n",
      "Epoch 02830: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4155 - val_loss: 1.4892\n",
      "Epoch 2831/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02831: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4157 - val_loss: 1.4895\n",
      "Epoch 2832/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02832: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4168 - val_loss: 1.4884\n",
      "Epoch 2833/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02833: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4147 - val_loss: 1.4901\n",
      "Epoch 2834/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02834: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4152 - val_loss: 1.4951\n",
      "Epoch 2835/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4205\n",
      "Epoch 02835: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4205 - val_loss: 1.4909\n",
      "Epoch 2836/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4190\n",
      "Epoch 02836: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4190 - val_loss: 1.4903\n",
      "Epoch 2837/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4184\n",
      "Epoch 02837: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4184 - val_loss: 1.4915\n",
      "Epoch 2838/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4172\n",
      "Epoch 02838: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4172 - val_loss: 1.4949\n",
      "Epoch 2839/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02839: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4173 - val_loss: 1.4930\n",
      "Epoch 2840/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02840: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4165 - val_loss: 1.4943\n",
      "Epoch 2841/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4193\n",
      "Epoch 02841: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4193 - val_loss: 1.4926\n",
      "Epoch 2842/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4185\n",
      "Epoch 02842: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4185 - val_loss: 1.4952\n",
      "Epoch 2843/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4177\n",
      "Epoch 02843: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4177 - val_loss: 1.4903\n",
      "Epoch 2844/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4211\n",
      "Epoch 02844: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4211 - val_loss: 1.4895\n",
      "Epoch 2845/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02845: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4204 - val_loss: 1.4924\n",
      "Epoch 2846/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4183\n",
      "Epoch 02846: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4183 - val_loss: 1.4891\n",
      "Epoch 2847/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4150\n",
      "Epoch 02847: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4150 - val_loss: 1.4889\n",
      "Epoch 2848/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 02848: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4167 - val_loss: 1.4930\n",
      "Epoch 2849/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4169\n",
      "Epoch 02849: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4169 - val_loss: 1.4920\n",
      "Epoch 2850/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4186\n",
      "Epoch 02850: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4186 - val_loss: 1.4887\n",
      "Epoch 2851/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02851: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4157 - val_loss: 1.4865\n",
      "Epoch 2852/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4163\n",
      "Epoch 02852: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4163 - val_loss: 1.4880\n",
      "Epoch 2853/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4135\n",
      "Epoch 02853: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4135 - val_loss: 1.4858\n",
      "Epoch 2854/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4141\n",
      "Epoch 02854: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4141 - val_loss: 1.4880\n",
      "Epoch 2855/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02855: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4157 - val_loss: 1.4905\n",
      "Epoch 2856/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02856: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4147 - val_loss: 1.4903\n",
      "Epoch 2857/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02857: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4151 - val_loss: 1.4902\n",
      "Epoch 2858/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4143\n",
      "Epoch 02858: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4143 - val_loss: 1.4904\n",
      "Epoch 2859/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4132\n",
      "Epoch 02859: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4132 - val_loss: 1.4902\n",
      "Epoch 2860/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4156\n",
      "Epoch 02860: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4156 - val_loss: 1.4919\n",
      "Epoch 2861/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02861: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4168 - val_loss: 1.4918\n",
      "Epoch 2862/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4148\n",
      "Epoch 02862: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4148 - val_loss: 1.4901\n",
      "Epoch 2863/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4160\n",
      "Epoch 02863: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4160 - val_loss: 1.4900\n",
      "Epoch 2864/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02864: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4147 - val_loss: 1.4888\n",
      "Epoch 2865/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4135\n",
      "Epoch 02865: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4135 - val_loss: 1.4880\n",
      "Epoch 2866/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02866: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4159 - val_loss: 1.4896\n",
      "Epoch 2867/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4191\n",
      "Epoch 02867: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4191 - val_loss: 1.4911\n",
      "Epoch 2868/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02868: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4151 - val_loss: 1.4906\n",
      "Epoch 2869/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02869: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4147 - val_loss: 1.4895\n",
      "Epoch 2870/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02870: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4176 - val_loss: 1.4919\n",
      "Epoch 2871/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02871: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4175 - val_loss: 1.4943\n",
      "Epoch 2872/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4161\n",
      "Epoch 02872: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4161 - val_loss: 1.4921\n",
      "Epoch 2873/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4140\n",
      "Epoch 02873: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4140 - val_loss: 1.4903\n",
      "Epoch 2874/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02874: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4149 - val_loss: 1.4907\n",
      "Epoch 2875/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02875: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4154 - val_loss: 1.4907\n",
      "Epoch 2876/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4140\n",
      "Epoch 02876: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4140 - val_loss: 1.4909\n",
      "Epoch 2877/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4134\n",
      "Epoch 02877: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4134 - val_loss: 1.4905\n",
      "Epoch 2878/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4142\n",
      "Epoch 02878: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4142 - val_loss: 1.4905\n",
      "Epoch 2879/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4160\n",
      "Epoch 02879: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4160 - val_loss: 1.4923\n",
      "Epoch 2880/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02880: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4152 - val_loss: 1.4939\n",
      "Epoch 2881/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4155\n",
      "Epoch 02881: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4155 - val_loss: 1.4969\n",
      "Epoch 2882/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4233\n",
      "Epoch 02882: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4233 - val_loss: 1.4958\n",
      "Epoch 2883/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4206\n",
      "Epoch 02883: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4206 - val_loss: 1.4956\n",
      "Epoch 2884/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02884: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4166 - val_loss: 1.4887\n",
      "Epoch 2885/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02885: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4157 - val_loss: 1.4949\n",
      "Epoch 2886/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02886: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4178 - val_loss: 1.4900\n",
      "Epoch 2887/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4163\n",
      "Epoch 02887: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4163 - val_loss: 1.4889\n",
      "Epoch 2888/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02888: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4166 - val_loss: 1.4892\n",
      "Epoch 2889/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4153\n",
      "Epoch 02889: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4153 - val_loss: 1.4923\n",
      "Epoch 2890/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4158\n",
      "Epoch 02890: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4158 - val_loss: 1.4924\n",
      "Epoch 2891/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4141\n",
      "Epoch 02891: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4141 - val_loss: 1.4915\n",
      "Epoch 2892/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4145\n",
      "Epoch 02892: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4145 - val_loss: 1.4898\n",
      "Epoch 2893/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4133\n",
      "Epoch 02893: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4133 - val_loss: 1.4891\n",
      "Epoch 2894/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02894: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4151 - val_loss: 1.4902\n",
      "Epoch 2895/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02895: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4159 - val_loss: 1.4926\n",
      "Epoch 2896/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4139\n",
      "Epoch 02896: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4139 - val_loss: 1.4937\n",
      "Epoch 2897/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02897: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4173 - val_loss: 1.4902\n",
      "Epoch 2898/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4138\n",
      "Epoch 02898: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4138 - val_loss: 1.4914\n",
      "Epoch 2899/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4137\n",
      "Epoch 02899: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4137 - val_loss: 1.4911\n",
      "Epoch 2900/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4143\n",
      "Epoch 02900: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4143 - val_loss: 1.4909\n",
      "Epoch 2901/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4141\n",
      "Epoch 02901: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4141 - val_loss: 1.4916\n",
      "Epoch 2902/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4135\n",
      "Epoch 02902: loss did not improve from 1.41305\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4135 - val_loss: 1.4915\n",
      "Epoch 2903/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4127\n",
      "Epoch 02903: loss improved from 1.41305 to 1.41267, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 1.4127 - val_loss: 1.4912\n",
      "Epoch 2904/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02904: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4149 - val_loss: 1.4902\n",
      "Epoch 2905/3000\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 1.4092\n",
      "Epoch 02905: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 1.4130 - val_loss: 1.4911\n",
      "Epoch 2906/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4142\n",
      "Epoch 02906: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4142 - val_loss: 1.4908\n",
      "Epoch 2907/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4148\n",
      "Epoch 02907: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4148 - val_loss: 1.4906\n",
      "Epoch 2908/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02908: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4151 - val_loss: 1.4895\n",
      "Epoch 2909/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4153\n",
      "Epoch 02909: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4153 - val_loss: 1.4911\n",
      "Epoch 2910/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4148\n",
      "Epoch 02910: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4148 - val_loss: 1.4951\n",
      "Epoch 2911/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02911: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4178 - val_loss: 1.4919\n",
      "Epoch 2912/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02912: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4904\n",
      "Epoch 2913/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02913: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4176 - val_loss: 1.4907\n",
      "Epoch 2914/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02914: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4147 - val_loss: 1.4902\n",
      "Epoch 2915/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4150\n",
      "Epoch 02915: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4150 - val_loss: 1.4891\n",
      "Epoch 2916/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4158\n",
      "Epoch 02916: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4158 - val_loss: 1.4926\n",
      "Epoch 2917/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02917: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4165 - val_loss: 1.4896\n",
      "Epoch 2918/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4170\n",
      "Epoch 02918: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4170 - val_loss: 1.4885\n",
      "Epoch 2919/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02919: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4157 - val_loss: 1.4901\n",
      "Epoch 2920/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4162\n",
      "Epoch 02920: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4162 - val_loss: 1.4919\n",
      "Epoch 2921/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4133\n",
      "Epoch 02921: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4133 - val_loss: 1.4899\n",
      "Epoch 2922/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4129\n",
      "Epoch 02922: loss did not improve from 1.41267\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4129 - val_loss: 1.4935\n",
      "Epoch 2923/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4122\n",
      "Epoch 02923: loss improved from 1.41267 to 1.41216, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4122 - val_loss: 1.4906\n",
      "Epoch 2924/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4136\n",
      "Epoch 02924: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4136 - val_loss: 1.4905\n",
      "Epoch 2925/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4141\n",
      "Epoch 02925: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4141 - val_loss: 1.4925\n",
      "Epoch 2926/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4160\n",
      "Epoch 02926: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4160 - val_loss: 1.4891\n",
      "Epoch 2927/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02927: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 1.4154 - val_loss: 1.4914\n",
      "Epoch 2928/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4145\n",
      "Epoch 02928: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4145 - val_loss: 1.4903\n",
      "Epoch 2929/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4147\n",
      "Epoch 02929: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4147 - val_loss: 1.4913\n",
      "Epoch 2930/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4143\n",
      "Epoch 02930: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4143 - val_loss: 1.4929\n",
      "Epoch 2931/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4133\n",
      "Epoch 02931: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4133 - val_loss: 1.4912\n",
      "Epoch 2932/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4137\n",
      "Epoch 02932: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4137 - val_loss: 1.4909\n",
      "Epoch 2933/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4133\n",
      "Epoch 02933: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4133 - val_loss: 1.4947\n",
      "Epoch 2934/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4159\n",
      "Epoch 02934: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4159 - val_loss: 1.4942\n",
      "Epoch 2935/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4154\n",
      "Epoch 02935: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4154 - val_loss: 1.4912\n",
      "Epoch 2936/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4153\n",
      "Epoch 02936: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4153 - val_loss: 1.4930\n",
      "Epoch 2937/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4128\n",
      "Epoch 02937: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4128 - val_loss: 1.4919\n",
      "Epoch 2938/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4151\n",
      "Epoch 02938: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4151 - val_loss: 1.4909\n",
      "Epoch 2939/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4162\n",
      "Epoch 02939: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4162 - val_loss: 1.4950\n",
      "Epoch 2940/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4200\n",
      "Epoch 02940: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4200 - val_loss: 1.4950\n",
      "Epoch 2941/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02941: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4178 - val_loss: 1.4915\n",
      "Epoch 2942/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4165\n",
      "Epoch 02942: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4165 - val_loss: 1.4923\n",
      "Epoch 2943/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4142\n",
      "Epoch 02943: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4142 - val_loss: 1.4938\n",
      "Epoch 2944/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4161\n",
      "Epoch 02944: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4161 - val_loss: 1.4926\n",
      "Epoch 2945/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02945: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4176 - val_loss: 1.4925\n",
      "Epoch 2946/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4144\n",
      "Epoch 02946: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4144 - val_loss: 1.4898\n",
      "Epoch 2947/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4132\n",
      "Epoch 02947: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4132 - val_loss: 1.4909\n",
      "Epoch 2948/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4137\n",
      "Epoch 02948: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4137 - val_loss: 1.4936\n",
      "Epoch 2949/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4143\n",
      "Epoch 02949: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4143 - val_loss: 1.4928\n",
      "Epoch 2950/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4150\n",
      "Epoch 02950: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4150 - val_loss: 1.4941\n",
      "Epoch 2951/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4179\n",
      "Epoch 02951: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4179 - val_loss: 1.4916\n",
      "Epoch 2952/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4144\n",
      "Epoch 02952: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4144 - val_loss: 1.4903\n",
      "Epoch 2953/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4142\n",
      "Epoch 02953: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4142 - val_loss: 1.4897\n",
      "Epoch 2954/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02954: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4157 - val_loss: 1.4919\n",
      "Epoch 2955/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4150\n",
      "Epoch 02955: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4150 - val_loss: 1.4905\n",
      "Epoch 2956/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4149\n",
      "Epoch 02956: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4149 - val_loss: 1.4896\n",
      "Epoch 2957/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4176\n",
      "Epoch 02957: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4176 - val_loss: 1.4890\n",
      "Epoch 2958/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4168\n",
      "Epoch 02958: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4168 - val_loss: 1.4949\n",
      "Epoch 2959/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4199\n",
      "Epoch 02959: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4199 - val_loss: 1.4954\n",
      "Epoch 2960/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4185\n",
      "Epoch 02960: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4185 - val_loss: 1.4914\n",
      "Epoch 2961/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4144\n",
      "Epoch 02961: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4144 - val_loss: 1.4919\n",
      "Epoch 2962/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4146\n",
      "Epoch 02962: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4146 - val_loss: 1.4912\n",
      "Epoch 2963/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4146\n",
      "Epoch 02963: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4146 - val_loss: 1.4895\n",
      "Epoch 2964/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4133\n",
      "Epoch 02964: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4133 - val_loss: 1.4901\n",
      "Epoch 2965/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4122\n",
      "Epoch 02965: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4122 - val_loss: 1.4913\n",
      "Epoch 2966/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4123\n",
      "Epoch 02966: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4123 - val_loss: 1.4928\n",
      "Epoch 2967/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4137\n",
      "Epoch 02967: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4137 - val_loss: 1.4976\n",
      "Epoch 2968/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4156\n",
      "Epoch 02968: loss did not improve from 1.41216\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4156 - val_loss: 1.4920\n",
      "Epoch 2969/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4119\n",
      "Epoch 02969: loss improved from 1.41216 to 1.41190, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4119 - val_loss: 1.4899\n",
      "Epoch 2970/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4104\n",
      "Epoch 02970: loss improved from 1.41190 to 1.41039, saving model to ./temp_decoder.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 1.4104 - val_loss: 1.4905\n",
      "Epoch 2971/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4125\n",
      "Epoch 02971: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4125 - val_loss: 1.4909\n",
      "Epoch 2972/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4145\n",
      "Epoch 02972: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4145 - val_loss: 1.4903\n",
      "Epoch 2973/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4125\n",
      "Epoch 02973: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4125 - val_loss: 1.4917\n",
      "Epoch 2974/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4145\n",
      "Epoch 02974: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4145 - val_loss: 1.4933\n",
      "Epoch 2975/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4122\n",
      "Epoch 02975: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4122 - val_loss: 1.4924\n",
      "Epoch 2976/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4130\n",
      "Epoch 02976: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4130 - val_loss: 1.4938\n",
      "Epoch 2977/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4137\n",
      "Epoch 02977: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4137 - val_loss: 1.4982\n",
      "Epoch 2978/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02978: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4152 - val_loss: 1.4944\n",
      "Epoch 2979/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4145\n",
      "Epoch 02979: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4145 - val_loss: 1.4978\n",
      "Epoch 2980/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4144\n",
      "Epoch 02980: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4144 - val_loss: 1.5057\n",
      "Epoch 2981/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4213\n",
      "Epoch 02981: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4213 - val_loss: 1.4969\n",
      "Epoch 2982/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02982: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4152 - val_loss: 1.4896\n",
      "Epoch 2983/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4130\n",
      "Epoch 02983: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4130 - val_loss: 1.4898\n",
      "Epoch 2984/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4121\n",
      "Epoch 02984: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4121 - val_loss: 1.4949\n",
      "Epoch 2985/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4138\n",
      "Epoch 02985: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4138 - val_loss: 1.4956\n",
      "Epoch 2986/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4204\n",
      "Epoch 02986: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4204 - val_loss: 1.4960\n",
      "Epoch 2987/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4195\n",
      "Epoch 02987: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4195 - val_loss: 1.4916\n",
      "Epoch 2988/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 02988: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4178 - val_loss: 1.4933\n",
      "Epoch 2989/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4175\n",
      "Epoch 02989: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4175 - val_loss: 1.4952\n",
      "Epoch 2990/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4173\n",
      "Epoch 02990: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4173 - val_loss: 1.4943\n",
      "Epoch 2991/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 02991: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4166 - val_loss: 1.4952\n",
      "Epoch 2992/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 02992: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4152 - val_loss: 1.4934\n",
      "Epoch 2993/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4137\n",
      "Epoch 02993: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4137 - val_loss: 1.4921\n",
      "Epoch 2994/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4124\n",
      "Epoch 02994: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4124 - val_loss: 1.4941\n",
      "Epoch 2995/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4140\n",
      "Epoch 02995: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4140 - val_loss: 1.4927\n",
      "Epoch 2996/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4157\n",
      "Epoch 02996: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4157 - val_loss: 1.5011\n",
      "Epoch 2997/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4269\n",
      "Epoch 02997: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4269 - val_loss: 1.4939\n",
      "Epoch 2998/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4256\n",
      "Epoch 02998: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4256 - val_loss: 1.4923\n",
      "Epoch 2999/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4192\n",
      "Epoch 02999: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 1.4192 - val_loss: 1.4935\n",
      "Epoch 3000/3000\n",
      "9/9 [==============================] - ETA: 0s - loss: 1.4166\n",
      "Epoch 03000: loss did not improve from 1.41039\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 1.4166 - val_loss: 1.4936\n"
     ]
    }
   ],
   "source": [
    "hist = mdl.fit(A_train_in,u_train[0,:,:,:,:],\n",
    "            epochs=nb_epoch,batch_size=batch_size,shuffle=True,\n",
    "            validation_data=(A_val_in,u_val[0,:,:,:,:]),\n",
    "            callbacks=cb,verbose=1)\n",
    "hist_train.extend(hist.history['loss'])\n",
    "hist_val.extend(hist.history['val_loss'])\n",
    "mdl.load_weights(tempfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist_train,label='train')\n",
    "plt.plot(hist_val,label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are results calculated through two methods the same? True\n"
     ]
    }
   ],
   "source": [
    "# get results\n",
    "y = mdl.predict(A_train_in)\n",
    "\n",
    "mode_shape = []\n",
    "ae_modes = []\n",
    "for i in range(latent_dim):\n",
    "    mode_shape.append(decoders[i].predict(np.reshape(1,(1,1))))\n",
    "    ae_modes.append(decoders[i].predict(A_train_in[:,[i]]))\n",
    "y_add = np.sum(ae_modes,axis=0)\n",
    "print('are results calculated through two methods the same?',np.array_equal(y,y_add))\n",
    "\n",
    "mode_shape = np.array(mode_shape)\n",
    "ae_modes = np.array(ae_modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visulise the weights of the decoder\n",
    "fig1,ax1 = plt.subplots(2,latent_dim,sharey='all')\n",
    "fig1.suptitle('shape of the modes,v')\n",
    "ax1[0,0].set_ylabel('POD')\n",
    "ax1[1,0].set_ylabel('model')\n",
    "for i in range(latent_dim):\n",
    "    pltV = np.reshape(Q_POD_data[:,i],[2*Ny,Nz])\n",
    "    im1 = ax1[0,i].imshow(pltV[:Ny,:],'jet')\n",
    "    ax1[0,i].set_title(str(i+1))\n",
    "    ax1[0,i].set_xticks([])\n",
    "    ax1[0,i].set_yticks([])\n",
    "    div = make_axes_locatable(ax1[0,i])\n",
    "    cax = div.append_axes('right',size='5%',pad='2%')\n",
    "    plt.colorbar(im1,cax=cax)\n",
    "\n",
    "    im2 = ax1[1,i].imshow(mode_shape[i,0,:,:,0],'jet')\n",
    "    div = make_axes_locatable(ax1[1,i])\n",
    "    cax = div.append_axes('right',size='5%',pad='2%')\n",
    "    plt.colorbar(im2,cax=cax)\n",
    "\n",
    "# visualise the changing modes\n",
    "t = 100\n",
    "\n",
    "fig2,ax2 = plt.subplots(2,latent_dim,sharey='all')\n",
    "# fig2,ax2 = plt.subplots(2,[latent_dim],sharey='all')\n",
    "fig2.suptitle(f'modes (changing with time) at time {t}')\n",
    "ax2[0,0].set_ylabel('POD')\n",
    "ax2[1,0].set_ylabel('model')\n",
    "for i in range(latent_dim):\n",
    "    pltV_t = pod_modes_t[i,0,:,:,t]\n",
    "    im1 = ax2[0,i].imshow(pltV_t,'jet')\n",
    "    ax2[0,i].set_title(str(i+1))\n",
    "    ax2[0,i].set_xticks([])\n",
    "    ax2[0,i].set_yticks([])\n",
    "    div = make_axes_locatable(ax2[0,i])\n",
    "    cax = div.append_axes('right',size='5%',pad='2%')\n",
    "    plt.colorbar(im1,cax=cax)\n",
    "\n",
    "    im2 = ax2[1,i].imshow(ae_modes[i,t,:,:,0],'jet')\n",
    "    div = make_axes_locatable(ax2[1,i])\n",
    "    cax = div.append_axes('right',size='5%',pad='2%')\n",
    "    plt.colorbar(im2,cax=cax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation coefficient 0.9565487936316703\n"
     ]
    }
   ],
   "source": [
    "ke = 0.5 * np.einsum('z t x y u -> z t', ae_modes**2)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(A_data[:,0]**2, ke[0,:], alpha=0.3)\n",
    "plt.xlabel('z**2')\n",
    "plt.ylabel('ke of corresponding mode')\n",
    "plt.show()\n",
    "\n",
    "print('correlation coefficient', np.corrcoef(np.abs(A_data[:,0]**2), ke[0,:])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reproject autoencoder modes onto data pod modes\n",
    "lam_modes = mode_eval.equivalent_pca_energy(ae_modes,Q_POD_data)\n",
    "lam_modes_percent = lam_modes/lam_data\n",
    "\n",
    "x_axis = np.arange(1, Nz*Ny*Nu+1)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(latent_dim):\n",
    "    plt.plot(x_axis,lam_modes[i,:],label=i+1,linestyle='--',marker='x')\n",
    "plt.plot(x_axis,lam_data,label='data')\n",
    "plt.xlim([0,20])\n",
    "plt.xticks(range(20))\n",
    "plt.ylim(bottom=0)\n",
    "plt.ylabel('variance of the POD axis (eigenvalues)')\n",
    "plt.xlabel('POD mode')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(latent_dim):\n",
    "    plt.plot(x_axis,lam_modes_percent[i,:],label=i+1,linestyle='--',marker='x')\n",
    "plt.xlim([0,20])\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(range(20))\n",
    "plt.legend()\n",
    "plt.xlabel('POD mode')\n",
    "plt.ylabel('precentage variance')\n",
    "plt.title('precentage of POD modes captured')\n",
    "\n",
    "\n",
    "# Similarity between time coeffcient and latent variables\n",
    "mag_A = einsum('t x -> x',A_data**2)**0.5\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle('cosine of the angle between the latent variables and POD time coefficient (as time series)')\n",
    "plt.xlabel('POD mode')\n",
    "plt.ylabel('|cos(angle_z&A)|')\n",
    "for i in range(latent_dim):\n",
    "    mag_z = np.sum(A_data[:,i]**2)**0.5\n",
    "    divisor = mag_A * mag_z\n",
    "    z_dot_A = A_data[:,[i]].T @ A_data\n",
    "    cos_angle = (z_dot_A / divisor).flatten()\n",
    "    plt.plot(x_axis, np.abs(cos_angle),label=str(i+1))\n",
    "plt.xlim([0,20])\n",
    "plt.xticks(range(20))\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:\n",
      "reconstruction by POD:    1.4777343273162842\n",
      "reconstructed by model:   1.4079658\n"
     ]
    }
   ],
   "source": [
    "c = []\n",
    "c.append(recons_data[:,:,:,0])\n",
    "c.append(recons_data[:,:,:,1])\n",
    "c = np.transpose(c,[1,2,3,0])\n",
    "\n",
    "print('mse:')\n",
    "print('reconstruction by POD:   ', mse(u_train[0,:,:,:,:],c).numpy())\n",
    "print('reconstructed by model:  ', mse(u_train[0,:,:,:,:],y).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('MD-CNN-AE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f85e8d5ca1c7d62daa514db97b690def00c9e189bf201b0a2c929de67d960fcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
